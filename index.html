<!DOCTYPE html>
<html  lang="en">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 3.9.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>David Kwon</title>


    <meta property="og:type" content="website">
<meta property="og:title" content="David Kwon">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="David Kwon">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/images/og_image.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="David Kwon">
<meta name="twitter:image" content="http://yoursite.com/images/og_image.png">








<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
    
    <style>body>.footer,body>.navbar,body>.section{opacity:0}</style>
    

    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">
    

    
    

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">


    
    
    
    

<link rel="stylesheet" href="/css/back-to-top.css">


    
    

    
    
    
    

    
    
<link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

    
    
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-2-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.svg" alt="David Kwon" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item is-active"
                href="/">Home</a>
                
                <a class="navbar-item"
                href="/archives">Archives</a>
                
                <a class="navbar-item"
                href="/categories">Categories</a>
                
                <a class="navbar-item"
                href="/tags">Tags</a>
                
                <a class="navbar-item"
                href="/about">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                    
                    
                    <a class="navbar-item" target="_blank" title="Download on GitHub" href="https://github.com/DavidKwon91">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                    
                
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-8-widescreen has-order-2 column-main">
    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-19T08:13:52.000Z">2019-09-19</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Decision-Tree/">Decision Tree</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    15 minutes read (About 2226 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/19/DecisionTree/">DecisionTree</a>
            
        </h1>
        <div class="content">
            <h1 id="Introduction-to-Decision-Tree"><a href="#Introduction-to-Decision-Tree" class="headerlink" title="Introduction to Decision Tree"></a>Introduction to Decision Tree</h1><p>In this post, I will discuss about Decision Tree, which is the fundamental algorithm of the well-known Tree-based algorithms. This post will also address two metrics, which is employed in Decision tree and many other machine learning algorithms, even in deep learning. </p>
<h2 id="1-Metric"><a href="#1-Metric" class="headerlink" title="1. Metric"></a>1. Metric</h2><p>Why do we need the “Metric” in tree-based algorithm? </p>
<p>The answer will be to find the best split in the tree. We use the metric, calculate a variable, measure the split score with the value, and choose the value for the best split at each step of the split or the predictors in the tree. Many other metrics or score functions are used to find the “best” split for different tree-based algorithms. I will deal with two popular metrics, which are Gini Impurity (or Gini Index) and Information Gain with Entropy. </p>
<h4 id="1-1-Gini-Index"><a href="#1-1-Gini-Index" class="headerlink" title="1.1 Gini Index"></a>1.1 Gini Index</h4><p>Suppose we have a dependent categorical variable, which have $\mathcal{J}$ classes. Let $i \in \{1,2,…,\mathcal{J}\}$ and $p_i$ is the probability given each factor levels of dependent variable.d</p>
<p>Then, the Gini Index, or Gini Impurity is defined as below, </p>
<p>$$I_{\mathcal{G}}(p) = \sum_{i=1}^{J}p_i \sum_{k \neq i}p_k = \sum_{i=1}^{J}p_i(1-p_i) = \sum_{i=1}^{J}(p_i-p_i^2) = \sum_{i=1}^{J}p_i - \sum_{i=1}^{J}p_i^2 \\ = 1-\sum_{i=1}^{J}p_i^2$$</p>
<p>For example, </p>
<p>suppose we have a binary variable that follows below. </p>
<p>$X = [FALSE,FALSE,TRUE,TRUE,….,TRUE,FALSE]$</p>
<img src="/2019/09/19/DecisionTree/image1.png" title="[table]">

<p>Then, the Gini Impurity will be the followng by the definition,</p>
<p>$$1 - (\frac{8}{20})^2 - (\frac{12}{20})^2 = 0.48$$</p>
<p>What about the Gini Impurity for this binary variable?</p>
<img src="/2019/09/19/DecisionTree/image2.png" title="[table]">

<p>Then, the Gini Impurity will be</p>
<p>$$1 - (\frac{1}{20})^2 - (\frac{19}{20})^2 = 0.095$$</p>
<p>Now, we see that the smaller Gini Impurity is, we find the better split point.</p>
<p>Therefore, we want to find the smaller Gini Impurity score as possible as we can at each step to find best split in the tree. </p>
<p>Now, let we have two binary variables and make table for each factors like the below, </p>
<p>$$Y = [FALSE, FALSE, … ,  TRUE] \\ X = [YES, YES, … , NO]$$</p>
<p>And, we want to predict $Y$ with $X$. </p>
<img src="/2019/09/19/DecisionTree/image3.png" title="[table]">

<p>We will have only one stump in a Decision Tree for this table. The root node, which is the very top of the tree, will be the predictor, $X = [YES, YES, … , NO]$</p>
<p>The tree is something like..</p>
<img src="/2019/09/19/DecisionTree/Tree1.png" title="[table]">

<p>The typical tree in real world is much more complicated with more nodes and splits, since they have many predictors.<br>This tree that has only one split is sometimes called a stump, which is often called a weak learner in Adaboost.<br>But, I will just call this as a tree for this example.  </p>
<p>Then, the process of finding Gini Impurity is the below;</p>
<p>1) Find Conditional probability</p>
<p>For example, the Conditional probability given $X$ is</p>
<p>$$P(Y = FALSE |X = YES) = \frac{5}{7} \\ P(Y = TRUE | X = YES) = \frac{2}{7} \\ P(Y = FALSE | X = NO) = \frac{3}{13} \\ P(Y = TRUE | X = NO) = \frac{10}{13}$$</p>
<p>2) Find each Gini Index for the conditional probability</p>
<p>$$I_{Gini}(Y|X=YES) = 1 - (\frac{5}{7})^2 - (\frac{2}{7})^2  =  0.4081633 \\ I_{Gini}(Y|X=NO) = 1 - (\frac{3}{13})^2 - (\frac{10}{13})^2 = 0.3550296 $$</p>
<p>Then, we find a Gini Impurity score for a leaf node in this tree, which is the very bottom of the tree or the final classification. </p>
<img src="/2019/09/19/DecisionTree/Tree2.png" title="[table]">



<p>3) Find Marginal probability</p>
<p>The margianl probabilty given $X$ is</p>
<p>$$P_X(X=YES) = \frac{7}{20} \\ P_X(X=NO) = \frac{13}{20}$$</p>
<p>Since the tree is splitted by $X$, we find the Gini Impurity of the total tree. </p>
<p>4) Multiply and add the Gini Index and marginal probability associated with the given probability respectively. Finally, we have Gini Impurity Score for a split. </p>
<p>Then, the <em>Total Gini Impurity Score</em> for this tree is, </p>
<p>$$I_{Gini}(Y|X=YES) \times P_X(X=YES) + I_{Gini}(Y|X=NO) \times P_X(X=NO) = \\ 0.4081633 \times \frac{7}{20} + 0.3550296 \times \frac{13}{20} = 0.3736264$$</p>
<p>Then, we eventually find the total Gini Impurity score for a tree. </p>
<img src="/2019/09/19/DecisionTree/Tree3.png" title="[table]">

<p>In real world problem, we have many predictors and so many nodes, possibly many trees such in Random Forest. </p>
<p>With this process of finding Gini Impurity, we compare the Gini score and find the best split point of the best predictor with best splits. </p>
<h4 id="1-2-Entropy"><a href="#1-2-Entropy" class="headerlink" title="1.2 Entropy"></a>1.2 Entropy</h4><p>Entropy is defined as below; </p>
<p>$$H(T) = -\sum_{i=1}^{J}p_ilog_2(p_i)$$</p>
<p>And the process of finding best split in a tree is to find the Information Gain. </p>
<p>The Information Gain is often called <em>Kullback-Leiber divergence</em> and defined as below;</p>
<p>$$IG(T,a) = H(T) - H(T|a)$$</p>
<p>where $IG(Y,X)$ is the Information Gain of Y given X, $H(Y)$ is the parent Entropy of Y, and $H(Y|X)$ is the children Entropy of Y given X, and the $H(Y|X)$ is often called <em>cross-entropy</em>.</p>
<p>The process of finding Information Gain is very similar to the Gini Score, but here we find the bigger value of IG. </p>
<p>I will use the same example problem as above. </p>
<p>For the above example, </p>
<p>$$Y = [FALSE, FALSE, … ,  TRUE] \\ X = [YES, YES, … , NO]$$</p>
<img src="/2019/09/19/DecisionTree/image3.png" title="[table]">

<p>Then, as I have shown, find the conditional probability. </p>
<p>1) Find the conditional probability given $X$</p>
<p>$$P(Y = FALSE |X = YES) = \frac{5}{7} \\ P(Y = TRUE | X = YES) = \frac{2}{7} \\ P(Y = FALSE | X = NO) = \frac{3}{13} \\ P(Y = TRUE | X = NO) = \frac{10}{13}$$</p>
<p>2) Find the entropy given $X$</p>
<p>$$I_{Entropy}(Y|X=YES) = -\frac{5}{7}log_2(\frac{5}{7}) - \frac{2}{7}log_2(\frac{2}{7}) = 0.8631206\\ I_{Entropy}(Y|X=NO) = -\frac{3}{13}log_2(\frac{3}{13}) - \frac{10}{13}log_2(\frac{10}{13})  =  0.7793498 $$</p>
<p>3) Find the marginal probability</p>
<p>The margianl probabilty given $X$ is</p>
<p>$$P_X(X=YES) = \frac{7}{20} \\ P_X(X=NO) = \frac{13}{20}$$</p>
<p>4) Calculate the Total Entropy and Cross-Entropy</p>
<p>Total Entropy for a tree divided by $X$ is</p>
<p>$$-P_X(X=YES) \times log_2(P_X(X=YES))-P_X(X=NO) \times log_2(P_X(X=NO)) \\ = -\frac{7}{20}log_2(\frac{7}{20}) - \frac{13}{20}log_2(\frac{13}{20}) \\ = 0.9340681$$</p>
<p>Cross Entropy for a leaf node is</p>
<p>$$I_{Entropy}(Y|X=YES) \times  P_X(X=YES) + I_{Entropy}(Y|X=NO) \times P_X(X=NO) \\ = 0.8631206 \times \frac{7}{20} + 0.7793498 \times \frac{13}{20} \\ = 0.8086696$$</p>
<p>5) Find Information Gain with Total Entropy and Cross Entropy</p>
<p>Information Gain = Total Entropy - Cross Entropy</p>
<p>$$= 0.9340681 - 0.8086696 = 0.1253985$$ </p>
<p>For another example that shows why bigger Information Gain is better, </p>
<p>suppose we have the dataset following; </p>
<img src="/2019/09/19/DecisionTree/table.png" title="[table]">

<p>It’s obviously better to split for Y given X than the above example, since this has only one mistake. </p>
<p>If we find the Information Gain of this data,</p>
<p>The Total Entropy for a tree divided by $X$ is 1, since this splits exactly the same number of target observations with 10 and 10. </p>
<p>$$-P_X(X=YES) \times log_2(P_X(X=YES))-P_X(X=NO) \times log_2(P_X(X=NO)) = \\ -\frac{10}{20}log_2(\frac{10}{20}) - \frac{10}{20}log_2(\frac{10}{20}) \\ = 0.5 + 0.5 = 1$$</p>
<p>The cross entropy for a leaf node is</p>
<p>$$I_{Entropy}(Y|X=YES) \times  P_X(X=YES) + I_{Entropy}(Y|X=NO) \times P_X(X=NO) \\ = 0.234478$$</p>
<p>calculated by the same process of the above.</p>
<p>Then, the Information Gain is</p>
<p>$$1 - 0.234478 = 0.7655022$$</p>
<p>Therefore, we find the bigger Information Gain as possible as we can. </p>
<p>As a summary, Decision Tree Algorithm is..</p>
<p>1) Find the best split with the metric score for each splits and each predictors. </p>
<p>2) Compare the metric score with every splits and other predictors and find the best split and best predictors. </p>
<p>3) The best predictors that has the best metric score will be the roof node, and keep building a tree with the metric scores. </p>
<p>4) The very bottom of the tree, which is the leaf node, will be our final classification. </p>
<p>If we have continuous predictors, there is no split point like “YES” or “NO”. Therefore, </p>
<p>1) we sort the data associated with the numerical predictors, </p>
<p>2) calculate the average for all adjacent indexes of the numerical predictor, </p>
<p>find the Gini Impurity for all the adjacent average value, </p>
<p>and compare the Gini Impurity to find the best split point of adjacent average of the numerical predictors. </p>
<p>Below is the Implementation in R. I used iris dataset, and a dataset that I created.<br>Some of codes are skipped, and you can refer to my Gibhub website to see full algorithm. </p>
<p><a href="https://github.com/DavidKwon91/MathematicsBehindAlgorithms" target="_blank" rel="noopener">DavidGithub</a></p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#removing one of the classes of the target variable, virginica in Iris Dataset. </span><br><span class="line">#to make target as a binary variable</span><br><span class="line">iris1 &lt;- iris[which(iris$Species != &quot;virginica&quot;),]</span><br><span class="line"></span><br><span class="line">#removing the factor level that we don&apos;t have any more</span><br><span class="line">iris1$Species &lt;- as.factor(as.character(iris1$Species))</span><br><span class="line"></span><br><span class="line">#quick decision tree built in R, rpart</span><br><span class="line">tr &lt;- rpart(Species~., training)</span><br><span class="line">rpart.plot(tr)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/19/DecisionTree/IntroductionDecisionTree-1.png" title="[tree1]">


<p>This is the prediction by Decision Tree built in R. </p>
<img src="/2019/09/19/DecisionTree/IntroductionDecisionTree-2.png" title="[tree1]">


<p>Here is the Decision Tree algorithm with the introduced two metrics that I built. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line">#average function to create adjacent average between predictor indexes</span><br><span class="line">avg &lt;- function(x1,x2)&#123;sum(x1,x2)/2&#125;</span><br><span class="line"></span><br><span class="line">#gini function for a leaf</span><br><span class="line">gini &lt;- function(x)&#123;</span><br><span class="line">  </span><br><span class="line">  if(dim(x)[1]==1)&#123;</span><br><span class="line">    return(1)</span><br><span class="line">  &#125;</span><br><span class="line">  else&#123;</span><br><span class="line">    #conditional probability given predictor (row : target, column : predictor)</span><br><span class="line">    p11&lt;-x[1,1]/sum(x[,1])</span><br><span class="line">    p21&lt;-x[2,1]/sum(x[,1])</span><br><span class="line">    p12&lt;-x[1,2]/sum(x[,2])</span><br><span class="line">    p22&lt;-x[2,2]/sum(x[,2])</span><br><span class="line">    </span><br><span class="line">    a.false.gini &lt;- 1-p11^2-p21^2</span><br><span class="line">    a.true.gini &lt;- 1-p12^2-p22^2</span><br><span class="line">    </span><br><span class="line">    #marginal probability</span><br><span class="line">    a.false.prob &lt;- (x[1,1]+x[1,2]) / sum(x)</span><br><span class="line">    a.true.prob &lt;- (x[2,1]+x[2,2]) / sum(x)</span><br><span class="line">    </span><br><span class="line">    gini.imp &lt;- a.false.prob * a.false.gini + a.true.prob * a.true.gini</span><br><span class="line">    return(gini.imp)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#log function with base 2 for entropy</span><br><span class="line">log2 &lt;- function(x)&#123;</span><br><span class="line">  if(x!=0)&#123;</span><br><span class="line">    return(log(x,base=2))</span><br><span class="line">  &#125;</span><br><span class="line">  if(x==0)&#123;</span><br><span class="line">    return(0)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#entropy function for a leaf</span><br><span class="line">entropy &lt;- function(x)&#123;</span><br><span class="line">  </span><br><span class="line">  if(dim(x)[1]==1)&#123;</span><br><span class="line">    return(0)</span><br><span class="line">  &#125;</span><br><span class="line">  else&#123;</span><br><span class="line">  	#conditional probability given predictor (row : target, column : predictor)</span><br><span class="line">    p11&lt;-x[1,1]/sum(x[,1])</span><br><span class="line">    p21&lt;-x[2,1]/sum(x[,1])</span><br><span class="line">    p12&lt;-x[1,2]/sum(x[,2])</span><br><span class="line">    p22&lt;-x[2,2]/sum(x[,2])</span><br><span class="line">    </span><br><span class="line">    #Calculating weights, which is the bottom of the tree</span><br><span class="line">    a.false.entropy &lt;- -(p11*log2(p11)+p21*log2(p21))</span><br><span class="line">    a.true.entropy &lt;- -(p12*log2(p12)+p22*log2(p22))</span><br><span class="line">    </span><br><span class="line">    #marginal probability</span><br><span class="line">    a.false.prob &lt;- (x[1,1]+x[2,1]) / sum(x)</span><br><span class="line">    a.true.prob &lt;- (x[1,2]+x[2,2]) / sum(x)</span><br><span class="line">    </span><br><span class="line">    #cross entropy</span><br><span class="line">    weighted.entropy &lt;- a.true.prob*a.true.entropy + a.false.prob*a.false.entropy</span><br><span class="line">    </span><br><span class="line">    #total entropy</span><br><span class="line">    total.entropy &lt;- -(a.false.prob*log2(a.false.prob) + a.true.prob*log2(a.true.prob))</span><br><span class="line">    </span><br><span class="line">    #Information Gain, which is the tree score to find best split</span><br><span class="line">    #If the bigger this value is, we find the better split</span><br><span class="line">    #maximum value is 1</span><br><span class="line">    IG &lt;- total.entropy - weighted.entropy</span><br><span class="line">    </span><br><span class="line">    return(IG) </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Calculating impurity to find which predictor is the best to split, which will be the top of the tree or first split in the tree</span><br><span class="line">var.impurity &lt;- function(x, dat, fun)&#123;</span><br><span class="line">  imp.dat &lt;- data.frame(matrix(0, nrow=nrow(dat)-1, ncol=3))</span><br><span class="line">  colnames(imp.dat) &lt;- c(&quot;index&quot;, &quot;impurity&quot;, &quot;adj.avg&quot;)</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">  for(i in 1:(nrow(dat)-1))&#123;</span><br><span class="line">    imp.dat[i,1] &lt;- paste0(&quot;between &quot;, i, &quot; and &quot;, i+1)</span><br><span class="line">    #average value of the adjacent values</span><br><span class="line">    a &lt;- avg(x[i], x[i+1])</span><br><span class="line">    </span><br><span class="line">    predictor.name &lt;- colnames(dat)[which(sapply(dat, function(x,want) isTRUE(all.equal(x,want)),x)==TRUE)]</span><br><span class="line">    </span><br><span class="line">    #Sorting the data by the predictor</span><br><span class="line">    dat1 &lt;- dat[order(x,decreasing=FALSE),]</span><br><span class="line">    mat &lt;- as.matrix(table(dat1[,predictor.name] &lt; a, dat1[,target] ))</span><br><span class="line">    </span><br><span class="line">    #apply the metric, Gini or Entropy</span><br><span class="line">    imp.dat[i,2] &lt;- fun(mat)</span><br><span class="line">    imp.dat[i,3] &lt;- a</span><br><span class="line">    &#125;</span><br><span class="line">  return(imp.dat)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#this function will give you the best split score for each predictors</span><br><span class="line">impurity.fun &lt;- function(dat, fun)&#123;</span><br><span class="line">  predictors &lt;- colnames(dat)[!colnames(dat) %in% target]</span><br><span class="line">  var.impur.dat &lt;- data.frame(matrix(0, nrow=length(predictors),ncol=2))</span><br><span class="line">  colnames(var.impur.dat) &lt;- c(&quot;var&quot;, &quot;impurity&quot;)</span><br><span class="line">  </span><br><span class="line">  for(i in 1:(ncol(dat)-1))&#123;</span><br><span class="line">    var.impur.dat[i,1] &lt;- predictors[i]</span><br><span class="line">    if(fun == &quot;gini&quot;)&#123;</span><br><span class="line">      #the least score of gini is the best split </span><br><span class="line">      var.impur.dat[i,2] &lt;- min(var.impurity(dat[,i], dat, gini)$impurity)</span><br><span class="line">    &#125;</span><br><span class="line">    if(fun == &quot;entropy&quot;)&#123;</span><br><span class="line">      #the greates score of entropy is the best split</span><br><span class="line">      var.impur.dat[i,2] &lt;- max(var.impurity(dat[,i], dat, entropy)$impurity)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  return(var.impur.dat)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#give you the best predictor to split or the top of the tree</span><br><span class="line">topTree.predictor &lt;- function(x,fun)&#123;</span><br><span class="line">  if(fun == &quot;entropy&quot;)&#123;</span><br><span class="line">    return(which.max(impurity.fun(x, &quot;entropy&quot;)[,2]))</span><br><span class="line">  &#125;</span><br><span class="line">  if(fun == &quot;gini&quot;)&#123;</span><br><span class="line">    return(which.min(impurity.fun(x, &quot;gini&quot;)[,2]))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#The best split point associated with the best predictor</span><br><span class="line">impurityOfbest &lt;- function(dat, best.pred, fun)&#123;</span><br><span class="line">  if(fun == &quot;entropy&quot;)&#123;</span><br><span class="line">    impurity.pred &lt;- var.impurity(dat[,best.pred], dat, entropy)$adj.avg[which.max(var.impurity(dat[,best.pred], dat, entropy)$impurity)]</span><br><span class="line">  &#125;</span><br><span class="line">  if(fun == &quot;gini&quot;)&#123;</span><br><span class="line">    impurity.pred &lt;- var.impurity(dat[,best.pred], dat, gini)$adj.avg[which.min(var.impurity(dat[,best.pred], dat, gini)$impurity)]</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  print(paste0(&quot;Best predictor, which is top tree node is &quot;, colnames(dat)[best.pred], &quot; with best split is  &quot;, impurity.pred, &quot; by the metric, &quot;, fun))</span><br><span class="line">  return(impurity.pred)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#by Entropy metric, we want to find the maximum entropy score</span><br><span class="line">fun &lt;- &quot;entropy&quot;</span><br><span class="line">target &lt;- &quot;Species&quot;</span><br><span class="line"></span><br><span class="line">imp.pred &lt;- topTree.predictor(iris1, fun) #This is the best predictor that splits our target the best. </span><br><span class="line">best.impur &lt;- impurityOfbest(iris1, imp.pred, fun) #This is the best split point for the best predictor. </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Let&apos;s see how well this value calculated by the function predicts</span><br><span class="line">table(iris1[,imp.pred] &lt; best.impur, iris1$Species)</span><br><span class="line">#perfectly predicted in training set</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pred.by&lt;- as.factor(ifelse(iris1[,imp.pred] &lt; best.impur, &quot;setosa&quot;,&quot;versiclor&quot;))</span><br><span class="line"></span><br><span class="line">t1 &lt;- iris1 %&gt;% </span><br><span class="line">  ggplot(aes(x=Petal.Width, y=Petal.Length ,col=Species)) + </span><br><span class="line">  geom_jitter() +</span><br><span class="line">  ggtitle(&quot;Actual&quot;)</span><br><span class="line">t2 &lt;- iris1 %&gt;% </span><br><span class="line">  ggplot(aes(x=Petal.Width, y=Petal.Length ,col=pred.by)) + </span><br><span class="line">  geom_jitter() +</span><br><span class="line">  geom_vline(xintercept =  best.impur, colour=&quot;blue&quot;, linetype=&quot;dashed&quot;) + </span><br><span class="line">  annotate(geom=&quot;text&quot;, label=best.impur, x=best.impur, y=0, vjust=-1) +</span><br><span class="line">  ggtitle(&quot;Predicted&quot;)</span><br><span class="line"></span><br><span class="line">grid.arrange(t1,t2)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/19/DecisionTree/Prediction-1.png" title="[tree1]">

















        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-15T09:07:27.000Z">2019-09-15</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Gaussian-Process/">Gaussian Process</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    12 minutes read (About 1840 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/15/GaussianProcess/">GaussianProcess</a>
            
        </h1>
        <div class="content">
            <h2 id="Introduction-to-Gaussian-Process"><a href="#Introduction-to-Gaussian-Process" class="headerlink" title="Introduction to Gaussian Process"></a>Introduction to Gaussian Process</h2><p>In this post, I will discuss about Gaussian Process, which employs Gaussian distribution (also often called normal distribution that plays an important role in statistics. The previous posts, such as CLT, Cholesky Decomposition, and Schur Complement, are posted because they are needed to understand the Gaussian Process. </p>
<p>Before I start with Gaussian Process, some important properties of Gaussian Distribution will be addressed. </p>
<h3 id="large-Gaussian-large-Distribution"><a href="#large-Gaussian-large-Distribution" class="headerlink" title="$\large{Gaussian}$ $\large{ Distribution}$"></a>$\large{Gaussian}$ $\large{ Distribution}$</h3><p>If we have random vector $X$ following Gaussian Distribution, then we have </p>
<p>$$X = \begin{bmatrix} X_1 \\ X_2 \\ … \\ X_n \end{bmatrix} \sim \mathcal{N}(\mu, \Sigma)$$</p>
<p>where $\Sigma = Cov(X_i, X_j) = E[(X_i - \mu_i)(X_j - \mu_j)^{T}]$</p>
<p>Now, Suppose we have two vectors, $X$ and $Y$ that are subsets of a random variable, Z, with the following; </p>
<p>$$P_Z = P_{X,Y} = \begin{bmatrix} X \\ Y \end{bmatrix} \sim \mathcal{N}(\mu, \Sigma) = \mathcal{N}(\begin{bmatrix} \mu_X \\ \mu_Y \end{bmatrix}, \begin{bmatrix} \Sigma_{XX} &amp; \Sigma_{XY} \\ \Sigma_{YX} &amp; \Sigma_{YY} \end{bmatrix})$$</p>
<p>Then, the following properties holds; </p>
<h3 id="1-Normalization"><a href="#1-Normalization" class="headerlink" title="1. Normalization"></a>1. Normalization</h3><p>The density function will be </p>
<p>$$\int_{Z} p(Z;\mu,\Sigma) dz = 1$$</p>
<p>where $Z = \begin{bmatrix} X \\ Y \end{bmatrix}$</p>
<h3 id="2-Marginalization"><a href="#2-Marginalization" class="headerlink" title="2. Marginalization"></a>2. Marginalization</h3><p>The maginal densities will be</p>
<p>$$p(X) = \int_{Y} p(X,Y; \mu, \Sigma) dy \\ p(Y) = \int_{X} p(X,Y; \mu, \Sigma) dx$$</p>
<p>which are Gaussian, therefore</p>
<p>$$X \sim \mathcal{N}(\mu_X, \Sigma_{XX}) \\ Y \sim \mathcal{N}(\mu_Y, \Sigma_{YY})$$</p>
<h3 id="3-Conditioning"><a href="#3-Conditioning" class="headerlink" title="3. Conditioning"></a>3. Conditioning</h3><p>The conditional densities will also be Gaussian, and the conditional expected value and covariance are calculated with the properties of Schur Complement as the following. </p>
<p>Refer to the wiki, <a href="https://en.wikipedia.org/wiki/Schur_complement" target="_blank" rel="noopener">Schur Complement</a>. </p>
<p>The conditional expectation and covariance of X given Y is the Schur Complement of C in $\Sigma$ where if $\Sigma$ is defined as </p>
<p>$$\Sigma = \begin{bmatrix} A &amp; B \\ B^{T} &amp; C \end{bmatrix}$$ </p>
<p>$$Cov(X|Y) = A-BC^{-1}B^{T} \\ E(X|Y) = E(X) + BC^{-1}(Y-E(Y))$$</p>
<p>Hence, the conditional covariance of X given Y will be</p>
<p>$$P_{X,Y} = \mathcal{N}(\begin{bmatrix} \mu_X \\ \mu_Y \end{bmatrix}, \begin{bmatrix} \Sigma_{XX} &amp; \Sigma_{XY} \\ \Sigma_{YX} &amp; \Sigma_{YY} \end{bmatrix})$$</p>
<p>$$X | Y \sim \mathcal{N}(\mu_X + \Sigma_{XY}\Sigma_{YY}^{-1}(Y-\mu_Y), \Sigma_{XX} - \Sigma_{XY}\Sigma_{YY}^{-1}\Sigma_{YX}) \\<br>Y | X \sim \mathcal{N}(\mu_Y + \Sigma_{YX}\Sigma_{XX}^{-1}(X-\mu_X), \Sigma_{YY} - \Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY})$$</p>
<h3 id="4-Summation"><a href="#4-Summation" class="headerlink" title="4. Summation"></a>4. Summation</h3><p>The sum of independent Gaussian random variables is also Gaussian;</p>
<p>Let $y \sim \mathcal{N}(\mu, \Sigma)$ and $z \sim \mathcal{N}(\mu’,\Sigma’)$</p>
<p>Then, </p>
<p>$$y + z \sim \mathcal{N}(\mu + \mu’, \Sigma+\Sigma’)$$</p>
<p>We see some of important properties of Gaussian distribution. </p>
<p>Now, let’s change our focus to linear regression problem. </p>
<p>Typical linear regression equation is defined as, </p>
<p>$$y = ax + b$$</p>
<p>Or, </p>
<p>$$y = \beta_0 + \beta_1X_1 + … + \beta_pX_p + \epsilon$$</p>
<p>And, it can be expressed by</p>
<p>$$y = f(x) + \epsilon$$</p>
<p>where $f(x) = \beta_0 + \beta_1X_1 + … + \beta_pX_p$. </p>
<p>Here, an important assumption of linear regression is that the error term, $\epsilon$ is normally distributed with mean zero.  </p>
<p>If we consider repeated sampling from our population, for large sample sizes, the distribution of the ordinary least squared estimates of the regression coefficients follow a normal distribution by the Central Limit Theorem. </p>
<p>Refer to the previous post, <a href="https://davidkwon91.github.io/2019/09/09/CLT/" target="_blank" rel="noopener">Central Limit Theorem</a></p>
<p>Therefore, the $\epsilon$ follows Gaussian Distribution. </p>
<p>We might then think about what if our $f(x)$ follows Gaussian distribution. </p>
<p>Then, in the above equation, $y = f(x) + \epsilon$ will follow Gaussian distribution by the property that the sum of two independent Gaussian distribution is Gaussian distribution. </p>
<p>Here is the start of the Gaussian Process. </p>
<h2 id="large-Gaussian-large-Process"><a href="#large-Gaussian-large-Process" class="headerlink" title="$\large{Gaussian}$ $\large{ Process}$"></a>$\large{Gaussian}$ $\large{ Process}$</h2><p>A Gaussian Process is one of stochastic process that is a collection of random variables, ${f(x) : x \in \mathcal{X}}$, indexed by elements from some set $\mathcal{X}$, known as the index set, and GP is such stochastic process that any finite subcollection of random variables has a multivariate Gaussian distribution. </p>
<p>Here, we assume the $f(x)$ follows Gaussian distribution with some $\mu$ and $\Sigma$. </p>
<p>The $\mu$ and $\Sigma$ are called as “mean function” and “covariance function” with the notation, </p>
<p>$$f(.) \sim \mathcal{GP}(m(.), k(.,.))$$</p>
<p>where $k(.,.)$ is covariance function and a kernel function. </p>
<p>Hence, we have the equation, </p>
<p>$$y^{i} = f(x^{i}) + \epsilon^{i}$$</p>
<p>where the error term (“noise”), $\epsilon^{i}$ with independent $\mathcal{N}(0, \sigma^2)$ distribution. </p>
<p>We assume the prior distribution over the function $f(.)$ with mean zero Gaussian; </p>
<p>$$f(.) \sim \mathcal{GP}(0, k(.,.))$$</p>
<p>for some valid kernel function $k(.,.)$. </p>
<p>Now, we can convert the $\mathcal{GP}$ prior $f(.)$ into a $\mathcal{GP}$ posterior function after having some data to make predictions $f_p$. </p>
<p>Then, the two functions for the training ($f(.)$) and test ($f_p(.)$), will follows</p>
<p>$$\begin{bmatrix} f \\ f_p \end{bmatrix}|X,X_p \sim \mathcal{N}\big(0, \begin{bmatrix} K(X,X) &amp; K(X, X_p)  \\  K(X_p, X) &amp; K(X_p, X_p) \end{bmatrix}\big)$$</p>
<p>Now, with $\epsilon$ and $\epsilon_p$, </p>
<p>$$\begin{bmatrix} \epsilon \\ \epsilon_p\end{bmatrix} \sim \mathcal{N}(0,\begin{bmatrix} \sigma^2I &amp; 0 \\ 0^{T} &amp; \sigma^2I\end{bmatrix})$$</p>
<p>As mentioned above, the sum of two independent Gaussian distribution is also Gaussian, then eventually we have,</p>
<p>$$\begin{bmatrix} y \\ y_p \end{bmatrix} | X, X_p = \begin{bmatrix} f \\ f_p \end{bmatrix} + \begin{bmatrix} \epsilon \\ \epsilon_p\end{bmatrix} \sim \mathcal{N}(0, \begin{bmatrix} K(X,X) + \sigma^2I &amp; K(X,X_p) \\ K(X_p, X) &amp; K(X_p,X_p)+\sigma^2I \end{bmatrix})$$</p>
<p>Here, we generate the prior distributions, which are multivariate normal distributions, with standard multivariate normal distribution and the Cholesky factor as the following. </p>
<p>Refer to my previous post for Cholesky Decomposition, <a href="https://davidkwon91.github.io/tags/CholeskyDecomposition/" target="_blank" rel="noopener">Cholesky Decomposition post</a>. </p>
<p>1) Compute covariance function, $\Sigma$, and Cholesky Decomposition for $\Sigma = LL^{T}$</p>
<p>2) Generate standard multivariate normal distribution, $u \sim \mathcal{N}(0,1)$</p>
<p>3) Compute $x = \mu + Lu$, then $x$ will the prior distribution for $\mathcal{GP}$.  </p>
<p>Back to our prediction with GP, we have </p>
<p>$$y_p|y,X,X_p \sim \mathcal{N}(\mu_p, \Sigma_p)$$ </p>
<p>by the rules for conditioning Gaussians as we have done above. </p>
<p>Then, we have </p>
<p>$$\mu_p = K(X_p, X)K((X,X)+\sigma^2I)^{-1}y \\ \Sigma_p = K(X_p, X_p) + \sigma^2I - K(X_p,X)(K(X,X)+\sigma^2I)^{-1}K(X,X_p)$$</p>
<p>Here, the conditional mean and covariance function are calculated by the Schur Complement as shown above, and since the mean is zero vector, the mean function is defined as the above. </p>
<p>Conditional expectation and covariance with Schur Complement again, </p>
<p>$$\Sigma = \begin{bmatrix} A &amp; B \\ B^{T} &amp; C \end{bmatrix}$$ </p>
<p>$$Cov(X|Y) = A-BC^{-1}B^{T} \\ E(X|Y) = E(X) + BC^{-1}(Y-E(Y))$$</p>
<p>Here, the $E(X)$ and $E(Y) = 0$ for $\mathcal{GP}$, thus we have just $BC^{-1}Y$ term, which is $\mu_p = K(X_p, X)K((X,X)+\sigma^2I)^{-1}y$</p>
<h3 id="Kernel-Function-K"><a href="#Kernel-Function-K" class="headerlink" title="Kernel Function, $K(.,.)$?"></a>Kernel Function, $K(.,.)$?</h3><p>Commonly used kernel function for $\mathcal{GP}$ is <em>squared exponential kernel function</em>, which is often called Gaussian kernel. </p>
<p>It’s defined as, </p>
<p>$$K(X,X_p) = exp(-\frac{1}{2\nu} ||X-X_p||^2) = exp(-\frac{1}{2\nu} (X-X_p)^{T}(X-X_p))$$</p>
<p>where the $\nu$ is the free parameter for bandwidth of the function smoothness. </p>
<p>Here is the implementation of Gaussian Process in R. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">xcor &lt;- seq(-5,4.9,by=0.1) #index or coordinate for our data</span><br><span class="line"></span><br><span class="line">#Kernel Function (Squared exponential kernel function or Gaussian kernel function)</span><br><span class="line">GP.kernel &lt;- function(x1,x2,l)&#123;</span><br><span class="line">  mat &lt;- matrix(rep(0, length(x1)*length(x2)), nrow=length(x1))</span><br><span class="line">  for (i in 1:length(x1)) &#123;</span><br><span class="line">    for (j in 1:length(x2)) &#123;</span><br><span class="line">      mat[i,j] &lt;- (x1[i]-x2[j])^2</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  kern &lt;- exp(-(0.5*mat)/l) #l is free parameter that can be seen as bandwidth</span><br><span class="line">  return(kern)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Zero-mean and covariance function by the squared exponential kernel function with the index to create prior distribution</span><br><span class="line">mu &lt;- rep(0,length(xcor))</span><br><span class="line">cov &lt;- GP.kernel(xcor,xcor,1) #nu is 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#3 prior distributions</span><br><span class="line">dat &lt;- data.frame(x=xcor, first=mvrnorm(1,mu,cov), sec=mvrnorm(1,mu,cov), thi=mvrnorm(1,mu,cov))</span><br><span class="line">#mvrnorm function is same with the using of Cholesky factor of the covariance function multiplied by the standard multivariate normal distribution added mu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#graph for the 3 priors</span><br><span class="line">dat %&gt;% ggplot() + </span><br><span class="line">  geom_line(aes(x=xcor,y=first, colour=&quot;red&quot;)) + </span><br><span class="line">  geom_line(aes(x=xcor,y=sec, colour=&quot;blue&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=thi, colour=&quot;green&quot;))</span><br></pre></td></tr></table></figure>

<img src="/2019/09/15/GaussianProcess/GP-1.png" title="[GP]">


<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#Assume we have a data</span><br><span class="line"></span><br><span class="line">#5 elements</span><br><span class="line">f &lt;- data.frame(x=c(-4,-3,-1,0,2),</span><br><span class="line">                y=c(-2,0,1,2,-1))</span><br><span class="line">x &lt;- f$x</span><br><span class="line">k.xx &lt;- GP.kernel(x,x,1)</span><br><span class="line">k.xxs &lt;- GP.kernel(x,xcor,1)</span><br><span class="line">k.xsxs &lt;- GP.kernel(xcor,xcor,1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f.star.bar &lt;- t(k.xxs)%*%solve(k.xx)%*%f$y #mean function</span><br><span class="line">cov.f.star &lt;- k.xsxs - t(k.xxs)%*%solve(k.xx)%*%k.xxs #covariance function</span><br><span class="line"></span><br><span class="line">#posterior distribution</span><br><span class="line">dat1 &lt;- data.frame(x=xcor, </span><br><span class="line">                  first=mvrnorm(1,f.star.bar,cov.f.star), </span><br><span class="line">                  sec=mvrnorm(1,f.star.bar,cov.f.star), </span><br><span class="line">                  thi=mvrnorm(1,f.star.bar,cov.f.star))</span><br><span class="line"></span><br><span class="line">#graph for 3 posterior distribution when nu is 1</span><br><span class="line">dat1 %&gt;% ggplot() + </span><br><span class="line">  geom_line(aes(x=xcor,y=first, colour=&quot;red&quot;)) + </span><br><span class="line">  geom_line(aes(x=xcor,y=sec, colour=&quot;blue&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=thi, colour=&quot;green&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=f.star.bar),colour=&quot;black&quot;)+</span><br><span class="line">  geom_errorbar(data=f, aes(x=x, y=NULL, ymin=y-2*0.1, ymax=y+2*0.1),width=0.2)+</span><br><span class="line">  geom_point(data=f,aes(x=x,y=y)) +</span><br><span class="line">  ggtitle(&quot;Posterior from Prior x likelihood/evidence (given x,x*,y)&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/15/GaussianProcess/GP-2.png" title="[GP]">



<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mu &lt;- rep(0,length(xcor))</span><br><span class="line">cov &lt;- GP.kernel(xcor,xcor,0.1) #nu is 0.1</span><br><span class="line"></span><br><span class="line">dat &lt;- data.frame(x=xcor, first=mvrnorm(1,mu,cov), sec=mvrnorm(1,mu,cov), thi=mvrnorm(1,mu,cov))</span><br><span class="line"></span><br><span class="line">dat %&gt;% ggplot() + </span><br><span class="line">  geom_line(aes(x=xcor,y=first, colour=&quot;red&quot;)) + </span><br><span class="line">  geom_line(aes(x=xcor,y=sec, colour=&quot;blue&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=thi, colour=&quot;green&quot;)) +</span><br><span class="line">  ggtitle(&quot;When sigma is 0.1&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mu &lt;- rep(0,length(xcor))</span><br><span class="line">cov &lt;- GP.kernel(xcor,xcor,10) #nu is 10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dat &lt;- data.frame(x=xcor, first=mvrnorm(1,mu,cov), sec=mvrnorm(1,mu,cov), thi=mvrnorm(1,mu,cov))</span><br><span class="line"></span><br><span class="line">dat %&gt;% ggplot() + </span><br><span class="line">  geom_line(aes(x=xcor,y=first, colour=&quot;red&quot;)) + </span><br><span class="line">  geom_line(aes(x=xcor,y=sec, colour=&quot;blue&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=thi, colour=&quot;green&quot;)) +</span><br><span class="line">  ggtitle(&quot;When sigma is 10&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/15/GaussianProcess/GP-3.png" title="[GP]">
<img src="/2019/09/15/GaussianProcess/GP-5.png" title="[GP]">









<p>Reference: </p>
<p><a href="http://krasserm.github.io/2018/03/19/gaussian-processes/" target="_blank" rel="noopener">Gaussian Process</a><br><a href="http://i-systems.github.io/HSE545/machine%20learning%20all/14%20Gaussian%20Process%20Regression/reference_files/cs229-gaussian_processes.pdf" target="_blank" rel="noopener">Gaussian Process</a><br><a href="https://distill.pub/2019/visual-exploration-gaussian-processes/#DimensionSwap" target="_blank" rel="noopener">A Visual Exploration of Gaussian Process</a><br><a href="http://www0.cs.ucl.ac.uk/staff/J.Shawe-Taylor/courses/ATML-1.pdf" target="_blank" rel="noopener">Gaussian Process: A Basic Properties and GP regression</a><br><a href="https://www.r-bloggers.com/gaussian-process-regression-with-r/" target="_blank" rel="noopener">Gaussian Process regression with R</a></p>
<p>and my previous posts,</p>
<p><a href="https://davidkwon91.github.io/tags/CholeskyDecomposition/" target="_blank" rel="noopener">Cholesky Decomposition</a><br><a href="https://davidkwon91.github.io/tags/CentralLimitTheorem/" target="_blank" rel="noopener">Central Limit Theorem</a><br><a href="https://davidkwon91.github.io/tags/CentralLimitTheorem/" target="_blank" rel="noopener">Schur Complement</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-14T21:01:18.000Z">2019-09-15</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/Today/">Today</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/Today/September/">September</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    a few seconds read (About 87 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/15/No-Leaders-Please/">No Leaders Please</a>
            
        </h1>
        <div class="content">
            <p>In the great poem by Charles Bukowski,</p>
<p>“No Leaders Please”</p>
<p>Invent yourself, reinvent yourself.</p>
<p>Don’t swim in the same slough.</p>
<p>Invent yourself, reinvent yourself, </p>
<p>and stay out of the clutches of mediocrity. </p>
<p>Change your tone and shape so often that they can never categorize you. </p>
<p>Reinvigorate yourself, and accept what is. </p>
<p>But only on the terms that you have invented and reinvented. </p>
<p>Be self-taught.</p>
<p>And reinvent your life.</p>
<p>Because your must. </p>
<p>It is your life<br>and its history<br>and the present<br>belong only to you. </p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-09T09:49:56.000Z">2019-09-09</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/Math/">Math</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/Math/Histogram/">Histogram</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    2 minutes read (About 331 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/09/hist/">Histogram</a>
            
        </h1>
        <div class="content">
            <h2 id="Creating-Histogram-function-using-Min-max-transformation"><a href="#Creating-Histogram-function-using-Min-max-transformation" class="headerlink" title="Creating Histogram function using Min-max transformation."></a>Creating Histogram function using Min-max transformation.</h2><p>The histogram plot of a vector or a data feature is to create bins, which is to create a series of interval, for the range of data values, and to count how many data values fall into each bins. </p>
<p>I create bins as the following;</p>
<p>Suppose we have $M$ bins, then</p>
<p>$$B_1 = [0,\frac{1}{M}), B_2 = [\frac{1}{M}, \frac{2}{M}), …, B_{M-1} = [\frac{M-2}{M}, \frac{M-1}{M}), B_{M} = [\frac{M-1}{M}, 1)$$</p>
<p>To create histogram function with the bins, I wanted to transform the data elements in interval $(0,1)$, so I can put them into each bins. </p>
<p>That’s why I used Min-max transformation, which makes the data reducing to a scale between 0 and 1. </p>
<p>Min-max transformation is the following formula;</p>
<p>$$z = \frac{x-min(x)}{max(x)-min(x)}$$</p>
<p>The below codes are the implementation of creating histogram plot in R. I used the values from CLT posts. </p>
<p><a href="https://davidkwon91.github.io/2019/09/09/CLT/" target="_blank" rel="noopener">CLT link</a></p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Hist &lt;- function(vec,bin)&#123;</span><br><span class="line">  vec.minmax &lt;- (vec - min(vec))/(max(vec)-min(vec)) #min-max transformation of the vector or the values</span><br><span class="line">  </span><br><span class="line">  vec.bins &lt;- rep(0,bin)</span><br><span class="line">  for(i in 1:bin)&#123;</span><br><span class="line">  	#find the values that is the closest to the value of each boundary of the bins</span><br><span class="line">    vec.bins[i] &lt;- vec[which(abs(vec.minmax - i/bin) == min(abs(vec.minmax - i/bin)))]</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  dat &lt;- data.frame(x=vec.bins, freq=0)</span><br><span class="line">  for(i in 1:bin)&#123;</span><br><span class="line">  	#put the values into each bins associateed</span><br><span class="line">    dat[i,2] &lt;- length(which(vec.minmax &gt; (i-1)/bin &amp; vec.minmax &lt;= i/bin))</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  #plotting</span><br><span class="line">  p &lt;- dat %&gt;% ggplot(aes(x=x, y=freq)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge(width=0.5)) + theme_bw()</span><br><span class="line"></span><br><span class="line">  return(p)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hist(sampled.1000,15)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/hist/histrogram-1.png" title="[hist]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hist(sampled.10000,15)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/hist/histrogram-2.png" title="[hist]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hist(bin.sampled.10000,15)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/hist/histrogram-3.png" title="[hist]">






<p>Reference:<br><a href="http://faculty.washington.edu/yenchic/18W_425/Lec6_hist_KDE.pdf" target="_blank" rel="noopener">Histogram and Kernel Density EStimation</a><br><a href="https://medium.com/@shirleyliu/histograms-from-scratch-482dba2a4e31" target="_blank" rel="noopener">Histogram from Scratch</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-09T07:08:11.000Z">2019-09-09</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/Math/">Math</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/Math/CLT/">CLT</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    4 minutes read (About 591 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/09/CLT/">CLT</a>
            
        </h1>
        <div class="content">
            <p>This post will discuss Central Limit Theorem.<br>Central Limit Theorem is one of the most important topic in statistics, especially in probability theory. </p>
<h3 id="large-Definition"><a href="#large-Definition" class="headerlink" title="$\large{Definition}$:"></a>$\large{Definition}$:</h3><p>The sample average of independent and identically distributed random variables drawn from an unknown distribution with mean $\mu$ and variance $\sigma$ is approximately normal distributed when $n$ gets larger. That is, by the law of large numbers, the sample mean converges in probability and almost surely to the expected value $\mu$ as $n \to \infty$.</p>
<p>Formally, let ${X_1,X_2,…X_n}$ be random samples of size $n$. Then, the sample average is defined as</p>
<p>$$S_n = \frac{X_1 + X_2 + … + X_n}{n}$$ with mean $\mu$ and variance $\sigma^2$</p>
<p>Then,<br>$$\sqrt{n}(S_n-\mu) \sim \mathcal{N}(0,\sigma^2)$$</p>
<p>The normalized random mean variable will be</p>
<p>$$Z_n = \frac{S_n-\mu}{\frac{\sigma}{\sqrt{n}}} \sim \mathcal{N}(0,1)$$</p>
<p>The below part will be implementation of CLT in R. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set.seed(1004)</span><br><span class="line">x1 &lt;- runif(10000, min=0,max=1000) #Generating random variables drawn from unifrom distribution with (0,1000)</span><br><span class="line"></span><br><span class="line">hist(x1) #Histogram</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/CLT/CLT-1.png" title="[hist]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sampled.5 &lt;- rep(0, length(x1))</span><br><span class="line"></span><br><span class="line">for(i in 1:length(x1))&#123;</span><br><span class="line">  sampled.5[i] &lt;- mean(sample(x1, 5, replace=TRUE)) #sample average with size 5</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">hist(sampled.5)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/CLT/CLT-3.png" title="[hist]">


<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">plot(density(sampled.5)) #density plot of sample average variables</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/CLT/CLT-4.png" title="[hist]">


<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sampled.30 &lt;- rep(0, length(x1))</span><br><span class="line">sampled.1000 &lt;- rep(0,length(x1))</span><br><span class="line">sampled.10000 &lt;- rep(0,length(x1))</span><br><span class="line"></span><br><span class="line">for(i in 1:length(x1))&#123;</span><br><span class="line">  sampled.30[i] &lt;- mean(sample(x1, 30, replace=TRUE)) #sample average of size 30</span><br><span class="line">  sampled.1000[i] &lt;- mean(sample(x1, 1000, replace=TRUE)) #sample average of size 1000</span><br><span class="line">  sampled.10000[i] &lt;- mean(sample(x1,10000,replace=TRUE)) #sample average of size 10000</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>$$\sqrt{n}(S_n-\mu) \sim \mathcal{N}(0,\sigma^2)$$</p>
<p>The following code is the above formula. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqrt(length(sampled.30))*(mean(sampled.30)-mean(x1))</span><br><span class="line"></span><br><span class="line">## [1] -39.36105</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqrt(length(sampled.1000))*(mean(sampled.1000)-mean(x1))</span><br><span class="line"></span><br><span class="line">## [1] 0.1330039</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqrt(length(sampled.10000))*(mean(sampled.10000)-mean(x1))</span><br><span class="line"></span><br><span class="line">## [1] 0.003917203</span><br></pre></td></tr></table></figure>

<p>It shows the $\sqrt{n}(S_n-\mu)$ tends to go to zero as the size of the sample increases, which means that the expected value of the sample average variables gets closer to the expected value of the random variables when $n$ gets larger. </p>
<p>Let’s see other example for random sample average drawn from Binomial distribution. </p>
<p>Suppose we have 10000 random Binomial variables with $p = 0.5$. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#Binomial</span><br><span class="line">n &lt;- 10000</span><br><span class="line">p &lt;- 1/2</span><br><span class="line">B &lt;- rbinom(n,1,p)</span><br></pre></td></tr></table></figure>

<p>The mean of Binomial distribution is $p$, that is $E(X) = p$.<br>The variance of Binomial distribution is $p(1-p)$, that is $Var(X) = p(1-p)$</p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">p</span><br><span class="line"></span><br><span class="line">## [1] 0.5</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">p*(1-p)</span><br><span class="line"></span><br><span class="line">## [1] 0.25</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mean(B)</span><br><span class="line"></span><br><span class="line">## [1] 0.4991</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var(B)</span><br><span class="line"></span><br><span class="line">## [1] 0.2500242</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#creating random sample average from Binomial random variables</span><br><span class="line">bin.sampled.30 &lt;- rep(0, length(B))</span><br><span class="line">bin.sampled.1000 &lt;- rep(0,length(B))</span><br><span class="line">bin.sampled.10000 &lt;- rep(0,length(B))</span><br><span class="line"></span><br><span class="line">for(i in 1:length(x1))&#123;</span><br><span class="line">  bin.sampled.30[i] &lt;- mean(sample(B, 30, replace=TRUE))</span><br><span class="line">  bin.sampled.1000[i] &lt;- mean(sample(B, 1000, replace=TRUE))</span><br><span class="line">  bin.sampled.10000[i] &lt;- mean(sample(B,10000,replace=TRUE))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(density(bin.sampled.30))</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/CLT/CLT-14.png" title="[hist]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(density(bin.sampled.1000))</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/CLT/CLT-15.png" title="[hist]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(density(bin.sampled.10000))</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/CLT/CLT-16.png" title="[hist]">



<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqrt(length(bin.sampled.30))*(mean(bin.sampled.30)-mean(B))</span><br><span class="line"></span><br><span class="line">## [1] 0.009333333</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqrt(length(bin.sampled.1000))*(mean(bin.sampled.1000)-mean(B))</span><br><span class="line"></span><br><span class="line">## [1] 0.01169</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqrt(length(bin.sampled.10000))*(mean(bin.sampled.10000)-mean(B))</span><br><span class="line"></span><br><span class="line">## [1] -0.000371</span><br></pre></td></tr></table></figure>

<p>These results shows that no matter what the distribution of the population is, the sample average drawn from the distribution will be approximately normal distribution as $n$ gets large with mean $\mu$.</p>
<p>Reference:<br><a href="https://www.probabilitycourse.com/chapter7/7_1_2_central_limit_theorem.php" target="_blank" rel="noopener">Central Limit Theorem</a><br><a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank" rel="noopener">Central Limit Theorem (wiki)</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-06T12:19:33.000Z">2019-09-06</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/Today/">Today</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/Today/September/">September</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    3 minutes read (About 397 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/06/190906/">190906</a>
            
        </h1>
        <div class="content">
            <p>September 06th, 2019</p>
<p>Today’s work</p>
<ol>
<li>Self-study for Gaussian Process <ul>
<li>Multivariate Normal Distribution</li>
<li>Cholesky Decomposition - Generating Multivariate Normal Random Number</li>
<li>Bayesian Optimization (Fully understanding this is final goal)</li>
</ul>
</li>
</ol>
<p>Since I am going to tune hyperparameter of GBM like LightGBM or Catboost for the Kaggle Competition that I am now in, I’d like to fully understand how the Bayesian Optimization. While studying these, I have been learning more detail of matrix decomposition and Gaussian Distrubiton. I’ve also been realizing that how important they are in machine learning. </p>
<ol start="2">
<li>New Posts<ul>
<li>Schur Complement</li>
<li>Cholesky Decomposition</li>
</ul>
</li>
</ol>
<p>A hard one for these posts is that I wasn’t sure how deeply I should study for them. Both of Schur Complement and Cholesky Decomposition are widely and significantly used in statistics. </p>
<p>2019년 9월 6일</p>
<p>오늘 할 일</p>
<ol>
<li>Gaussian Process 셀프 스터디<ul>
<li>Multivariate Normal Distribution</li>
<li>Cholesky Decomposition - Generating Multivariate Normal Random Number</li>
<li>Bayesian Optimization (Fully understanding this is final goal)</li>
</ul>
</li>
</ol>
<p>Gaussian Process 를 공부하게 된 계기는 현재 참여하고 있는 Kaggle 대회에서 LightGBM 이나 Catboost 알고리즘을 사용하려 하는데, 여기서 Hyperparameter tuning을 Random Search 나 Grid Search 가 아닌 Bayesian Optimization을 사용하기에 공부하기 시작했다.<br>공부하다보니 가장 기본이 되는 Matrix Decomposition, Gaussian Distribution등에 대해 더 자세히 배우고 있으며, 이들이 머신러닝에서 얼마나 많이 사용되며 중요한지 배우게 되었다. </p>
<ol start="2">
<li>새로운 포스팅<ul>
<li>Schur Complement</li>
<li>Cholesky Decomposition</li>
</ul>
</li>
</ol>
<p>이번 포스팅중 어려웠던 점은 Schur Complement과 Cholesky Decomposition은 Matrix 를 다루는 분석 혹 연구에서 매우 중요하게, 그리고 많이 쓰이기에 그 개념들을 얼마나 깊게 공부해야 하는지였다. 결국 다른 공부를 할때에도 그 개념들은 이어질테니 그때 다시 더 깊게 공부해보고 싶다. </p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-06T10:18:13.000Z">2019-09-06</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/Math/">Math</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/Math/Cholesky-Decomposition/">Cholesky Decomposition</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    3 minutes read (About 416 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/06/Cholesky/">Cholesky Decomposition</a>
            
        </h1>
        <div class="content">
            <h1 id="Introduction-to-the-Cholesky-Decomposition"><a href="#Introduction-to-the-Cholesky-Decomposition" class="headerlink" title="Introduction to the Cholesky Decomposition."></a>Introduction to the Cholesky Decomposition.</h1><p>Cholesky Decomposition is used for its superior efficiency in linear calculation, such as affine function, $Ax = b$. Among many applications of Cholesky Decomposition, I will discuss about how the multivariate normal random numbers are generated with Cholesky Decomposition. </p>
<h2 id="large-Definition"><a href="#large-Definition" class="headerlink" title="$\large{Definition}$:"></a>$\large{Definition}$:</h2><p>Every positive definite matrix $A \in \mathcal{R}^{n \times n}$, </p>
<p>The Cholesky Decomposition is a form of </p>
<p>$$A = LL^{T}$$</p>
<p>where $L$ is the lower triangular matrix with real and positive diagonal elements. $L$ is called Cholesky factor of $A$ and it can be interpreted as the square root of a positive definite matrix. </p>
<h2 id="large-Algorithm"><a href="#large-Algorithm" class="headerlink" title="$\large{Algorithm}$:"></a>$\large{Algorithm}$:</h2><p>This algorithm is used in this <a href="https://aaronschlegel.me/cholesky-decomposition-r-example.html" target="_blank" rel="noopener">link</a>, and there are many apporaches to decompose a matrix with Cholesky Decomposition. </p>
<ol>
<li>Compute $L_{1} = \sqrt{a_{11}}$</li>
<li>For $k = 2, … ,n$, find $L_{k-1}l_{k} = a_{k} for l_{k}$</li>
<li>$l_{kk} = \sqrt{a_{kk}-l_{k}^{T}l_{k}}$</li>
<li>$L_{k} = \begin{bmatrix} L_{k-1} &amp; 0 \\ l_{k}^{T} &amp; l_{kk} \end{bmatrix}$</li>
</ol>
<p>then $L_{k}$ is the lower triangular matrix of Cholesky Decomposition. </p>
<p>Other approaches; </p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Proof_for_positive_semi-definite_matrices" target="_blank" rel="noopener">Cholesky Decomposition</a></li>
<li><a href="http://www.seas.ucla.edu/~vandenbe/133A/lectures/chol.pdf" target="_blank" rel="noopener">link</a></li>
</ul>
<p>$\large{Generating}$ $\large{Multivariate}$ $\large{Normal}$ $\large{Random}$ $\large{Number}$ $\large{with}$ $\large{Cholesky}$ $\large{Decomposition}$</p>
<p>If we have $X$ that follows Normal Distribution, </p>
<p>then</p>
<p>$$X \sim \mathcal{N}(\mu, \Sigma)$$</p>
<p>Let $Z$ follows standard normal distribution with mean 0 and variance 1.</p>
<p>Then,</p>
<p>$$Z \sim \mathcal{N}(0,I)$$</p>
<p>Then, we can express the $X$ with $Z$ as the following; </p>
<p>$$X = A+BZ$$</p>
<p>then $X$ will follow normal distribution as the following; </p>
<p>$$X \sim \mathcal{N}(A,BB’)$$</p>
<p>Here, $\mu$ is the $A$, and $\Sigma$ is the $BB’$. The $B$ is the lower triangular matrix of the decomposed $X$, which is the Cholesky factor. </p>
<p>Therefore, If we have a Cholesky factor, $B$, of covariance matrix of $X$, then the product of $B$ and standard normal random number matrix will generate the multivariate normal random number associated with the mean $\mu$ and covariance $\Sigma$ of $X$. </p>
<p>Reference:</p>
<p><a href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Proof_for_positive_semi-definite_matrices" target="_blank" rel="noopener">Cholesky Decomposition</a><br><a href="http://www.seas.ucla.edu/~vandenbe/133A/lectures/chol.pdf" target="_blank" rel="noopener">Cholesky Factorization</a><br><a href="https://aaronschlegel.me/cholesky-decomposition-r-example.html" target="_blank" rel="noopener">Cholesky Decomposition with R example</a><br><a href="http://rinterested.github.io/statistics/multivariate_normal_draws.html" target="_blank" rel="noopener">link</a><br><a href="https://www2.stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec22.pdf" target="_blank" rel="noopener">Bivariate Normal Distribution</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-03T03:03:56.000Z">2019-09-03</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/Math/">Math</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/Math/Schur-Complement/">Schur Complement</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    4 minutes read (About 651 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/03/Matrix Basic/">Schur Complement</a>
            
        </h1>
        <div class="content">
            <p>Brief introduction to Schur Complement. </p>
<p>This is just a brief introduction of the properties of Schur Complement. Schur Complement has many applications in numerical analysis, such as optimization, machine learning algorithms, or probability and matrix theories. </p>
<p>Let’s begin with the definition of the Schur Complement. </p>
<p>$\large{Definition}$: </p>
<p>The Schur Complement of a black matrix is defined as;</p>
<p>Let $M$ be $n \times n$ matrix</p>
<p>$$M =  \begin{bmatrix} A &amp; B \\ C &amp; D \end{bmatrix}$$</p>
<p>Then, the Schur Complement of the block $D$ of the matrix $M$ is </p>
<p>$$M/D := A - BD^{-1}C$$</p>
<p>if $D$ is invertible. </p>
<p>The Schur Complement of the block $A$ of the matrix $M$ is then,</p>
<p>$$M/A := D - CA^{-1}B$$</p>
<p>if $A$ is invertible. </p>
<p>$$\large{Properties}$$</p>
<ul>
<li>Linear system</li>
</ul>
<p>Let<br>$A$ is a $p \times p$ matrix,<br>$D$ is a $q \times q$ matrix, with $n = p + q$<br>So, $B$ is a $p \times q$ matrix. </p>
<p>Let $M$ be defined as,</p>
<p>$$M = \begin{bmatrix} A &amp; B \\ B^{T} &amp; D \end{bmatrix}$$</p>
<p>Suppose we have a linear system as,</p>
<p>$$Ax + By = c \\ B^{T}x + Dy = d$$</p>
<p>Then, </p>
<p>$$\begin{bmatrix} A &amp; B \\ B^{T} &amp; D \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} c \\ d \end{bmatrix}$$</p>
<p>Assuming $D$ is invertible, </p>
<p>then we have $y$ as</p>
<p>$$y = D^{-1}(d - B^{T}x)$$ </p>
<p>Then, from the equation, </p>
<p>$$Ax + By = c \\ Ax + B(D^{-1}(d - B^{T}x)) = c$$</p>
<p>Then, we see that is,</p>
<p>$$(A - BD^{-1}B^{T})x = c - BD^{-1}d$$</p>
<p>Here, the $A - BD^{-1}B^{T}$ is the Schur Complement of a block $D$ of matrix $M$. </p>
<p>It follows, </p>
<p>$$x = (A-BD^{-1}B^{T})^{-1}(c-BD^{-1}d) \\ y = D^{-1}(d-B^{T}(A-BD^{-1}B^{T})^{-1}(c-BD^{-1}d))$$</p>
<p>And, </p>
<p>$$x = (A-BD^{-1}B^{T})^{-1}c - (A-BD^{-1}B^{T})^{-1}BD^{-1}d \\ y = -D^{-1}B^{T}(A-BD^{-1}B^{T})^{-1}c + (D^{-1}+D^{-1}B^{T}(A-BD^{-1}B^{T})^{-1}BD^{-1})d$$</p>
<p>The $x$ and $y$ are formed as linear functions associated with $c$ and $d$, </p>
<p>and it can be a formula for an inverse of $M$ in terms of the Schur Complement of $D$ in $M$.</p>
<p>$$\begin{bmatrix} A &amp; B \\ B^{T} &amp; D \end{bmatrix} ^{-1} = \begin{bmatrix} (A-BD^{-1}B^{T})^{-1} &amp; -(A-BD^{-1}B^{T})^{-1}BD^{-1} \\ -D^{-1}B^{T}(A-BD^{-1}B^{T})^{-1} &amp; D^{-1}+D^{-1}B^{T}(A-BD^{-1}B^{T})^{-1}BD^{-1}  \end{bmatrix}$$</p>
<p>This can be written as,</p>
<p>$$\begin{bmatrix} A &amp; B \\ B^{T} &amp; D \end{bmatrix} ^{-1} = \begin{bmatrix} (A-BD^{-1}B^{T})^{-1} &amp; 0 \\ -D^{-1}B^{T}(A-BD^{-1}B^{T})^{-1} &amp; D^{-1} \end{bmatrix} \begin{bmatrix} I &amp; -BD^{-1} \\ 0 &amp; I \end{bmatrix}$$</p>
<p>Eventually, </p>
<p>$$\begin{bmatrix} A &amp; B \\ B^{T} &amp; D \end{bmatrix} ^{-1} = \begin{bmatrix} I &amp; 0 \\ -D^{-1}B^{T} &amp; I \end{bmatrix} \begin{bmatrix} (A-BD^{-1}B^{T})^{-1} &amp; 0 \\ 0 &amp; D^{-1} \end{bmatrix} \begin{bmatrix} I &amp; -BD^{-1} \\ 0 &amp; I \end{bmatrix}$$</p>
<p>If we take inverse on both sides,</p>
<p>$$\begin{bmatrix} A &amp; B \\ B^{T} &amp; D \end{bmatrix} = \begin{bmatrix} I &amp; BD^{-1} \\ 0 &amp; I \end{bmatrix} \begin{bmatrix} A-BD^{-1}B^{T} &amp; 0 \\ 0 &amp; D \end{bmatrix} \begin{bmatrix} I &amp; 0 \\ D^{-1}B^{T} &amp; I \end{bmatrix}$$</p>
<p>$$= \begin{bmatrix} I &amp; BD^{-1} \\ 0 &amp; I \end{bmatrix} \begin{bmatrix} A-BD^{-1}B^{T} &amp; 0 \\ 0 &amp; D \end{bmatrix} \begin{bmatrix} I &amp; BD^{-1} \\ 0 &amp; I \end{bmatrix} ^{T}$$</p>
<p>This is done by an assumption with $D$ is invertible, and this decomposition is called Block LDU Decomposition. </p>
<p>Then, we can get another factorization of $M$ if A is invertible. </p>
<ul>
<li>Positive Semi-definite</li>
</ul>
<p><em>For any symmetric matrix,</em> $M$,</p>
<p>if D is invertible, then</p>
<p>1) $M \succ 0$   $iff$   $D \succ 0$ <em>and</em>  $A-BC^{-1}B^{T} \succ 0$ </p>
<p>2) <em>If</em> $C \succ 0$, <em>then</em> $M \succeq 0$  $iff$  $A-BC^{-1}B^{T} \succeq 0$</p>
<p>We can easily get the matrix $M$ in the linear combination and the inverse of the $M$ with Schur Complement.<br>We also can check if the $M$ is positive semi-definite.</p>
<p>Reference:</p>
<p><a href="https://en.wikipedia.org/wiki/Schur_complement" target="_blank" rel="noopener">Schur Complement</a><br><a href="http://www.cis.upenn.edu/~jean/schur-comp.pdf" target="_blank" rel="noopener">The Schur Complement and Symmetric Positive Semidefinite (and Definite) Matrices</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-08-29T05:48:09.000Z">2019-08-29</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Logistic-Regression/">Logistic Regression</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    6 minutes read (About 904 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/08/29/logit/">Logit - Logistic Regression</a>
            
        </h1>
        <div class="content">
            <h2 id="Logistic-Regression-from-Logit"><a href="#Logistic-Regression-from-Logit" class="headerlink" title="Logistic Regression from Logit"></a>Logistic Regression from Logit</h2><h3 id="1-What-is-Logit"><a href="#1-What-is-Logit" class="headerlink" title="1. What is Logit?"></a>1. What is Logit?</h3><p>To discuss logit, we need to know what the odds is. </p>
<p>Simply say, Odds is $\frac{p}{1-p}$, where $p$ is the probability of a binary outcome. </p>
<p>Since the odds has different properties than probability, it enables us to make new functions or new graphs. </p>
<p>With the properties of odds, logarithm of odds is popularly used in machine learning industry, which is called logit. </p>
<p>Logit (also log odds) = $log(\frac{p}{1-p})$. </p>
<p>Let logit be defined as $\theta$, then </p>
<p>$$\theta = log(\frac{p}{1-p})$$</p>
<p>We can express this in terms of $p$ as the following,</p>
<p>$$e^{\theta} = \frac{p}{1-p} \\ (1-p)e^{\theta} = p \\ e^{\theta} = p + p e^{\theta} \\ e^{\theta} = p(1+e^{\theta}) \\ \therefore p = \frac{e^{\theta}}{(1+e^{\theta})} = \frac{1}{1+e^{-\theta}}$$</p>
<p>This is the logistic function, which is the special case of sigmoid function that is widely used for classification, such as logistic regression, tree-based algorithm for classification, or deep learning.<br>The logistic function is the CDF of the logistic distribution. </p>
<h3 id="2-Logistic-Distribution"><a href="#2-Logistic-Distribution" class="headerlink" title="2. Logistic Distribution"></a>2. Logistic Distribution</h3><p>A property of the logistic distribution is that the logistic distribution is very similar with the normal distribution except for the kurtosis. </p>
<p>The PDF of the logistic distribution is defined as, </p>
<p>$$f(x; \mu, s) =  \frac{e^{-\frac{(x-\mu)}{s}}}{s(1+e^{-\frac{x-\mu}{s}})^2}$$</p>
<p>The CDF of the logistic distribution is defined as,</p>
<p>$$F(x; \mu, s) = \frac{1}{1+e^{-\frac{x-\mu}{s}}}$$</p>
<p>Some simulations are performed in <a href="https://core.ac.uk/download/pdf/41787448.pdf" target="_blank" rel="noopener">this paper</a>, and the simulation was to measure the absolute deviation between the cumulative standard normal distribution and the cumulative logistic distribution with mean zero and variance one. </p>
<p>By <a href="https://core.ac.uk/download/pdf/41787448.pdf" target="_blank" rel="noopener">the paper</a>, “the cumulative logistic distribution with mean zero and variance one is known as $(1+e^{-\nu x})^{-1}$ “<br>And, the paper also states that the kurtosis of the logistic distribution is effected by the parameter coefficient, $\nu$.<br>With the $\nu = 1.702$, the deviation is 0.0095. </p>
<p>As a summary, the logistic distribution approximately follows the normal distribution with the variance $\sigma^2$, and we could use the property of the normal distribution as the following; </p>
<p>By <a href="https://en.wikipedia.org/wiki/Standard_deviation#targetText=If%20a%20data%20distribution%20is,deviations%20(%CE%BC%20%C2%B1%203%CF%83)." target="_blank" rel="noopener">wikipedia</a>, </p>
<p>“If a data distribution is approximately normal, then about 68 percent of the data values are within one standard deviation of the mean (mathematically, $\mu \pm \sigma$, where $\mu$ is the arithmetic mean), about 95 percent are within two standard deviations ($\mu \pm 2\sigma$), and about 99.7 percent lie within three standard deviations ($\mu \pm 3\sigma$).”</p>
<h3 id="3-Logistic-Regression"><a href="#3-Logistic-Regression" class="headerlink" title="3. Logistic Regression"></a>3. Logistic Regression</h3><p>In a binary logistic regression, the outcome is binary, which can be said that it follows binomial distribution.</p>
<p>Let $y_i$ = number of successes in $m_i$ trials of a binomial process where $i = 1,…,n$, and we have a single predictor, $x_i$.</p>
<p>Then, </p>
<p>$$y_i | x_i \sim Bin(m_i, \theta(x_i))$$</p>
<p>By the definition of binomial distribution,</p>
<p>$$P(Y_i = y_i | x_i) = \binom{m_i}{y_i}\theta(x_i)^{y_i}(1-\theta(x_i))^{m_i-y_i}$$</p>
<p>Then, the loglikelihood function for Logistic Regression will be </p>
<p>$$\mathcal{L} = \prod_{i=1}^{n} P(Y_i = y_i | x_i) \\ = \prod_{i=1}^{n}\binom{m_i}{y_i}\theta(x_i)^{y_i}(1-\theta(x_i))^{m_i-y_i}$$</p>
<p>Here, the $\theta(x_i)$ is defined as</p>
<p>$$\theta(x_i) = \frac{1}{1+e^{-(\beta_0 + \beta_1 x_i)}}$$</p>
<p>And, </p>
<p>$$log(\frac{\theta(x_i)}{1-\theta(x_i)}) = \beta_0 + \beta_1 x_i$$</p>
<p>Just taking the derivative on the log-likelihood function and setting this as zero is hard to solve the problem, so the parameter $\beta_0$ and $\beta_1$ can be estimated by some other optimization method, such as Newton’s method. </p>
<h3 id="4-Wald-Test-in-Logistic-Regression"><a href="#4-Wald-Test-in-Logistic-Regression" class="headerlink" title="4. Wald Test in Logistic Regression"></a>4. Wald Test in Logistic Regression</h3><p>Wald Test uses the property of the logistic distribution. </p>
<p>As mentioned above, the logistic distribution is almost approximately standard normal distribution. </p>
<p>Hence, the Wald test statistic is used to test </p>
<p>$$H_0 : \beta_1 = 0$$ </p>
<p>in Logistic regression. </p>
<p>The Wald test statistic is defined as</p>
<p>$$Z = \frac{\hat{\beta_1} - \beta_1}{estimated \qquad se(\hat{\beta_1})}$$</p>
<p>In the test, $\beta_1 = 0$ as the null hypothesis, and this follows standard normal distribution. </p>
<p>Hence, </p>
<p>$$Z = \frac{\hat{\beta_1}}{\hat{se}(\hat{\beta_1})} \sim \mathcal{N}(0,1)$$</p>
<p>As mentioned above, the logistic distribution is approximately following the normal distribution with variance, $\sigma^2$.<br>Hence, dividing by the standard error for the estimated paramter gives you standard normal distribution, $\mathcal{N}(0,1)$.</p>
<p>We can determine the coefficient significant with the Wald Test for the Logistic Regression. </p>
<h3 id="4-Interpretation-for-coefficients-of-Logistic-Regression"><a href="#4-Interpretation-for-coefficients-of-Logistic-Regression" class="headerlink" title="4. Interpretation for coefficients of Logistic Regression."></a>4. Interpretation for coefficients of Logistic Regression.</h3><p>Unlike Linear Regression, the linear combination of coefficients of Logistic regression is logit. </p>
<p>Hence, we can interpret the coefficient in different way. </p>
<p>For example, with the single predictor as used above, the logit is</p>
<p>$$log(\frac{p}{1-p}) = \beta_0 + \beta_1 x_i$$</p>
<p>If we take exponential both sides, </p>
<p>$$\frac{p}{1-p} = e^{(\beta_0 + \beta_1 x_i)}$$</p>
<p>Then, we can say as following,</p>
<p>“For one-unit in $x_i$ increase, it is expected to see ..% increase in odds of success, $p$”.</p>
<p>Reference:<br><em>A Modern Approach to Regression With R by Simon J. Sheater, Springer</em><br><a href="https://core.ac.uk/download/pdf/41787448.pdf" target="_blank" rel="noopener">A logistic approximation to the cumulative normal distribution</a><br><a href="https://en.wikipedia.org/wiki/Standard_deviation#targetText=If%20a%20data%20distribution%20is,deviations%20(%CE%BC%20%C2%B1%203%CF%83)." target="_blank" rel="noopener">Standard Deviation</a><br><a href="https://en.wikipedia.org/wiki/Logistic_distribution" target="_blank" rel="noopener">Logistic Distribution</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-08-29T03:50:08.000Z">2019-08-29</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Likelihood/">Likelihood</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    7 minutes read (About 1026 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/08/29/Likelihood/">Likelihood</a>
            
        </h1>
        <div class="content">
            <h2 id="Likelihood-vs-Probability"><a href="#Likelihood-vs-Probability" class="headerlink" title="Likelihood vs Probability"></a>Likelihood vs Probability</h2><p>Likelihood Definition (Wikipedia) : In statistics, the likelihood function expresses how probable a given set of observations is for different values of statistical parameters. It is equal to the joint probability distribution of the random sample evaluated at the given observations, and it is, thus, solely a functoin of paramters that index the family of those probability distributions. </p>
<h2 id="1-What-is-Likelihood"><a href="#1-What-is-Likelihood" class="headerlink" title="1. What is Likelihood?"></a>1. What is Likelihood?</h2><p>Likelihoods are the y-axis values for fixed data points with distribution that can be moved, as the following; </p>
<p>$$\mathcal{L}(\theta | x) = \mathcal{L}(distribution | data)$$</p>
<p>Whereas, Probabilities are the areas under a fixed distribution.</p>
<p>$$P(data | distribution)$$</p>
<p>If we find the maximum y-axis value for fixed data points by moving the distribution, we find the optimal distribution for the fixed data.<br>Therefore, we can use the likelihood function to fit a optimal distribution on the fixed data by finding maximum likelihood estimator. </p>
<h2 id="2-Maximum-Likelihood"><a href="#2-Maximum-Likelihood" class="headerlink" title="2. Maximum Likelihood?"></a>2. Maximum Likelihood?</h2><p>To find the maximum likelihood estimator, we might find a function for likelihoods given fixed data points, and take a derivative for the function, and set the derivatives as 0. </p>
<h3 id="Example-1-Exponential-distribution"><a href="#Example-1-Exponential-distribution" class="headerlink" title="Example 1: Exponential distribution"></a>Example 1: Exponential distribution</h3><p>The Probability Density Function of Exponential Distribution is defined as the following;</p>
<p>$$f(x; \lambda) = \lambda e ^ {- \lambda x} \qquad if  \qquad x \geq 0$$</p>
<p>Otherwise, </p>
<p>$$f(x; \lambda) = 0$$</p>
<p>for $\lambda &gt;0$, which is rate parameter of the distribution. </p>
<p>Hence, the exponential distribution will be shaped by the $\lambda$. </p>
<p>Therefore, we want to find the maximum likelihood of $\lambda$ for given data. </p>
<p>$$\mathcal{L}(\lambda | x_1, x_2, … , x_n) =  \mathcal{L}(\lambda | x_1)\mathcal{L}(\lambda | x_2)…\mathcal{L}(\lambda | x_n)    $$</p>
<p>$$= \lambda e ^ {- \lambda x_1}\lambda e ^ {- \lambda x_2}…\lambda e ^ {- \lambda x_n} \\ = \lambda^{n}(e ^ {-\lambda  (x_1 + x_2 + … + x_n)})$$</p>
<p>Now, take a derivative for the likelihood function to find maximum likelihood estimator of $\lambda$.</p>
<p>$$ \frac{d}{d\lambda} \mathcal{L}(\lambda | x_1, x_2, … , x_n) = \frac{d}{d\lambda} \lambda^{n} (e ^ {-\lambda * (x_1 + x_2 + … + x_n)})$$</p>
<p>Before derivitaves, there is more easier way to differentiate this function, which is to take a lograithm on the function.<br>Take a logarithm on the likelihood function, then this becomes the log likelihood function. </p>
<p>$$log (\mathcal{L}(\lambda | x_1, x_2, … , x_n)) =  log(\mathcal{L}(\lambda | x_1)\mathcal{L}(\lambda | x_2)…\mathcal{L}(\lambda | x_n))<br>$$</p>
<p>Which makes easier differentiate the function. </p>
<p>$$ \frac{d}{d\lambda} log(\mathcal{L}(\lambda | x_1, x_2, … , x_n)) = \frac{d}{d\lambda} log(\lambda^{n}(e ^ {-\lambda * (x_1 + x_2 + … + x_n)}))$$</p>
<p>$$= \frac{d}{d\lambda} (log(\lambda ^ {n}) + log(\lambda^{n}(e ^ {-\lambda * (x_1 + x_2 + … + x_n)}))) \\ = \frac{d}{d\lambda} (n log(\lambda) - \lambda (x_1 + x_2 + … + x_n)) \\ = n \frac{1}{\lambda} - (x_1 + x_2 + … + x_n)$$</p>
<p>Set this as zero, </p>
<p>$$n \frac{1}{\lambda} - (x_1 + x_2 + … + x_n) = 0$$</p>
<p>Which gives you, </p>
<p>$$ \frac{(x_1 + x_2 + … + x_n)}{n} = \mu = \frac{1}{\lambda}$$</p>
<p>Therefore, the maximum likelihood estimator of $\lambda$ is</p>
<p>$$\lambda = \frac{1}{\mu}$$.</p>
<p>And, this connects to the concept of the maximum entropy probability distribution.</p>
<p>“The exponential distribution is the maximum entropy distribution among all continuous distribution supported in $[0, \infty]$ that have a specified mean of $\frac{1}{\lambda}$”</p>
<h3 id="Example-2-Normal-Distribution"><a href="#Example-2-Normal-Distribution" class="headerlink" title="Example 2: Normal Distribution"></a>Example 2: Normal Distribution</h3><p>In normal distribution (often called Gaussian distribution), the mean $\mu$ and the variance $\sigma^2$ will be the parameter that the determines how the normal distribution looks like. With $\mu$ and $\sigma^2$, we will find the maximum likelihood estimator for the normal distribution given fixed data. </p>
<p>The PDF of normal distribution is defined as,</p>
<p>$$f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$</p>
<p>The Likelihood of $\mu$ and $\sigma$ for normal distribution will be, </p>
<p>$$\mathcal{L}(\mu, \sigma|x_1, x_2, … , x_n) = \mathcal{L}(\mu, \sigma^2|x_1)\mathcal{L}(\mu, \sigma^2|x_2)…\mathcal{L}(\mu, \sigma^2|x_n)$$</p>
<p>$$= \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_1-\mu)^2}{2\sigma^2}}\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_2-\mu)^2}{2\sigma^2}}…\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_n-\mu)^2}{2\sigma^2}}$$</p>
<p>Before take a derivatives, we take a log on the function as done in exponential. </p>
<p>$$log(\mathcal{L}(\mu, \sigma|x_1, x_2, … , x_n))= log(\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_1-\mu)^2}{2\sigma^2}}\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_2-\mu)^2}{2\sigma^2}}…\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_n-\mu)^2}{2\sigma^2}})$$</p>
<p>By simplifying the log likelihood function with the properties of logarithm,</p>
<p>$$= \sum_{i=1}^{n}(log(\frac{1}{\sqrt{2\pi\sigma^2}}) -\frac{(x_i-\mu)^2}{2\sigma^2}) \\ = -\frac{n}{2}log(2\pi) - nlog(\sigma) - \sum_{i=1}^{n}\frac{(x_i-\mu)^2}{2\sigma^2}$$</p>
<p>Now, take the derivative with respect to the $\mu$,</p>
<p>$$\frac{\partial}{\partial \mu} log(\mathcal{L}(\mu, \sigma|x_1, x_2, … , x_n)) \\ = 0 - 0 + \sum_{i=1}^{n} \frac{(x_i-\mu)}{\sigma^2} \\ = \sum_{i=1}^{n} \frac{x_i}{\sigma^2} - n\mu$$</p>
<p>Set this as 0, </p>
<p>$$\sum_{i=1}^{n} \frac{x_i}{\sigma^2} - n\mu = 0$$</p>
<p>Since $\sigma^2$ is constant here, </p>
<p>$$ \sum_{i=1}^{n} x_i = n\mu$$</p>
<p>Hence,</p>
<p>$$\mu = \sum_{i=1}^{n} x_i / n$$<br>which is the mean of the measurements. </p>
<p>Taking derivative with respect to the $\sigma$, </p>
<p>$$\frac{\partial}{\partial \sigma} log(\mathcal{L}(\mu, \sigma|x_1, x_2, … , x_n)) = \frac{\partial}{\partial \sigma} (-\frac{n}{2}log(2\pi) - nlog(\sigma) - \sum_{i=1}^{n}\frac{(x_i-\mu)^2}{2\sigma^2}) \\ = 0 - \frac{n}{\sigma} + \sum_{i=1}^{n} \frac{(x_i - \mu)^2}{\sigma^3}$$</p>
<p>Set this as 0,</p>
<p>$$0 - \frac{n}{\sigma} + \sum_{i=1}^{n} \frac{(x_i - \mu)^2}{\sigma^3} = 0 \\ n = \sum_{i=1}^{n} \frac{(x_i - \mu)^2}{\sigma^2} \\ \sigma^2 = \sum_{i=1}^{n} \frac{(x_i - \mu)^2}{n} \\ \sigma = \sum_{i=1}^{n} \sqrt{\frac{(x_i - \mu)^2}{n}}$$<br>which is the standard deviation of the measurements. </p>
<p>In conclusion, </p>
<p>the mean of the data is the maximum likelihood estimate for where the center of the distribution should be, </p>
<p>the standard deviation of the data is the maximum likelihood estimate for how wide the curve of the distribution should be. </p>
<p>This also connects to the maximum entropy probability distribution as the following; </p>
<p>The normal distribution has maximum entropy among all real-valued distribution supported on $(-\infty, \infty)$ with a specified variance $\sigma^2$.</p>
<p>Reference:<br><a href="https://en.wikipedia.org/wiki/Likelihood_function" target="_blank" rel="noopener">Likelihood(wiki)</a><br><a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution" target="_blank" rel="noopener">Maximum Entropy Probability Distribution(wiki)</a><br><a href="https://www.youtube.com/watch?v=pYxNSUDSFH4" target="_blank" rel="noopener">Related to Likelihood for distributions (StatQuest with Josh Starmer)</a></p>

        </div>
        
        
        
    </div>
</div>









    
<div class="card card-transparent">
    <nav class="pagination is-centered" role="navigation" aria-label="pagination">
        <div class="pagination-previous is-invisible is-hidden-mobile">
            <a class="is-flex-grow has-text-black-ter" href="/page/0/">Previous</a>
        </div>
        <div class="pagination-next">
            <a class="is-flex-grow has-text-black-ter" href="/page/2/">Next</a>
        </div>
        <ul class="pagination-list is-hidden-mobile">
            
            <li><a class="pagination-link is-current" href="/">1</a></li>
            
            <li><a class="pagination-link has-text-black-ter" href="/page/2/">2</a></li>
            
        </ul>
    </nav>
</div>
</div>
                




<div class="column is-4-tablet is-4-desktop is-4-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                        <img class="image is-128x128 has-mb-6" src="/images/David.png" alt="Yongbock (David) Kwon">
                    
                    
                    <p class="is-size-4 is-block">
                        Yongbock (David) Kwon
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        The trick in life isn&#39;t getting what you want, it&#39;s wanting it after you get it
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>Seoul, Korea</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Posts
                    </p>
                    <p class="title has-text-weight-normal">
                        12
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Categories
                    </p>
                    <p class="title has-text-weight-normal">
                        13
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Tags
                    </p>
                    <p class="title has-text-weight-normal">
                        17
                    </p>
                </div>
            </div>
        </nav>
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/DavidKwon91" target="_blank">
                Follow</a>
        </div>
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Github" href="https://github.com/DavidKwon91">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Linkedin" href="https://www.linkedin.com/in/yongbock-david-kwon-7a195994/">
                
                <i class="fab fa-linkedin"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        
    
        


    
        
<div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Categories
            </h3>
            <ul class="menu-list">
            <li>
        <a class="level is-marginless" href="/categories/ML/">
            <span class="level-start">
                <span class="level-item">ML</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">4</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/ML/Decision-Tree/">
            <span class="level-start">
                <span class="level-item">Decision Tree</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Gaussian-Process/">
            <span class="level-start">
                <span class="level-item">Gaussian Process</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Likelihood/">
            <span class="level-start">
                <span class="level-item">Likelihood</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Logistic-Regression/">
            <span class="level-start">
                <span class="level-item">Logistic Regression</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/Math/">
            <span class="level-start">
                <span class="level-item">Math</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">4</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/Math/CLT/">
            <span class="level-start">
                <span class="level-item">CLT</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Math/Cholesky-Decomposition/">
            <span class="level-start">
                <span class="level-item">Cholesky Decomposition</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Math/Histogram/">
            <span class="level-start">
                <span class="level-item">Histogram</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Math/Schur-Complement/">
            <span class="level-start">
                <span class="level-item">Schur Complement</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/Today/">
            <span class="level-start">
                <span class="level-item">Today</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">3</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/Today/August/">
            <span class="level-start">
                <span class="level-item">August</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Today/September/">
            <span class="level-start">
                <span class="level-item">September</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">2</span>
            </span>
        </a></li></ul></li>
            </ul>
        </div>
    </div>
</div>
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Tag Cloud
        </h3>
        <a href="/tags/CentralLimitTheorem/" style="font-size: 10px;">CentralLimitTheorem</a> <a href="/tags/CharlesBukowski/" style="font-size: 10px;">CharlesBukowski</a> <a href="/tags/CholeskyDecomposition/" style="font-size: 10px;">CholeskyDecomposition</a> <a href="/tags/Decision-Tree/" style="font-size: 10px;">Decision Tree</a> <a href="/tags/Entropy/" style="font-size: 10px;">Entropy</a> <a href="/tags/GaussianProcess/" style="font-size: 10px;">GaussianProcess</a> <a href="/tags/Gini-Index/" style="font-size: 10px;">Gini Index</a> <a href="/tags/Histogram/" style="font-size: 10px;">Histogram</a> <a href="/tags/Likelihood/" style="font-size: 15px;">Likelihood</a> <a href="/tags/LogLikelihood/" style="font-size: 10px;">LogLikelihood</a> <a href="/tags/Logit/" style="font-size: 10px;">Logit</a> <a href="/tags/ML/" style="font-size: 20px;">ML</a> <a href="/tags/Matrix/" style="font-size: 15px;">Matrix</a> <a href="/tags/Odds/" style="font-size: 10px;">Odds</a> <a href="/tags/Poem/" style="font-size: 10px;">Poem</a> <a href="/tags/SchurComplement/" style="font-size: 10px;">SchurComplement</a> <a href="/tags/Today/" style="font-size: 15px;">Today</a>
    </div>
</div>

    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Recent
        </h3>
        
        <article class="media">
            
            <a href="/2019/09/19/DecisionTree/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="DecisionTree">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-09-19T08:13:52.000Z">2019-09-19</time></div>
                    <a href="/2019/09/19/DecisionTree/" class="title has-link-black-ter is-size-6 has-text-weight-normal">DecisionTree</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/ML/">ML</a> / <a class="has-link-grey -link" href="/categories/ML/Decision-Tree/">Decision Tree</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/09/15/GaussianProcess/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="GaussianProcess">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-09-15T09:07:27.000Z">2019-09-15</time></div>
                    <a href="/2019/09/15/GaussianProcess/" class="title has-link-black-ter is-size-6 has-text-weight-normal">GaussianProcess</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/ML/">ML</a> / <a class="has-link-grey -link" href="/categories/ML/Gaussian-Process/">Gaussian Process</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/09/15/No-Leaders-Please/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="No Leaders Please">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-09-14T21:01:18.000Z">2019-09-15</time></div>
                    <a href="/2019/09/15/No-Leaders-Please/" class="title has-link-black-ter is-size-6 has-text-weight-normal">No Leaders Please</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/Today/">Today</a> / <a class="has-link-grey -link" href="/categories/Today/September/">September</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/09/09/hist/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="Histogram">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-09-09T09:49:56.000Z">2019-09-09</time></div>
                    <a href="/2019/09/09/hist/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Histogram</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/Math/">Math</a> / <a class="has-link-grey -link" href="/categories/Math/Histogram/">Histogram</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/09/09/CLT/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="CLT">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-09-09T07:08:11.000Z">2019-09-09</time></div>
                    <a href="/2019/09/09/CLT/" class="title has-link-black-ter is-size-6 has-text-weight-normal">CLT</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/Math/">Math</a> / <a class="has-link-grey -link" href="/categories/Math/CLT/">CLT</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            Archives
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2019/09/">
                <span class="level-start">
                    <span class="level-item">September 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">8</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2019/08/">
                <span class="level-start">
                    <span class="level-item">August 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Tags
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/CentralLimitTheorem/">
                        <span class="tag">CentralLimitTheorem</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/CharlesBukowski/">
                        <span class="tag">CharlesBukowski</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/CholeskyDecomposition/">
                        <span class="tag">CholeskyDecomposition</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Decision-Tree/">
                        <span class="tag">Decision Tree</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Entropy/">
                        <span class="tag">Entropy</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/GaussianProcess/">
                        <span class="tag">GaussianProcess</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Gini-Index/">
                        <span class="tag">Gini Index</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Histogram/">
                        <span class="tag">Histogram</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Likelihood/">
                        <span class="tag">Likelihood</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/LogLikelihood/">
                        <span class="tag">LogLikelihood</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Logit/">
                        <span class="tag">Logit</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/ML/">
                        <span class="tag">ML</span>
                        <span class="tag is-grey">8</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Matrix/">
                        <span class="tag">Matrix</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Odds/">
                        <span class="tag">Odds</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Poem/">
                        <span class="tag">Poem</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/SchurComplement/">
                        <span class="tag">SchurComplement</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Today/">
                        <span class="tag">Today</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.svg" alt="David Kwon" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2019 David Kwon&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Creative Commons" href="https://creativecommons.org/">
                        
                        <i class="fab fa-creative-commons"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/">
                        
                        <i class="fab fa-creative-commons-by"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("en");</script>

<script>
var IcarusThemeSettings = {
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>



    
    
    
    <script src="/js/animation.js"></script>
    

    
    
    
    <script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
    <script src="/js/gallery.js" defer></script>
    

    
    

<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


    
    
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>

    
    

<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>


    
    

    
    
    
    

    
    
    
    
    


<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>