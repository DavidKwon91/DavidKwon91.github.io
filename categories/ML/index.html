<!DOCTYPE html>
<html  lang="en">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 3.9.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>Category: ML - David Kwon</title>


    <meta property="og:type" content="website">
<meta property="og:title" content="David Kwon">
<meta property="og:url" content="http://yoursite.com/categories/ML/index.html">
<meta property="og:site_name" content="David Kwon">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/images/og_image.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="David Kwon">
<meta name="twitter:image" content="http://yoursite.com/images/og_image.png">








<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
    
    <style>body>.footer,body>.navbar,body>.section{opacity:0}</style>
    

    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">
    

    
    

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">


    
    
    
    

<link rel="stylesheet" href="/css/back-to-top.css">


    
    

    
    
    
    

    
    
<link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

    
    
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-2-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.svg" alt="David Kwon" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">Home</a>
                
                <a class="navbar-item"
                href="/archives">Archives</a>
                
                <a class="navbar-item"
                href="/categories">Categories</a>
                
                <a class="navbar-item"
                href="/tags">Tags</a>
                
                <a class="navbar-item"
                href="/about">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                    
                    
                    <a class="navbar-item" target="_blank" title="Download on GitHub" href="https://github.com/DavidKwon91">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                    
                
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-8-widescreen has-order-2 column-main"><div class="card">
    <div class="card-content">
        <nav class="breadcrumb" aria-label="breadcrumbs">
        <ul>
            <li><a href="/categories">Categories</a></li>
            
            <li class="is-active"><a href="#" aria-current="page">ML</a></li>
        </ul>
        </nav>
    </div>
</div>

    <div class="card">
    
    <div class="card-image">
        <a href="/2020/01/02/logistic/" class="image is-7by1">
            <img class="thumbnail" src="/images/sigmoid.png" alt="Logistic Regression - Newton&#39;s method">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2020-01-02T03:20:29.000Z">2020-01-02</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Logistic-Regression-II/">Logistic Regression II</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    11 minutes read (About 1631 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2020/01/02/logistic/">Logistic Regression - Newton&#39;s method</a>
            
        </h1>
        <div class="content">
            <h2 id="Logistic-Regression-Algorithm-with-Newton’s-Method"><a href="#Logistic-Regression-Algorithm-with-Newton’s-Method" class="headerlink" title="Logistic Regression Algorithm with Newton’s Method"></a>Logistic Regression Algorithm with Newton’s Method</h2><p>This post will somehow overlap the previous post, <a href="https://davidkwon91.github.io/categories/ML/Logistic-Regression/" target="_blank" rel="noopener">Logistic Regression I</a>, since I will discuss about the Logistic Regression further the post. </p>
<p>We start with the Binomial Distribution. </p>
<h3 id="Binomial-Distribution"><a href="#Binomial-Distribution" class="headerlink" title="Binomial Distribution"></a>Binomial Distribution</h3><p>Let $Y$ be the number of success in $m$ trials of a binomial process. Then Y is said to have a binomial distribution with paramters of $m$ and $\theta$. </p>
<p>$$Y \sim Bin(m, \theta)$$</p>
<p>The probability that $Y$ takes the integer value $j$ ($j = 0, 1, …, m$) is given by, </p>
<p>$$P(Y = j) = \binom{m}{j} \theta^j (1-\theta)^{m-j}$$</p>
<p>Then, let $y_i$ be the number of success in $m_i$ trials of a binomial process where $i = 1, …, n$. </p>
<p>$$(Y = y_i|X = x_i) \sim Bin(m_i, \theta(x_i))$$</p>
<h3 id="Sigmoid-Function"><a href="#Sigmoid-Function" class="headerlink" title="Sigmoid Function"></a>Sigmoid Function</h3><p>In logistic regression for binary response variable, we use sigmoid function to figure out the optimization of loss function, or cost function, of logistic regression model because the sigmoid function can be differentiated. </p>
<p>Let we have one predictor variable, $x$, to predict binary outcome variable. </p>
<p>Then, sigmoid function is defined as below, </p>
<p>$$\theta(x) = \frac{e^{\beta_0 + \beta_1x}}{1 + e^{\beta_0 + \beta_1x}} = \frac{1}{1 + e^{-(\beta_0 + \beta_1x)}}$$</p>
<p>Here, the $\theta(x)$ is the probability of success.<br>Solving the equation for $\beta_0 + \beta_1x$, </p>
<p>$$\beta_0 + \beta_1x = log(\frac{\theta(x)}{1-\theta(x)}) $$<br>Here, the $\frac{\theta(x)}{1-\theta(x)}$ is known as odds, which can be described as “odds in favor of success”</p>
<p>The odds can be described as, </p>
<p>$$odds = \frac{Probability}{1-Probability}$$</p>
<p>As the previous posts, <a href="https://davidkwon91.github.io/categories/ML/Logistic-Regression/" target="_blank" rel="noopener">Logistic Regression I</a> and <a href="https://davidkwon91.github.io/categories/ML/Likelihood/" target="_blank" rel="noopener">Likelihood</a> discussed, the likelihood and the probability in the dataset can be seen as,</p>
<p>$$\mathcal{L}(\theta | x) = \mathcal{L}(distribution | data)$$</p>
<p>$$P(data | distribution)$$</p>
<p>Then, if we find the optimal distribution given by the maximum likelihood or the minimum loss function, we can find the probability of the outcome. </p>
<h3 id="Likelihood-of-Logistic-Regression"><a href="#Likelihood-of-Logistic-Regression" class="headerlink" title="Likelihood of Logistic Regression"></a>Likelihood of Logistic Regression</h3><p>Let $\theta(X) = \frac{1}{1+e^{-(X\beta)}}$, then the likelihood function is the function of the unkown probability of success $\theta(X)$ given by..</p>
<p>$$L = \prod_{i=1}^{n} P(Y_i = y_i|X = x_i) \\ = \prod_{i=1}^{n} \binom{m_i}{y_i} \theta(x_i)^{y_i} (1-\theta(x_i))^{m_i - y_i}$$</p>
<p>To simplify the likelihood, we take the log on the equation, </p>
<p>$$log(L) = \sum_{i=1}^{n}[log(\binom{m_i}{y_i}) + log(\theta(x_i)^{y_i}) + log((1-\theta(x_i))^{m_i - y_i})] \\ = \sum_{i=1}^{n}[y_ilog(\theta(x_i)) + (m_i-y_i)log(1-\theta(x_i)) + log(\binom{m_i}{y_i})] $$</p>
<p>Here, the all $m_i$ is equal to 1 for the binary outcome variable. Therefore, </p>
<p>$$log(L) = \sum_{i=1}^{n}[y_ilog(\theta(x_i)) + (1-y_i)log(1-\theta(x_i))] \tag{Eq 1}$$</p>
<p>The constant, $log(\binom{m_i}{y_i})$ is excluded here, since it won’t affect in optimization. The equation $(Eq 1)$ is the log likelihood function that we want to maximize. We take the minus sign for the log likelihood, which is the cost function of logistic regression. </p>
<p>Then, the cost function for Logistic regression is the normalization of the negative log likelihood function, which is the final cost function that we want to minimize. </p>
<p>$$J = -\frac{1}{n}\sum_{i=1}^{n}[y_ilog(\theta(x_i)) + (1-y_i)log(1-\theta(x_i))] \tag{Eq 2}$$</p>
<h3 id="Newton-Raphson-Method"><a href="#Newton-Raphson-Method" class="headerlink" title="Newton-Raphson Method"></a>Newton-Raphson Method</h3><p>This optimization method is often called as Newton’s method, and the form is given by, </p>
<p>$$\theta_{k+1} = \theta_k - H_k^{-1}g_k$$</p>
<p>where $H_k$ is the Hessian matrix, which is the second partial derivative matrix, and $g_k$, which is the first partial derivative matrix, is the gradient matrix. </p>
<p>It comes from the Taylor approximation of $f(\theta)$ around $\theta_k$. </p>
<p>$$f_{quad}(\theta) =  f(\theta_k) + g_k^{T}(\theta - \theta_k) + \frac{1}{2}(\theta - \theta_k)^{T}H_k(\theta - \theta_k)$$ </p>
<p>Setting the derivative of the function for $\theta$ equal to 0 to find the critical point. </p>
<p>$$\frac{\partial f_{quad}(\theta)}{\partial \theta} = 0 + g_k + H_k(\theta - \theta_k) = 0 \\ -g_k = H_k(\theta-\theta_k) \\ = H_k\theta - H_k\theta_k \\ H_k\theta = H_k\theta_k - g_k \\ \therefore \theta_{k+1} = \theta_k - H_k^{-1}g_k $$</p>
<h3 id="Linear-Regression-with-Newton-Raphson-Method"><a href="#Linear-Regression-with-Newton-Raphson-Method" class="headerlink" title="Linear Regression with Newton-Raphson Method"></a>Linear Regression with Newton-Raphson Method</h3><p>In linear regression, the $\hat y$ is given by $X\theta$, and therefore, least squares, which is the cost function, is given by ..</p>
<p>$$f(\theta) = f(\theta; X, y) = (y-X\theta)^{T}(y-X\theta) \\ = y^Ty - y^TX\theta - \theta^TX^Ty + \theta^TX^TX\theta \\  = y^Ty - 2\theta^TX^Ty + \theta^TX^TX\theta$$</p>
<p>Then, the gradient is..</p>
<p>$$g = \partial f(\theta) = -2X^T y + 2X^T X \theta$$</p>
<p>The Hessian is..</p>
<p>$$H = \partial^2f(\theta) = 2X^TX $$</p>
<p>The Newton’s method for Linear Regression is..</p>
<p>$$\therefore \theta_{k+1} = \theta_k - H_k^{-1}g_k \\ = \theta_k - (2X^TX)^{-1}(-2X^Ty + 2X^TX\theta_k) \\ = \theta_k + (X^TX)^{-1}(X^Ty) - (X^TX)^{-1}X^TX\theta_k \\ \therefore \hat \beta= (X^TX)^{-1}X^Ty  $$</p>
<h3 id="Logistic-Regression-with-Newton’s-method"><a href="#Logistic-Regression-with-Newton’s-method" class="headerlink" title="Logistic Regression with Newton’s method"></a>Logistic Regression with Newton’s method</h3><p>Recall that the cost function for logistic regression is described as $(Eq 2)$, </p>
<p>$$J = -\frac{1}{n}\sum_{i=1}^{n}[y_ilog(\theta(x_i)) + (1-y_i)log(1-\theta(x_i))] $$</p>
<p>Therefore, the gradient of the cost function is given by </p>
<p>$$g = X^T(\theta(X) - y)$$</p>
<p>the Hessian is given by </p>
<p>$$ X^TWX $$</p>
<p>Then, the $\beta$ is given by the Newton’s method </p>
<p>$$\theta_{k+1} = \theta_k - H_k^{-1}g_k \\ = \theta_k - (X^TWX)^{-1}(X^T(\theta_k(X) - y)) \\ \therefore \hat \beta = \theta_k - (X^TWX)^{-1}(X^T(\theta_k(X) - y))$$</p>
<h3 id="Calculation-of-the-results"><a href="#Calculation-of-the-results" class="headerlink" title="Calculation of the results"></a>Calculation of the results</h3><p>The standard error for the coefficients are the square root of the diagonal elements of the Hessian matrix. </p>
<p>The z value is the </p>
<p>$$Z = \frac{\hat \beta}{estimated \qquad se(\hat \beta)}$$</p>
<p>and this statistic is used to test if $\beta = 0$, and it’s called a Wald test statistic. </p>
<p>As I dealt with the similarity of the logit with the standard normal distribution in the previous post, <a href="https://davidkwon91.github.io/categories/ML/Logistic-Regression/" target="_blank" rel="noopener">Logistic Regression I</a>, we can use this statistic to test for statistical significance. The confidence interval can be calculated by</p>
<p>$$\hat \beta \pm z_{1-\alpha/2} \hat {se}(\hat \beta)$$</p>
<h3 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h3><figure class="highlight plain hljs"><figcaption><span>functions&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">#Sigmoid function - make cost function derivative</span><br><span class="line">sigmoid &lt;- function(lo)&#123;</span><br><span class="line">  sig &lt;- 1/(1+exp(-lo))</span><br><span class="line">  return(sig)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#cost function - mean of negative log likelihood function</span><br><span class="line">LL.cost &lt;- function(X,Y,theta)&#123;</span><br><span class="line">  </span><br><span class="line">  J &lt;- (-1/n)*sum(Y*log(sigmoid(X%*%theta)) + (1-Y)*log(1-sigmoid(X%*%theta)))</span><br><span class="line">  </span><br><span class="line">  return(J)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#first derivatives</span><br><span class="line">gradient.func &lt;- function(X,Y,theta)&#123;</span><br><span class="line">  G &lt;- (1/n)*(t(X)%*%(sigmoid(X%*%theta) - Y))</span><br><span class="line">  return(G)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#second derivatives</span><br><span class="line">#not use this function</span><br><span class="line">hessian.func &lt;- function(X,Y,theta)&#123;</span><br><span class="line">  H &lt;- (1/n)*((t(X)*diag(sigmoid(X%*%theta)*(1-sigmoid(X%*%theta))))%*%X)</span><br><span class="line">  return(H)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#Logistic regression with Newton Raphson method</span><br><span class="line">newton.method.logisticReg &lt;- function(X, y, theta, num_iter,tol)&#123;</span><br><span class="line">  </span><br><span class="line">  summar &lt;- data.frame(H.diag = rep(0,p+1), theta = rep(0,p+1), se.theta=rep(0,p+1))</span><br><span class="line">  p &lt;- dim(x)[2]</span><br><span class="line">  #iteration to update the theta until the weight is too small</span><br><span class="line">  for(i in 1:num_iter)&#123;</span><br><span class="line">    grad &lt;- gradient.func(X,y,theta)</span><br><span class="line">    H &lt;- hessian(LL.cost, X = X, Y = y, theta, method = &quot;complex&quot;)</span><br><span class="line">    #&quot;complex&quot; method = calculated as the Jacobian of the gradient</span><br><span class="line">    </span><br><span class="line">    H.diag &lt;- diag(ginv(H))/(n)</span><br><span class="line">    se.theta &lt;- sqrt(abs(H.diag))</span><br><span class="line">    newton.weight &lt;- ginv(H)%*%grad</span><br><span class="line">    theta &lt;- theta - newton.weight</span><br><span class="line">    </span><br><span class="line">    summar$theta &lt;- theta</span><br><span class="line">    </span><br><span class="line">    #new hessian with updated theta</span><br><span class="line">    H.new &lt;- hessian(LL.cost, X = X, Y = y, theta, method = &quot;complex&quot;)</span><br><span class="line">    #&quot;complex&quot; method = calculated as the Jacobian of the gradient</span><br><span class="line">    </span><br><span class="line">    H.diag &lt;- diag(ginv(H.new))/(n)</span><br><span class="line">    #Standard Error for Coefficients are the square root of the diagonal elements of the Hessian divided by n</span><br><span class="line">    se.theta &lt;- sqrt(abs(H.diag))</span><br><span class="line">    </span><br><span class="line">    summar$H.diag &lt;- H.diag</span><br><span class="line">    summar$se.theta &lt;- se.theta</span><br><span class="line">    summar$z.value &lt;- summar$theta/summar$se.theta</span><br><span class="line">    summar$p.value &lt;- pnorm(-abs(summar$theta)/summar$se.theta)*2</span><br><span class="line">    summar$lower.conf &lt;- summar$theta - 1.96 * summar$se.theta</span><br><span class="line">    summar$upper.conf &lt;- summar$theta + 1.96 * summar$se.theta</span><br><span class="line">    summar$loglik &lt;- -LL.cost(X,y,summar$theta)*n</span><br><span class="line">    summar$AIC &lt;- -2*summar$loglik + 2*(p+1)</span><br><span class="line">    </span><br><span class="line">    #stop if weights are less than tolerance</span><br><span class="line">    if(any(abs(newton.weight)&lt;tol))&#123;</span><br><span class="line">      summar$iter &lt;- i</span><br><span class="line">      return(summar)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>The following part is the example of implementation the functions. </p>
<figure class="highlight plain hljs"><figcaption><span>randomData&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">set.seed(11)</span><br><span class="line"></span><br><span class="line">x &lt;- matrix(rnorm(400), ncol = 4) #random predictors</span><br><span class="line">y &lt;- round(runif(100, 0, 1)) # create a binary outcome</span><br><span class="line">n &lt;- length(x)</span><br><span class="line">X &lt;- cbind(rep(1, 100), x)</span><br><span class="line">p &lt;- dim(x)[2] #intercept term (Beta0) - # of predictors</span><br><span class="line">theta&lt;-rep(0, 5)</span><br><span class="line"></span><br><span class="line">data1 &lt;- data.frame(cbind(y,x))</span><br><span class="line"></span><br><span class="line">result1 &lt;- newton.method.logisticReg(X,y,theta, 20,1e-5)</span><br><span class="line">result1</span><br></pre></td></tr></table></figure>

<img src="/2020/01/02/logistic/LogisticResult1.png" title="[logistic]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#fitting by glm syntax built in R</span><br><span class="line">glm1 &lt;- glm(y~x,  family = &quot;binomial&quot;)</span><br><span class="line"></span><br><span class="line">summary(glm1)</span><br><span class="line">logLik(glm1)</span><br><span class="line">AIC(glm1)</span><br></pre></td></tr></table></figure>

<img src="/2020/01/02/logistic/LogisticResult2-1.png" title="[logistic]">
<img src="/2020/01/02/logistic/LogisticResult2-2.png" title="[logistic]">

<p>The results by the algorithm are almost the same with the result given by the syntax built in R. </p>
<p>Below is the another example with titanic dataset. </p>
<figure class="highlight plain hljs"><figcaption><span>Titanic&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">train&lt;-titanic_train</span><br><span class="line"></span><br><span class="line">train &lt;- train %&gt;%</span><br><span class="line">  select(subset= -c(PassengerId, Pclass, Name, Sex, Ticket, Cabin, Embarked)) %&gt;%</span><br><span class="line">  mutate_all(funs(ifelse(is.na(.), mean(., na.rm=TRUE), .))) %&gt;%</span><br><span class="line">  mutate_if(is.integer, as.numeric)</span><br><span class="line"></span><br><span class="line">train.indx &lt;- createDataPartition(train$Survived, p=0.7, list=FALSE)</span><br><span class="line"></span><br><span class="line">training &lt;- train[train.indx,]</span><br><span class="line">valid &lt;- train[-train.indx,]</span><br><span class="line"></span><br><span class="line">x &lt;- as.matrix(training %&gt;% select(-Survived))</span><br><span class="line">y &lt;- training$Survived %&gt;% as.numeric</span><br><span class="line">n &lt;- nrow(x)</span><br><span class="line">X &lt;- cbind(rep(1, nrow(x)), x)</span><br><span class="line">p &lt;- dim(x)[2]</span><br><span class="line">theta&lt;-rep(0, ncol(X))</span><br><span class="line">data1 &lt;- data.frame(cbind(y,x))</span><br><span class="line"></span><br><span class="line">result1 &lt;- newton.method.logisticReg(X, y, theta,1000, 1e-15)</span><br><span class="line">result1</span><br></pre></td></tr></table></figure>

<img src="/2020/01/02/logistic/LogisticResult3.png" title="[logistic]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">glm1 &lt;- glm(y~x, family = binomial)</span><br><span class="line">summary(glm1)</span><br><span class="line">logLik(glm1)</span><br><span class="line">AIC(glm1)</span><br></pre></td></tr></table></figure>

<img src="/2020/01/02/logistic/LogisticResult4-1.png" title="[logistic]">

<img src="/2020/01/02/logistic/LogisticResult4-2.png" title="[logistic]">



<h3 id="Prediction-with-the-calculated-beta"><a href="#Prediction-with-the-calculated-beta" class="headerlink" title="Prediction with the calculated $\beta$"></a>Prediction with the calculated $\beta$</h3><p>Fitting for training dataset. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#fitting</span><br><span class="line">pred &lt;- as.vector(sigmoid(X%*%result1$theta))</span><br><span class="line">pred.hands &lt;- ifelse(pred &gt; .5, 1, 0)</span><br><span class="line"></span><br><span class="line">pred1 &lt;- as.vector(predict(glm1, data1, type=&quot;response&quot;))</span><br><span class="line">pred1.hands &lt;- ifelse(pred1 &gt; .5, 1, 0)</span><br><span class="line"></span><br><span class="line">identical(pred.hands, pred1.hands)</span><br></pre></td></tr></table></figure>

<img src="/2020/01/02/logistic/LogisticResult5.png" title="[logistic]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#Training Accuracy</span><br><span class="line">mean(pred.hands == y)</span><br><span class="line">mean(pred1.hands == y)</span><br></pre></td></tr></table></figure>

<img src="/2020/01/02/logistic/LogisticResult6.png" title="[logistic]">

<p>For validation dataset, </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#Validation Accuracy</span><br><span class="line">x &lt;- as.matrix(valid %&gt;% select(-Survived))</span><br><span class="line">y &lt;- valid$Survived %&gt;% as.numeric</span><br><span class="line">n &lt;- nrow(x)</span><br><span class="line">X &lt;- cbind(rep(1, nrow(x)), x)</span><br><span class="line">p &lt;- dim(x)[2]</span><br><span class="line"></span><br><span class="line">data1 &lt;- data.frame(cbind(y,x))</span><br><span class="line"></span><br><span class="line">pred &lt;- as.vector(sigmoid(X%*%result1$theta))</span><br><span class="line">pred.hands &lt;- ifelse(pred &gt; .5, 1, 0)</span><br><span class="line"></span><br><span class="line">pred1 &lt;- as.vector(predict(glm1, data1, type=&quot;response&quot;))</span><br><span class="line">pred1.hands &lt;- ifelse(pred1 &gt; .5, 1, 0)</span><br><span class="line"></span><br><span class="line">#Validation Accuracy</span><br><span class="line">mean(pred.hands == y)</span><br><span class="line">mean(pred1.hands == y)</span><br></pre></td></tr></table></figure>

<img src="/2020/01/02/logistic/LogisticResult7.png" title="[logistic]">




        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/10/24/PCA/" class="image is-7by1">
            <img class="thumbnail" src="/images/pca.png" alt="PCA">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-10-24T06:08:47.000Z">2019-10-24</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Principal-Components-Analysis/">Principal Components Analysis</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    13 minutes read (About 1897 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/10/24/PCA/">PCA</a>
            
        </h1>
        <div class="content">
            <h2 id="Introduction-to-Principal-Components-Analysis"><a href="#Introduction-to-Principal-Components-Analysis" class="headerlink" title="Introduction to Principal Components Analysis"></a>Introduction to Principal Components Analysis</h2><p>PCA (Principal Components Analysis) is a popular statistical method to reduce dimensions when the dimension of a dataset is huge, especially, when $n \geq m$ in $m \times n$ matrix or dataset.</p>
<p>Before digging into PCA, I start with some basic of linear algebra. </p>
<h3 id="1-Numerator-and-Denominator-Layout-in-Matrix-Differentiation"><a href="#1-Numerator-and-Denominator-Layout-in-Matrix-Differentiation" class="headerlink" title="1. Numerator and Denominator Layout in Matrix Differentiation"></a>1. Numerator and Denominator Layout in Matrix Differentiation</h3><p>In matrix differentiation, there are two notational convention to express the derivative of a vector with respect to a vector, that is $\frac{\partial y}{\partial x}$, which are “Numerator Layout” and “Denominator Layout”.</p>
<p>It doesn’t matter which way you would like to use, but make sure you stick with a notational convention if you start with a particular convention. </p>
<p>Let $x$ be a $n \times 1$ vector, $y$ be a $m \times 1$ vector. </p>
<p>Then, for $\frac{\partial y}{\partial x}$,</p>
<p>Numerator Layout: $m \times n$ matrix as following; </p>
<img src="/2019/10/24/PCA/numlay.png" title="[table]">

<p>Denominator Layout: $n \times m$ matrix as following;</p>
<img src="/2019/10/24/PCA/denomlay.png" title="[table]">

<p>The different kinds of two notational convention can be found from <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Vector-by-vector_identities" target="_blank" rel="noopener">Wiki</a></p>
<p>Most of books or papers don’t state which convention they use, but the Denominator Layout is mostly used in many papers or software. This article is going to use Denominator Layout in this article. </p>
<h3 id="2-Variance-Covariance-Matrix"><a href="#2-Variance-Covariance-Matrix" class="headerlink" title="2. Variance-Covariance Matrix"></a>2. Variance-Covariance Matrix</h3><p>Variance-Covariance Matrix, called just Covariance Matrix, of a matrix $X$ can be expressed in two ways as following;</p>
<p>$$S = \frac{1}{n-1} XX^T \tag{1}$$</p>
<p>or</p>
<p>$$S = \frac{1}{n-1} X^{T}X \tag{2}$$</p>
<p>where $X$ is the $m \times n$ matrix. </p>
<p>I will make the data be centered by subtracting the mean of the data for later convenience.  </p>
<p>The (1) equation will give you the covariance matrix by row vectors as following;</p>
<p>$$S_{n \times n} = \frac{1}{n-1} X_{n \times 1} X_{1 \times n}^T = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \mu)(x_i - \mu)^T \tag{3}$$</p>
<p>where $\mu$ is the row mean vector and $S$ will be the $n \times n$ square matrix. </p>
<p>The (2) equation will give you the covariance matrix by column vectors as following;</p>
<p>$$S_{m \times m} = \frac{1}{n-1} X_{m \times 1}^{T}X_{1 \times m} = \frac{1}{n-1} \sum_{i=1}^{m} (x_i - \mu)^{T}(x_i - \mu) \tag{4}$$</p>
<p>where $\mu$ is the column mean vector and $S$ will be the $m \times m$ square matrix.</p>
<h6 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h6><p>Let $X$ be $2 \times 3$ matrix as the following; </p>
<p>$$X = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{bmatrix}$$</p>
<p>Then, for (1) equation, </p>
<p>The row vectors will be $3 \times 1$ vector as following;</p>
<p>$X_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ and $X_2 = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}$.</p>
<p>Then, the covariance matrix will be</p>
<p>$$S_{3 \times 3} = \frac{1}{n-1} X_{3 \times 1} X_{1 \times 3}^{T}$$.</p>
<p>For (2) equation, </p>
<p>The column vectors will be $1 \times 2$ vectors as following;</p>
<p>$X_1 = \begin{bmatrix} 1 &amp; 4 \end{bmatrix}$, $X_2 = \begin{bmatrix} 2 &amp; 5 \end{bmatrix}$, $X_3 = \begin{bmatrix} 3 &amp; 6 \end{bmatrix}$.</p>
<p>Then, the covariance matrix will be </p>
<p>$$S_{2 \times 2} = \frac{1}{n-1} X_{2 \times 1}^{T} X_{1 \times 2}$$</p>
<p>In this paper, we will need the (2) covariance matrix since mostly columns are the predictors, and covariance matrix is calculated by the columns. </p>
<h3 id="3-Intuition-on-Eigenvalues-and-Eigenvectors"><a href="#3-Intuition-on-Eigenvalues-and-Eigenvectors" class="headerlink" title="3. Intuition on Eigenvalues and Eigenvectors"></a>3. Intuition on Eigenvalues and Eigenvectors</h3><p>Formal Definition of Eigenvalues and Eigenvectors:</p>
<p>Let A be $n \times n$ a sqaure matrix, then $v$ and $\lambda$ is the Eigenvector and the Eigenvalue of $A$, respectively, if</p>
<p>$$Av = \lambda v \tag{5}$$</p>
<p>where $v$ is a non-zero vector and $\lambda$ can be any scalar. </p>
<p>Geometrical meaning of Eigenvectors and Eigenvalues would be..</p>
<p>From <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors" target="_blank" rel="noopener">Wiki</a>,</p>
<p>Eigenvectors can be seen the scaled direction vectors in the difrection of the particular vector; therefore, the direction of the vector is preserved, but we are just scaling the lengths.<br>Eigenvalue can be seen the factor by which it is stretched. </p>
<p>For equation (5), it is equivalent to the following;</p>
<p>$$(A - \lambda I)v = 0$$</p>
<p>where $I$ is the $n \times n$ identity matrix, and $0$ is the zero vector. </p>
<p>Also, if the $(A - \lambda I)$ is invertible, then $v$ can be zero-vector; therefore, in order to have a non-zero vector, $v$, the $(A - \lambda I)$ must be not invertible. It leads the determinant of $(A - \lambda I)$ must be zero as the following; </p>
<p>$$det(A - \lambda I) = 0$$</p>
<h3 id="4-Eigendecomposition-or-Spectral-Decomposition"><a href="#4-Eigendecomposition-or-Spectral-Decomposition" class="headerlink" title="4. Eigendecomposition or Spectral Decomposition"></a>4. Eigendecomposition or Spectral Decomposition</h3><p>Let $A$ be $n \times n$ a sqaure matrix with its rank is $n$, $rank(A) = n$, which is equivalent to have the number of linearly independent of columns or rows. </p>
<p>Then, $A$ can be factorized as</p>
<p>$$A_{n \times n} = Q_{n \times n} \Lambda_{n \times n} Q_{n \times n}^{-1}$$</p>
<p>where $Q$ is the $n \times n$ square matrix that the $i$th column is the eigenvector of A, and the $\Lambda$ is the diagonal matrix that the diagonal elements are the eigenvalues of A, $\Lambda_{ii} = \lambda_i$.</p>
<p>If the $Q$ is orthogonal matrix, then $A$ is equivalent to the following;</p>
<p>$$A = Q \Lambda Q^{-1} = Q \Lambda Q^T \tag{6}$$</p>
<h3 id="5-SVD-Singular-Value-Decomposition"><a href="#5-SVD-Singular-Value-Decomposition" class="headerlink" title="5. SVD (Singular Value Decomposition)"></a>5. SVD (Singular Value Decomposition)</h3><p>Let $A$ be $m \times n$ a rectangular matrix, then $A$ can be factorized as,</p>
<p>$$A_{m \times n} = U_{m \times m} \Sigma_{m \times n} V_{n \times n}^{T}$$</p>
<p>where $U$ and $V$ is orthonormal square matrix, which means $U^{T}U = V^{T}V = I$, and $\Sigma$ is the diagonal matrix. </p>
<p>Then, the square matrix, </p>
<p>$$A_{m \times n}A_{n \times m}^{T} = (U \Sigma V^T) (U \Sigma V^T)^T \\ = (U \Sigma V^T) (V \Sigma^{T} U^{T}) \\ = U(\Sigma \Sigma^{T}) U^{T} \tag{7}$$</p>
<p>which gives us $U$ is the eigenvector of $AA^T$ by the definition of Eigendecomposition and the fact that we assume that $U$ is the orthonormal matrix, and $U$ is called left-singular vector. </p>
<p>$$A_{n \times m}^{T}A_{m \times n} = (U \Sigma V^T)^{T} (U \Sigma V^T) \\ = (V \Sigma^{T} U^{T}) (U \Sigma V^T) \\ = V (\Sigma^{T} \Sigma) V^{T} \tag{8}$$</p>
<p>which gives us $V$ is the eigenvector of $A^{T}A$ by the definition of Eigendecomposition and the fact that we assume that $V$ is the orthonormal matrix, and $V$ is called right-singular vector. </p>
<p>Also, $\Sigma \Sigma^{T} = \Sigma^{T} \Sigma$ is the diagonal matrix that the diagonal elements are the eigenvalues. </p>
<h3 id="6-PCA-Principal-Components"><a href="#6-PCA-Principal-Components" class="headerlink" title="6. PCA (Principal Components)"></a>6. PCA (Principal Components)</h3><p>The goal of the PCA is to identify the most meaningful basis to re-express a dataset. </p>
<p>Let $X$ be $m \times n$ matrix, $Y$ be another $m \times n$ matrix related by a linear transformation $P_{n \times n}$. </p>
<p>Then,</p>
<p>$$Y = XP \\ = \begin{bmatrix} x_{11} &amp; \dots &amp; x_{1n}  \\ \vdots &amp; \ddots \\ x_{m1} &amp; \dots &amp; x_{mn} \end{bmatrix}  \begin{bmatrix} p_{11} &amp; \dots &amp; p_{1n}  \\ \vdots &amp; \ddots \\ p_{n1} &amp; \dots &amp; p_{nn} \end{bmatrix}$$</p>
<p>which gives us the dot product of $X$ and $P$. </p>
<p>The dot product can be interpreted as a linear transformation or projecting $X$ onto another basis by $P$.</p>
<p>In this case, let’s see the covariance matrix of $Y$. Notice that the covariance matrix of $Y$ or the covariance matrix of $X$ are centered as stated above (3) or (4). </p>
<p>$$C_{Y} = \frac{1}{n-1} Y^{T}Y \\ = \frac{1}{n-1} (XP)^{T}(XP) \\ = \frac{1}{n-1}(P^{T}X^{T}XP) \\ = P^{T}C_{X} P$$</p>
<p>If we prove that the matrix $P$ is orthonormal matrix, then we can see </p>
<p>$$C_{Y} =  P^{T}C_{X} P =  P^{-1}C_{X} P$$</p>
<p>by (6)</p>
<p>and hence, it implies that $P$ is the eigenvector matrix of covariance matrix of $X$. </p>
<ol>
<li>First Approach to prove $P$ is orthonormal. </li>
</ol>
<p>We know that $C_{Y}$ is the symmetric matrix since it is the covariance matrix of $Y$, </p>
<p>then, $C_{Y} = C_{Y}^{T}$ is true. </p>
<p>Let assume that $C_{Y} = P^{-1} C_{X} P$ by definition of Eigendecomposition. </p>
<p>$$C_{Y}^{T} = (P^{-1} C_{X} P)^{T} = (C_{X})^{T} (P^{-1})^{T} \\ = C_{Y} $$</p>
<p>Therefore, </p>
<p>$$P^{T} (C_{X})^{T} (P^{-1})^{T} =  P^{T}C_{X} P$$</p>
<p>We know that $C_X = C_{X}^{T}$ since the covariance matrix of $X$ is symmetric matrix.</p>
<p>Finally, </p>
<p>$$P^{T} C_{X} (P^{-1})^{T} =  P^{T}C_{X} P \\ (P^{-1})^{T} = P$$</p>
<p>Now, we can see the $P$ is orthonormal matrix. </p>
<p>This proof implies that we can get the $P$ from SVD by calculating $V$. </p>
<p>The covariance matrix of $X$ is.. </p>
<p>$$C_X = \frac{1}{n-1} X^{T}X$$ </p>
<p>By (8) in SVD, $X^{T}X$ is.. </p>
<p>$$X^{T}X = V (\Sigma_{X}^{T} \Sigma_{X}) V^{T}$$</p>
<p>Therefore, </p>
<p>$$V^{T} = P$$ </p>
<ol start="2">
<li>Second Approach to prove $P$ is orthonormal (this proof was provided by a member of my study group)</li>
</ol>
<p>We have </p>
<p>$$Y = XP$$</p>
<p>where $Y$ is the projected values for $X$ by linear tranformation of $P$. </p>
<p>Suppose we have $U$ as the following;</p>
<p>$$YU = X_{recovered}$$ </p>
<p>where $X_{recovered}$ is the re-proejcted values onto original values from the basis where the dataset are projected onto. </p>
<p>Then, </p>
<p>$$XPU = X_{recovered}$$</p>
<p>Therefore, </p>
<p>$$PU = I \tag{9}$$</p>
<p>Let $x$ is column vectors from $X$, and $\hat{x}$ is column vectors from $Y$.</p>
<p>$$x_{n \times 1} \in X_{m \times n}, \hat{x_{n \times 1}} \in Y_{m \times n}$$</p>
<p>The loss function of PCA is</p>
<p>$$\arg\min_{\hat{x}} ||x_{n \times 1} - U_{n \times n}\hat{x_{n \times 1}}||$$</p>
<p>To optimize, we differentiate the equation with respect to $\hat{x}$. </p>
<p>$$\frac{\partial}{\partial \hat{x}} ||x - U\hat{x}|| \\ = \frac{\partial}{\partial \hat{x}} (x-U\hat{x})^{T}(x-U\hat{x}) \\ = \frac{\partial}{\partial \hat{x}} (x^T - \hat{x}^{T}U^{T})(x-U\hat{x}) \\ = \frac{\partial}{\partial \hat{x}} (x^{T}x - \hat{x}^{T}U^{T}x - x^{T}U\hat{x} + \hat{x}^{T}\hat{x}) $$</p>
<p>Here, $\hat{x}^{T}U^{T}x = x^{T}U\hat{x}$ since they are scalar value. </p>
<p>$$= \frac{\partial}{\partial \hat{x}} (x^{T}x - 2x^{T}U\hat{x} + \hat{x}^{T}\hat{x}) \\ = 0 - 2U^{T}x + 2\hat{x} \tag{10}$$</p>
<p>By denominator layout as explained above, </p>
<p>$$\frac{\partial}{\partial \hat{x}} \hat{x}^{T}\hat{x} = 2\hat{x}$$ </p>
<p>$$\frac{\partial}{\partial \hat{x}} x^{T}U\hat{x} = (x^{T}U)^{T} = U^{T}x$$</p>
<p>Set the (10) equation as zero, </p>
<p>$$  - 2U^{T}x + 2\hat{x} = 0 \\ \hat{x} = U^{T}x$$</p>
<p>Therefore, $\hat{x} = Px$ from the first assumption, and $P = U^{T}$.</p>
<p>From (9) equation, </p>
<p>$$PU = PP^T = I \\ P^{-1} = P^T$$</p>
<p>We proved that $P$ is the orthonormal matrix. </p>
<p>As a result, </p>
<ol>
<li>$P$ is the Principal Components that allows us to get the most meaningful basis to re-express our dataset by linear transforming our dataset. </li>
<li>$P$ is the eigenvector matrix of the covariance matrix of our dataset for the column vectors (predictors)</li>
<li>$P$ is the transpose of right-singular vector, which is the $V^T$ in $U \Sigma V^T$, from Singular Value Decomposition. </li>
</ol>
<p>Reference:<br><a href="https://arxiv.org/pdf/1404.1100.pdf" target="_blank" rel="noopener">A Tutorial on Principal Component Analysis</a><br><a href="https://www2.imm.dtu.dk/pubdb/views/edoc_download.php/4000/pdf/imm4000" target="_blank" rel="noopener">Singular Value Decomposition and Principal Component Analysis</a><br><a href="https://intoli.com/blog/pca-and-svd/" target="_blank" rel="noopener">HOW ARE PRINCIPAL COMPONENT ANALYSIS AND SINGULAR VALUE DECOMPOSITION RELATED?</a><br><a href="https://www.comp.nus.edu.sg/~cs5240/lecture/matrix-differentiation.pdf" target="_blank" rel="noopener">Matrix Differentiation</a><br><a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors" target="_blank" rel="noopener">Eigenvalues and eigenvectors</a><br><a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix" target="_blank" rel="noopener">Eigendecompositoin of a matrix</a><br><a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">Matrix Calculus</a><br><a href="https://medium.com/bluekiri/understanding-principal-component-analysis-once-and-for-all-9f75e7b33635" target="_blank" rel="noopener">Understanding Principal Component Analysis Once And For All</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/10/16/AUROC/" class="image is-7by1">
            <img class="thumbnail" src="/images/auroc.png" alt="Classfication">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-10-16T04:04:57.000Z">2019-10-16</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Classification/">Classification</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    13 minutes read (About 1977 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/10/16/AUROC/">Classfication</a>
            
        </h1>
        <div class="content">
            <h2 id="Introduction-to-Classification-Metrics"><a href="#Introduction-to-Classification-Metrics" class="headerlink" title="Introduction to Classification Metrics"></a>Introduction to Classification Metrics</h2><p>This paper will discuss on binary classification metrics, Sensitivity vs Specificity, Precision vs Recall, and AUROC. </p>
<h3 id="1-Confusion-Matrix"><a href="#1-Confusion-Matrix" class="headerlink" title="1. Confusion Matrix"></a>1. Confusion Matrix</h3><img src="/2019/10/16/AUROC/confusion.png" title="[table]">

<p>True Positive (TP) - Predicts a value as positive when the value is actually positive.<br>False Positive (FP) - Predicts a value as positive when the value is actually negative.<br>True Negative (TN) - Predicts a value as negative when the value is actually negative.<br>False Negative (FN) - Predicts a value as negative when the value is actually positive. </p>
<p>In Hypothesis test, we used to set up the null hypothesis is the against value. </p>
<p>What this means is that we want to set up the null hypothesis as the value such as a case that a person doesn’t get a disease or a case that a transaction is not a fraud, and the alternative hypothesis as opposite. </p>
<p>For example, we want to test if a patient in a group gets a disease.<br>Then, null hypothesis, $H_0$, is the patient doesn’t get a disease. With some test, if a p-value, which is the probability that happens an event in the probability distribution of null hypothesis, is less than a value like 0.05, then we reject the null hypothesis and accept the alternative hypothesis, which is to conclude that the patient gets a disease. </p>
<p>Actual Positive ($H_0$ is false) : The patient gets a disease.<br>Actual Negative ($H_0$ is true) : The patient doesn’t get a disease. </p>
<p>Predicted Positive (Accepting $H_1$: reject $H_0$) : The patient gets a disease<br>Predicted Negative (not Accepting $H_1$: fail to reject $H_0$) : The patient doesn’t get a disease. </p>
<p>Type 1 Error (False Positive) : When $H_0$ is false, which is that the patient actually gets a disease, but we reject $H_0$ by a classifier, which is that we predict patient doesn’t get a disease. </p>
<p>Type 2 Error (False Negative) : When $H_0$ is true, which is that the patient actually doesn’t get a disease, but we fail to reject $H_0$ by a classifier, which is that we predict patient gets a disease. </p>
<h3 id="2-Metrics"><a href="#2-Metrics" class="headerlink" title="2. Metrics"></a>2. Metrics</h3><p>Sensitivity, Recall, or True Positive Rate (TPR) : $\frac{TP}{TP + FN}$</p>
<p>Specificity, True Negative Rate (TNR), 1 - FPR: $\frac{TN}{TN + FP}$</p>
<p>Notice that the denominator for both is the total number of actual value. The denominator of sensitivity is the total number of actual positive value, and the denominator of specificity is the total number of actual negative value. </p>
<p>That means, sensitivity is the probability of correctly predicting positive values among actual positive values. Specificity is the probability of correctly predicting negative values among actual negative values. Therefore, with this metrics, we can see how a model correctly predicts overall actual values for both positive and negative values. </p>
<p>Precision, Positive Predicted Value : $\frac{TP}{TP + FP}$</p>
<p>Recall, Sensitivity, or True Positive Rate (TPR) : $\frac{TP}{TP + FN}$</p>
<p>Here, we can see that the denominator of precision is the total number of predicted value as positive. However, the denominator of recall is the total number of actual positive value. </p>
<p>That means, precision is the probability of correctly predicting positive values among predicted values as positve. This will show how useful the model is, or the quality of the model. Recall is the probability of correctly predicting positive values among actual positive values. This will show how complete the results are, or the quantity of the results. </p>
<p>Therefore, with this metrics, we can see how a model correctly predicts positive values among predicted values and actual values. </p>
<p>In above example of predicting diseased patients, we might want to predict the diseased patients more; therefore, we want to focus on increasing the TRUE POSITIVE values and not focus on predicting TRUE NEGATIVE, but not losing too much predicting accuracy, which is to focus on increasing Precision and Recall. </p>
<p>As a result, sensitivity and specificity is generally used for overall balanced binary target variable or a case that we don’t have to focus on positive or negative values like if it is a dog or a cat in an image classification, but precision and recall should be used to predict an imbalanced binary target variable. </p>
<h3 id="3-Trade-off"><a href="#3-Trade-off" class="headerlink" title="3. Trade-off"></a>3. Trade-off</h3><p>It will be the best scenario if we have high performance of sensitivity and specicity or precision and recall. However, in many cases, it is hard to see such cases since there is a trade-off. </p>
<p>For Precision and Recall, the only difference between them is the denominator. The denominator of precision has Type 1 Error (FP), and the denominator of recall has Type 2 Error (FN). </p>
<p>Precision, Positive Predicted Value : $\frac{TP}{TP + FP}$</p>
<p>Recall, Sensitivity, or True Positive Rate (TPR) : $\frac{TP}{TP + FN}$</p>
<p>Precision and Recall shares same parameter, which is TP; therefore, by shifting the threshold of probability that classifies the values, FP and FN values can be varied. </p>
<p>Example codes below with Pima Indians Diabetes</p>
<p>This dataset has imbalanced binary target variable, “diabetes” as below. </p>
<img src="/2019/10/16/AUROC/target.png" title="[table]">


<p>I splitted the dataset by training and test set, and performed logistic regression to predict the “diabetes” variable in the test set. </p>
<p>glm.pred1 is the predicted probability values that the patients is diabetes, which is “pos”. </p>
<p>When the threshold is 0.1, which is to classify the predicted value as “pos” if the probability is greater than 0.1. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred1 &lt;- as.factor(ifelse(glm.pred1 &gt;= 0.1, &quot;pos&quot;,&quot;neg&quot;))</span><br><span class="line">confusionMatrix(pred1, as.factor(test$diabetes), positive=&quot;pos&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/10/16/AUROC/pred1CM.png" title="[table]">


<p>When the threshold is 0.3.</p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred3 &lt;- as.factor(ifelse(glm.pred1 &gt;= 0.3, &quot;pos&quot;,&quot;neg&quot;))</span><br><span class="line">confusionMatrix(pred3, as.factor(test$diabetes), positive=&quot;pos&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/10/16/AUROC/pred3CM.png" title="[table]">

<p>When the threshold is 0.5.</p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred5 &lt;- as.factor(ifelse(glm.pred1 &gt;= 0.5, &quot;pos&quot;, &quot;neg&quot;))</span><br><span class="line">confusionMatrix(pred5, as.factor(test$diabetes), positive=&quot;pos&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/10/16/AUROC/pred5CM.png" title="[table]">

<p>When the threshold is 0.7.</p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred7 &lt;- as.factor(ifelse(glm.pred1 &gt;= 0.7, &quot;pos&quot;, &quot;neg&quot;))</span><br><span class="line">confusionMatrix(pred7, as.factor(test$diabetes), positive=&quot;pos&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/10/16/AUROC/pred7CM.png" title="[table]">

<p>As you can see above, by increasing the threshold value, Recall is decreasing, and Precision is increasing by trade-off because the FN is increasing and FP is decreasing. Therefore, we have to find the optimal threshold. </p>
<p>When the threshold is 0.5, we have the best Accuracy with 77.83%.<br>However, when the threshold is 0.3, it seems to have optimal values for Precision and Recall with.. </p>
<p>Precision : 0.6100<br>Recall : 0.7625</p>
<p>As I told above, when we predict imbalanced binary dataset and we want to focus on predicting the positive values like finding diabetes patients, then we want to increase the Precision and Recall values, even if the Accuracy is not the best value. </p>
<p>In this case, we would select the threshold as 0.3. Or, if the threshold is already set up, then we might have to change our model to improve the Precision and Recall. </p>
<p>The trade-off between Sensitivity and Specificity will be discussed below section, AUROC (Area Under ROC curve)</p>
<h3 id="4-AUROC"><a href="#4-AUROC" class="headerlink" title="4. AUROC"></a>4. AUROC</h3><p>ROC (Receiver Operating Characteristic) is a plot used to see the quality of the model in many cases of classification problem. The X-axis of ROC curve is False Positive Rate, and the Y-axis of the curve is True Positive Rate. It is created by various thresholds. </p>
<p>False Positive Rate: $\frac{FP}{FP + TN} = 1 - TNR = 1 - Specificity$</p>
<p>True Positive Rate, also Recall, and Sensitivity: $\frac{TP}{TP + FN}$</p>
<p>Therefore, sensitivty and specificity or ROC curve deal with the each two columns of confusion matrix. We can see the overall accuracy by various thresholds with the metrics for both positive and negative values. </p>
<p>The trade-off between Sensitivity and Specificty or ROC curve is quite similar with the trade-off between Precision and Recall as above example shows. </p>
<p>When threshold is 0.1, the Sensitivity is 0.95, and the Specificity is 0.3467, then the FPR will be $1-0.3467 = 0.6533$. </p>
<p>Threshold is 0.1 :</p>
<ul>
<li>Sensitivity = 0.95</li>
<li>Specificity = 0.3467</li>
<li>FPR = $1-0.3467 = 0.6533$</li>
</ul>
<p>Threshold is 0.3 : </p>
<ul>
<li>Sensitivity = 0.7625</li>
<li>Specificity = 0.7400</li>
<li>FPR = $1-0.7400 = 0.26$</li>
</ul>
<p>Threshold is 0.5 :</p>
<ul>
<li>Sensitivity = 0.6125</li>
<li>Specificity = 0.8667</li>
<li>FPR = $1-0.8667 = 0.1333$</li>
</ul>
<p>Threshold is 0.7 :</p>
<ul>
<li>Sensitivity = 0.4</li>
<li>Specificity = 0.9533</li>
<li>FPR = $1-0.9533 = 0.0467$</li>
</ul>
<p>In this scenario, we also would think that when the threshold is 0.3 is the optimal cutoff because they seem the optimal values for both. However, we can see the overall quality of the classifier with the AUC values. </p>
<p>AUC (Area Under Curve) is the area under the ROC curve. The greater the AUC value is, the better classifier we get. </p>
<p>Below is the implementation of my own functions for developing all of the above metrics, such as Precision and Recall, Sensitivity and Specificity, ROC curves, and AUC values. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">#This function creates a dataframe that has TP, TN, FP, and FN values</span><br><span class="line">#This function should have same levels for the both target and pred variable</span><br><span class="line">tptnfpfn &lt;- function(x,y)&#123;</span><br><span class="line">  tap &lt;- tapply(x,x,length)</span><br><span class="line">  f.names &lt;- tap[1] %&gt;% names</span><br><span class="line">  </span><br><span class="line">  if(tap[1] &gt; tap[2])&#123;</span><br><span class="line">    target &lt;- ifelse(x == f.names, 0, 1)</span><br><span class="line">    pred &lt;- ifelse(y == f.names, 0, 1)</span><br><span class="line">  &#125;</span><br><span class="line">  if(tap[2] &gt; tap[1])&#123;</span><br><span class="line">    target &lt;- ifelse(x == f.names, 1, 0)</span><br><span class="line">    pred &lt;- ifelse(y == f.names, 1, 0)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  #target &lt;- x</span><br><span class="line">  #pred &lt;- y</span><br><span class="line">  </span><br><span class="line">  dat &lt;- data.frame(target, pred)</span><br><span class="line">  </span><br><span class="line">  TP &lt;- length(which(dat$target == 1 &amp; dat$pred == 1))</span><br><span class="line">  FP &lt;- length(which(dat$target == 0 &amp; dat$pred == 1))</span><br><span class="line">  TN &lt;- length(which(dat$target == 0 &amp; dat$pred == 0))</span><br><span class="line">  FN &lt;- length(which(dat$target == 1 &amp; dat$pred == 0))</span><br><span class="line">  </span><br><span class="line">  new.dat &lt;- data.frame(TP,FP,TN,FN)</span><br><span class="line">  return(new.dat)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Precision = TP / (TP + FP) &lt;- the denominator is total predicted positive values</span><br><span class="line">precision &lt;- function(tp.dat)&#123;</span><br><span class="line">  precision &lt;- tp.dat$TP / (tp.dat$TP + tp.dat$FP)</span><br><span class="line">  return(precision)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Recall = sensitivity = TP / (TP + FN) &lt;- the denominator is total actual positive values </span><br><span class="line"></span><br><span class="line">recall &lt;- function(tp.dat)&#123;</span><br><span class="line">  recall &lt;- tp.dat$TP / (tp.dat$TP + tp.dat$FN)</span><br><span class="line">  return(recall)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#Sensitivity = Recall</span><br><span class="line"></span><br><span class="line">#Specificity = TN / (TN + FP) &lt;- the denominator is total actual negative values</span><br><span class="line">spec &lt;- function(tp.dat)&#123;</span><br><span class="line">  specificity &lt;- tp.dat$TN / (tp.dat$TN + tp.dat$FP)</span><br><span class="line">  return(specificity)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#ROC = TPR vs FPR = Recall vs 1-TNR = TP/(TP+FN) vs FP/(FP+TN)</span><br><span class="line">roc.func &lt;- function(target,pred)&#123;</span><br><span class="line">  dummy &lt;- data.frame(TPR = rep(0, length(target)), </span><br><span class="line">                      FPR = rep(0, length(target)), </span><br><span class="line">                      Spec = rep(0,length(target)),</span><br><span class="line">                      Precision = rep(0, length(target)),</span><br><span class="line">                      f1score = rep(0, length(target)))</span><br><span class="line">  </span><br><span class="line">  tap &lt;- tapply(target,target,length)</span><br><span class="line">  if(tap[1] &gt; tap[2])&#123;</span><br><span class="line">    f.name &lt;- levels(as.factor(target))[2]</span><br><span class="line">    s.name &lt;- levels(as.factor(target))[1]</span><br><span class="line">  &#125;</span><br><span class="line">  if(tap[2] &gt; tap[1])&#123;</span><br><span class="line">    f.name &lt;- levels(as.factor(target))[1]</span><br><span class="line">    s.name &lt;- levels(as.factor(target))[2]</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  for(i in 1:length(target))&#123;</span><br><span class="line">    #splitting the probabilities by cutoff with same levels</span><br><span class="line">    pred.cutoff &lt;- ifelse(pred &gt;= sort(pred)[i], f.name, s.name)</span><br><span class="line">    </span><br><span class="line">    tptn &lt;- tptnfpfn(target,pred.cutoff)</span><br><span class="line">    </span><br><span class="line">    dummy$cutoff[i] &lt;- sort(pred)[i]</span><br><span class="line">    dummy$TPR[i] &lt;- recall(tptn)</span><br><span class="line">    dummy$FPR[i] &lt;- tptn$FP / (tptn$FP + tptn$TN)</span><br><span class="line">    dummy$Spec[i] &lt;- spec(tptn)</span><br><span class="line">    dummy$Precision[i] &lt;- precision(tptn)</span><br><span class="line">    dummy$f1score[i] &lt;- f1.score(tptn)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  #dummy$TPR &lt;- ifelse(dummy$TPR == &quot;NaN&quot;, 0, dummy$TPR)</span><br><span class="line">  #dummy$FPR &lt;- ifelse(dummy$FPR == &quot;NaN&quot;, 0, dummy$FPR)</span><br><span class="line">  return(dummy)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#This auc function is from below link. </span><br><span class="line">#Refer to </span><br><span class="line">#https://mbq.me/blog/augh-roc/</span><br><span class="line">#a little changes is applied into the codes from above link</span><br><span class="line">#This is using the test statistic from &quot;Mann-Whitney-Wilcoxon test&quot;</span><br><span class="line">#Further link:</span><br><span class="line">#https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test#Area-under-curve_(AUC)_statistic_for_ROC_curves</span><br><span class="line">auc.func &lt;- function(target, pred)&#123;</span><br><span class="line">  tap &lt;- tapply(target, target, length)</span><br><span class="line">  f.name &lt;- tap[1] %&gt;% names</span><br><span class="line">  </span><br><span class="line">  if(tap[1] &gt; tap[2])&#123;</span><br><span class="line">    target1 &lt;- ifelse(target == f.name, TRUE, FALSE)</span><br><span class="line">  &#125;</span><br><span class="line">  if(tap[2] &gt; tap[1])&#123;</span><br><span class="line">    target1 &lt;- ifelse(target == f.name, TRUE, FALSE)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  n1 &lt;- sum(!target1)</span><br><span class="line">  n2 &lt;- sum(target1)</span><br><span class="line">  U &lt;- sum(rank(pred)[!target1]) - n1 * (n1 + 1) / 2</span><br><span class="line">  </span><br><span class="line">  return(1 - U / (n1*n2))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Below is the results by built-in syntax in R with “ROCR” package</p>
<img src="/2019/10/16/AUROC/1.png" title="[table]">
<img src="/2019/10/16/AUROC/2.png" title="[table]">
<img src="/2019/10/16/AUROC/8.png" title="[table]">

<p>Below is the results by the created functions as above. </p>
<img src="/2019/10/16/AUROC/3.png" title="[table]">
<img src="/2019/10/16/AUROC/4.png" title="[table]">
<img src="/2019/10/16/AUROC/5.png" title="[table]">
<img src="/2019/10/16/AUROC/6.png" title="[table]">
<img src="/2019/10/16/AUROC/7.png" title="[table]">


<p>Full implementation will be <a href="https://github.com/DavidKwon91/MathematicsBehindAlgorithms/blob/master/AUROC/AUROC.md" target="_blank" rel="noopener">here</a></p>
<p>Reference:<br><a href="https://en.wikipedia.org/wiki/Precision_and_recall" target="_blank" rel="noopener">Precision and Recall</a><br><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">Receiver Operating Characteristic</a><br><a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors" target="_blank" rel="noopener">Type 1 and Type 2 Error</a><br><a href="https://blog.revolutionanalytics.com/2017/03/auc-meets-u-stat.html" target="_blank" rel="noopener">AUC Meets the Wilcoxon-Mann-Whitney U-Statistic</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/10/05/Monty-Hall/" class="image is-7by1">
            <img class="thumbnail" src="/images/montyhall.png" alt="Monty Hall">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-10-05T09:53:20.000Z">2019-10-05</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Monty-Hall/">Monty Hall</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    3 minutes read (About 516 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/10/05/Monty-Hall/">Monty Hall</a>
            
        </h1>
        <div class="content">
            <h2 id="Monty-Hall-Problem-with-Bayes’-Theorem"><a href="#Monty-Hall-Problem-with-Bayes’-Theorem" class="headerlink" title="Monty Hall Problem with Bayes’ Theorem"></a>Monty Hall Problem with Bayes’ Theorem</h2><p>Monty Hall Problem is a famous probability problem based on the TV show, “Let’s make a Deal.” </p>
<p>Suppose that you are a contestant and are asked to choose a door among 3 doors, which contains 1 car and 2 goats. If you choose a door that is car behind the door, then you win the prize. </p>
<p>Let C = car behind the door $i$, and D = open door $j$. </p>
<p>Here, we can formulate the probability for the car behind the door as the following:</p>
<p>$$P(C=1) = 1/3 \qquad P(C = 2) = 1/3 \qquad P(C = 3) = 1/3$$</p>
<p>Let’s say you pick the door 1, and host pick one of door 2 or 3, which goat behind the door. You now have a choice whether you stick with the originally choosed door and you switch. </p>
<p>Suppose the host choose the door 3, given that a goat is behind the door 3. </p>
<p>The trick of the probablity here is that the host knows which door has a car behind it and will never open the door. Then, there’s 50-50 chances that one of the two doors, which is not the door that host opens, has a car behind it. </p>
<p>The probability of the door that is chosen by the host, given that the car behind the door, is zero, since the host never pick the door that has a car,<br>the probability of the door that is chosen by the contestant, given that the car behind the door, is a half, since it is the half probability,<br>and the probability of the door that is not chosen by the host, given that the car behind the door, is one, because the host never pick the door that has a car and the door that the contestant chose, there’s only one door left. </p>
<p>Then, the likelihood will be </p>
<p>$$P(D = 3 | C = 1) = 1/2 \qquad P(D = 3 | C = 2) = 1 \qquad P(D = 3 | C = 1) = 0 $$</p>
<p>Now, the formula of Bayes Theorem is the following:</p>
<p>$$P(C = i | D = 3) = \frac{P(D = 3 | C = i)P(C = i)}{P(D = 3)}$$</p>
<p>It is easy to answer that the probability for $P(D = 3)$ is $\frac{1}{2}$, because you already choose door 1, and the host will choose one of the rest 2 doors. However, we can calculate the marginal probability for $P(D = 3)$ as the following. </p>
<p>$$P(D = 3) = \sum_{i=1}^{3} P(C = i, D = 3) = \sum_{i=1}^{3} P(D = 3 | C = i)P(C = i) = \\ P(D = 3 | C = 1)P(C = 1) + P(D = 3 | C = 2)P(C = 2) + P(D = 3 | C = 3)P(C = 3) = \\ \frac{1}{3} \times \frac{1}{2} + \frac{1}{3} \times 1 + \frac{1}{3} \times 0 = \frac{1}{2}$$</p>
<p>Now, let’s calculate the posterior probability in the Bayes Theorem, which is the probability of the door has a car behind it, given the door 3 has opened. </p>
<p>$$P(C = 1 | D = 3) = \frac{P(D = 3 |C = 1)P(C = 1)}{P(D = 3)} =  \frac{\frac{1}{2} \times \frac{1}{3}}{\frac{1}{2}} = \frac{1}{3}$$</p>
<p>$$P(C = 2 | D = 3) = \frac{P(D = 3 |C = 2)P(C = 2)}{P(D = 3)} =  \frac{1 \times \frac{1}{3}}{\frac{1}{2}} = \frac{2}{3}$$</p>
<p>$$P(C = 3 | D = 3) = \frac{P(D = 3 |C = 3)P(C = 3)}{P(D = 3)} =  \frac{0 \times \frac{1}{3}}{\frac{1}{2}} = 0$$</p>
<p>Therefore, as a contestant, you have to switch your choice to door 2 that has more probability than the door you chose at the first time. </p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/09/19/DecisionTree/" class="image is-7by1">
            <img class="thumbnail" src="/images/treeimage.jpg" alt="DecisionTree">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-19T08:13:52.000Z">2019-09-19</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Decision-Tree/">Decision Tree</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    15 minutes read (About 2225 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/19/DecisionTree/">DecisionTree</a>
            
        </h1>
        <div class="content">
            <h1 id="Introduction-to-Decision-Tree"><a href="#Introduction-to-Decision-Tree" class="headerlink" title="Introduction to Decision Tree"></a>Introduction to Decision Tree</h1><p>In this post, I will discuss about Decision Tree, which is the fundamental algorithm of the well-known Tree-based algorithms. This post will also address two metrics that are employed in Decision tree and many other machine learning algorithms, even in deep learning. </p>
<h2 id="1-Metric"><a href="#1-Metric" class="headerlink" title="1. Metric"></a>1. Metric</h2><p>Why do we need the “Metric” in tree-based algorithm? </p>
<p>The answer will be to find the best splits in the tree. We use the metric, calculate a variable, measure the split score with the value, and choose the value for the best split at each step of finding the split or the predictors in the tree. Many other metrics or score functions are used to find the “best” split for different tree-based algorithms. Two metrics will be discussed, which are Gini Impurity (or Gini Index) and Information Gain with Entropy. </p>
<h4 id="1-1-Gini-Index"><a href="#1-1-Gini-Index" class="headerlink" title="1.1 Gini Index"></a>1.1 Gini Index</h4><p>Suppose we have a dependent categorical variable, which have $\mathcal{J}$ classes. Let $i \in \{1,2,…,\mathcal{J}\}$ and $p_i$ is the probability given each factor levels of dependent variable.d</p>
<p>Then, the Gini Index, or Gini Impurity is defined as below, </p>
<p>$$I_{\mathcal{G}}(p) = \sum_{i=1}^{J}p_i \sum_{k \neq i}p_k = \sum_{i=1}^{J}p_i(1-p_i) = \sum_{i=1}^{J}(p_i-p_i^2) = \sum_{i=1}^{J}p_i - \sum_{i=1}^{J}p_i^2 \\ = 1-\sum_{i=1}^{J}p_i^2$$</p>
<p>For example, </p>
<p>suppose we have a binary variable that follows below. </p>
<p>$X = [FALSE,FALSE,TRUE,TRUE,….,TRUE,FALSE]$</p>
<img src="/2019/09/19/DecisionTree/image1.png" title="[table]">

<p>Then, the Gini Impurity will be the followng by the definition,</p>
<p>$$1 - (\frac{8}{20})^2 - (\frac{12}{20})^2 = 0.48$$</p>
<p>What about the Gini Impurity for this binary variable?</p>
<img src="/2019/09/19/DecisionTree/image2.png" title="[table]">

<p>Then, the Gini Impurity will be</p>
<p>$$1 - (\frac{1}{20})^2 - (\frac{19}{20})^2 = 0.095$$</p>
<p>Now, we see that the smaller Gini Impurity is, we find the better split point.</p>
<p>Therefore, we want to find the smaller Gini Impurity score as possible as we can at each step to find best split in the tree. </p>
<p>Now, let we have two binary variables and make table for each factors like the below, </p>
<p>$$Y = [FALSE, FALSE, … ,  TRUE] \\ X = [YES, YES, … , NO]$$</p>
<p>And, we want to predict $Y$ with $X$. </p>
<img src="/2019/09/19/DecisionTree/image3.png" title="[table]">

<p>We will have only one stump in a Decision Tree for this table. The root node, which is the very top of the tree, will be the predictor, $X = [YES, YES, … , NO]$</p>
<p>The tree is something like..</p>
<img src="/2019/09/19/DecisionTree/Tree1.png" title="[table]">

<p>The typical tree in real world is much more complicated with more nodes and splits, since they have many predictors.<br>This tree that has only one split is sometimes called a stump, which is often called a weak learner in Adaboost.<br>But, I will just call this as a tree for this example.  </p>
<p>Then, the process of finding Gini Impurity is the below;</p>
<p>1) Find Conditional probability</p>
<p>For example, the Conditional probability given $X$ is</p>
<p>$$P(Y = FALSE |X = YES) = \frac{5}{7} \\ P(Y = TRUE | X = YES) = \frac{2}{7} \\ P(Y = FALSE | X = NO) = \frac{3}{13} \\ P(Y = TRUE | X = NO) = \frac{10}{13}$$</p>
<p>2) Find each Gini Index for the conditional probability</p>
<p>$$I_{Gini}(Y|X=YES) = 1 - (\frac{5}{7})^2 - (\frac{2}{7})^2  =  0.4081633 \\ I_{Gini}(Y|X=NO) = 1 - (\frac{3}{13})^2 - (\frac{10}{13})^2 = 0.3550296 $$</p>
<p>Then, we find a Gini Impurity score for a leaf node in this tree, which is the very bottom of the tree or the final classification. </p>
<img src="/2019/09/19/DecisionTree/Tree2.png" title="[table]">



<p>3) Find Marginal probability</p>
<p>The margianl probabilty given $X$ is</p>
<p>$$P_X(X=YES) = \frac{7}{20} \\ P_X(X=NO) = \frac{13}{20}$$</p>
<p>Since the tree is splitted by $X$, we find the Gini Impurity of the total tree. </p>
<p>4) Multiply and add the Gini Index and marginal probability associated with the given probability respectively. Finally, we have Gini Impurity Score for a split. </p>
<p>Then, the <em>Total Gini Impurity Score</em> for this tree is, </p>
<p>$$I_{Gini}(Y|X=YES) \times P_X(X=YES) + I_{Gini}(Y|X=NO) \times P_X(X=NO) = \\ 0.4081633 \times \frac{7}{20} + 0.3550296 \times \frac{13}{20} = 0.3736264$$</p>
<p>Then, we eventually find the total Gini Impurity score for a tree. </p>
<img src="/2019/09/19/DecisionTree/Tree3.png" title="[table]">

<p>In real world problem, we have many predictors and so many nodes, possibly many trees such in Random Forest. </p>
<p>With this process of finding Gini Impurity, we compare the Gini score and find the best split point of the best predictor with best splits. </p>
<h4 id="1-2-Entropy"><a href="#1-2-Entropy" class="headerlink" title="1.2 Entropy"></a>1.2 Entropy</h4><p>Entropy is defined as below; </p>
<p>$$H(T) = -\sum_{i=1}^{J}p_ilog_2(p_i)$$</p>
<p>And the process of finding best split in a tree is to find the Information Gain. </p>
<p>The Information Gain is often called <em>Kullback-Leiber divergence</em> and defined as below;</p>
<p>$$IG(T,a) = H(T) - H(T|a)$$</p>
<p>where $IG(Y,X)$ is the Information Gain of Y given X, $H(Y)$ is the parent Entropy of Y, and $H(Y|X)$ is the children Entropy of Y given X, and the $H(Y|X)$ is often called <em>cross-entropy</em>.</p>
<p>The process of finding Information Gain is very similar to the Gini Score, but here we find the bigger value of IG. </p>
<p>I will use the same example problem as above. </p>
<p>For the above example, </p>
<p>$$Y = [FALSE, FALSE, … ,  TRUE] \\ X = [YES, YES, … , NO]$$</p>
<img src="/2019/09/19/DecisionTree/image3.png" title="[table]">

<p>Then, as I have shown, find the conditional probability. </p>
<p>1) Find the conditional probability given $X$</p>
<p>$$P(Y = FALSE |X = YES) = \frac{5}{7} \\ P(Y = TRUE | X = YES) = \frac{2}{7} \\ P(Y = FALSE | X = NO) = \frac{3}{13} \\ P(Y = TRUE | X = NO) = \frac{10}{13}$$</p>
<p>2) Find the entropy given $X$</p>
<p>$$I_{Entropy}(Y|X=YES) = -\frac{5}{7}log_2(\frac{5}{7}) - \frac{2}{7}log_2(\frac{2}{7}) = 0.8631206\\ I_{Entropy}(Y|X=NO) = -\frac{3}{13}log_2(\frac{3}{13}) - \frac{10}{13}log_2(\frac{10}{13})  =  0.7793498 $$</p>
<p>3) Find the marginal probability</p>
<p>The margianl probabilty given $X$ is</p>
<p>$$P_X(X=YES) = \frac{7}{20} \\ P_X(X=NO) = \frac{13}{20}$$</p>
<p>4) Calculate the Total Entropy and Cross-Entropy</p>
<p>Total Entropy for a tree divided by $X$ is</p>
<p>$$-P_X(X=YES) \times log_2(P_X(X=YES))-P_X(X=NO) \times log_2(P_X(X=NO)) \\ = -\frac{7}{20}log_2(\frac{7}{20}) - \frac{13}{20}log_2(\frac{13}{20}) \\ = 0.9340681$$</p>
<p>Cross Entropy for a leaf node is</p>
<p>$$I_{Entropy}(Y|X=YES) \times  P_X(X=YES) + I_{Entropy}(Y|X=NO) \times P_X(X=NO) \\ = 0.8631206 \times \frac{7}{20} + 0.7793498 \times \frac{13}{20} \\ = 0.8086696$$</p>
<p>5) Find Information Gain with Total Entropy and Cross Entropy</p>
<p>Information Gain = Total Entropy - Cross Entropy</p>
<p>$$= 0.9340681 - 0.8086696 = 0.1253985$$ </p>
<p>For another example that shows why bigger Information Gain is better, </p>
<p>suppose we have the dataset following; </p>
<img src="/2019/09/19/DecisionTree/table.png" title="[table]">

<p>It’s obviously better to split for Y given X than the above example, since this has only one mistake. </p>
<p>If we find the Information Gain of this data,</p>
<p>The Total Entropy for a tree divided by $X$ is 1, since this splits exactly the same number of target observations with 10 and 10. </p>
<p>$$-P_X(X=YES) \times log_2(P_X(X=YES))-P_X(X=NO) \times log_2(P_X(X=NO)) = \\ -\frac{10}{20}log_2(\frac{10}{20}) - \frac{10}{20}log_2(\frac{10}{20}) \\ = 0.5 + 0.5 = 1$$</p>
<p>The cross entropy for a leaf node is</p>
<p>$$I_{Entropy}(Y|X=YES) \times  P_X(X=YES) + I_{Entropy}(Y|X=NO) \times P_X(X=NO) \\ = 0.234478$$</p>
<p>calculated by the same process of the above.</p>
<p>Then, the Information Gain is</p>
<p>$$1 - 0.234478 = 0.7655022$$</p>
<p>Therefore, we find the bigger Information Gain as possible as we can. </p>
<p>As a summary, Decision Tree Algorithm is..</p>
<p>1) Find the best split with the metric score for each splits and each predictors. </p>
<p>2) Compare the metric score with every splits and other predictors and find the best split and best predictors. </p>
<p>3) The best predictors that has the best metric score will be the roof node, and keep building a tree with the metric scores. </p>
<p>4) The very bottom of the tree, which is the leaf node, will be our final classification. </p>
<p>If we have continuous predictors, there is no split point like “YES” or “NO”. Therefore, </p>
<p>1) we sort the data associated with the numerical predictors, </p>
<p>2) calculate the average for all adjacent indexes of the numerical predictor, </p>
<p>find the Gini Impurity for all the adjacent average value, </p>
<p>and compare the Gini Impurity to find the best split point of adjacent average of the numerical predictors. </p>
<p>Below is the Implementation in R. I used iris dataset, and a dataset that I created.<br>Some of codes are skipped, and you can refer to my Gibhub website to see full algorithm. </p>
<p><a href="https://github.com/DavidKwon91/MathematicsBehindAlgorithms" target="_blank" rel="noopener">DavidGithub</a></p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#removing one of the classes of the target variable, virginica in Iris Dataset. </span><br><span class="line">#to make target as a binary variable</span><br><span class="line">iris1 &lt;- iris[which(iris$Species != &quot;virginica&quot;),]</span><br><span class="line"></span><br><span class="line">#removing the factor level that we don&apos;t have any more</span><br><span class="line">iris1$Species &lt;- as.factor(as.character(iris1$Species))</span><br><span class="line"></span><br><span class="line">#quick decision tree built in R, rpart</span><br><span class="line">tr &lt;- rpart(Species~., training)</span><br><span class="line">rpart.plot(tr)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/19/DecisionTree/IntroductionDecisionTree-1.png" title="[tree1]">


<p>This is the prediction by Decision Tree built in R. </p>
<img src="/2019/09/19/DecisionTree/IntroductionDecisionTree-2.png" title="[tree1]">


<p>Here is the Decision Tree algorithm with the introduced two metrics that I built. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line">#average function to create adjacent average between predictor indexes</span><br><span class="line">avg &lt;- function(x1,x2)&#123;sum(x1,x2)/2&#125;</span><br><span class="line"></span><br><span class="line">#gini function for a leaf</span><br><span class="line">gini &lt;- function(x)&#123;</span><br><span class="line">  </span><br><span class="line">  if(dim(x)[1]==1)&#123;</span><br><span class="line">    return(1)</span><br><span class="line">  &#125;</span><br><span class="line">  else&#123;</span><br><span class="line">    #conditional probability given predictor (row : target, column : predictor)</span><br><span class="line">    p11&lt;-x[1,1]/sum(x[,1])</span><br><span class="line">    p21&lt;-x[2,1]/sum(x[,1])</span><br><span class="line">    p12&lt;-x[1,2]/sum(x[,2])</span><br><span class="line">    p22&lt;-x[2,2]/sum(x[,2])</span><br><span class="line">    </span><br><span class="line">    a.false.gini &lt;- 1-p11^2-p21^2</span><br><span class="line">    a.true.gini &lt;- 1-p12^2-p22^2</span><br><span class="line">    </span><br><span class="line">    #marginal probability</span><br><span class="line">    a.false.prob &lt;- (x[1,1]+x[1,2]) / sum(x)</span><br><span class="line">    a.true.prob &lt;- (x[2,1]+x[2,2]) / sum(x)</span><br><span class="line">    </span><br><span class="line">    gini.imp &lt;- a.false.prob * a.false.gini + a.true.prob * a.true.gini</span><br><span class="line">    return(gini.imp)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#log function with base 2 for entropy</span><br><span class="line">log2 &lt;- function(x)&#123;</span><br><span class="line">  if(x!=0)&#123;</span><br><span class="line">    return(log(x,base=2))</span><br><span class="line">  &#125;</span><br><span class="line">  if(x==0)&#123;</span><br><span class="line">    return(0)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#entropy function for a leaf</span><br><span class="line">entropy &lt;- function(x)&#123;</span><br><span class="line">  </span><br><span class="line">  if(dim(x)[1]==1)&#123;</span><br><span class="line">    return(0)</span><br><span class="line">  &#125;</span><br><span class="line">  else&#123;</span><br><span class="line">  	#conditional probability given predictor (row : target, column : predictor)</span><br><span class="line">    p11&lt;-x[1,1]/sum(x[,1])</span><br><span class="line">    p21&lt;-x[2,1]/sum(x[,1])</span><br><span class="line">    p12&lt;-x[1,2]/sum(x[,2])</span><br><span class="line">    p22&lt;-x[2,2]/sum(x[,2])</span><br><span class="line">    </span><br><span class="line">    #Calculating weights, which is the bottom of the tree</span><br><span class="line">    a.false.entropy &lt;- -(p11*log2(p11)+p21*log2(p21))</span><br><span class="line">    a.true.entropy &lt;- -(p12*log2(p12)+p22*log2(p22))</span><br><span class="line">    </span><br><span class="line">    #marginal probability</span><br><span class="line">    a.false.prob &lt;- (x[1,1]+x[2,1]) / sum(x)</span><br><span class="line">    a.true.prob &lt;- (x[1,2]+x[2,2]) / sum(x)</span><br><span class="line">    </span><br><span class="line">    #cross entropy</span><br><span class="line">    weighted.entropy &lt;- a.true.prob*a.true.entropy + a.false.prob*a.false.entropy</span><br><span class="line">    </span><br><span class="line">    #total entropy</span><br><span class="line">    total.entropy &lt;- -(a.false.prob*log2(a.false.prob) + a.true.prob*log2(a.true.prob))</span><br><span class="line">    </span><br><span class="line">    #Information Gain, which is the tree score to find best split</span><br><span class="line">    #If the bigger this value is, we find the better split</span><br><span class="line">    #maximum value is 1</span><br><span class="line">    IG &lt;- total.entropy - weighted.entropy</span><br><span class="line">    </span><br><span class="line">    return(IG) </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Calculating impurity to find which predictor is the best to split, which will be the top of the tree or first split in the tree</span><br><span class="line">var.impurity &lt;- function(x, dat, fun)&#123;</span><br><span class="line">  imp.dat &lt;- data.frame(matrix(0, nrow=nrow(dat)-1, ncol=3))</span><br><span class="line">  colnames(imp.dat) &lt;- c(&quot;index&quot;, &quot;impurity&quot;, &quot;adj.avg&quot;)</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">  for(i in 1:(nrow(dat)-1))&#123;</span><br><span class="line">    imp.dat[i,1] &lt;- paste0(&quot;between &quot;, i, &quot; and &quot;, i+1)</span><br><span class="line">    #average value of the adjacent values</span><br><span class="line">    a &lt;- avg(x[i], x[i+1])</span><br><span class="line">    </span><br><span class="line">    predictor.name &lt;- colnames(dat)[which(sapply(dat, function(x,want) isTRUE(all.equal(x,want)),x)==TRUE)]</span><br><span class="line">    </span><br><span class="line">    #Sorting the data by the predictor</span><br><span class="line">    dat1 &lt;- dat[order(x,decreasing=FALSE),]</span><br><span class="line">    mat &lt;- as.matrix(table(dat1[,predictor.name] &lt; a, dat1[,target] ))</span><br><span class="line">    </span><br><span class="line">    #apply the metric, Gini or Entropy</span><br><span class="line">    imp.dat[i,2] &lt;- fun(mat)</span><br><span class="line">    imp.dat[i,3] &lt;- a</span><br><span class="line">    &#125;</span><br><span class="line">  return(imp.dat)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#this function will give you the best split score for each predictors</span><br><span class="line">impurity.fun &lt;- function(dat, fun)&#123;</span><br><span class="line">  predictors &lt;- colnames(dat)[!colnames(dat) %in% target]</span><br><span class="line">  var.impur.dat &lt;- data.frame(matrix(0, nrow=length(predictors),ncol=2))</span><br><span class="line">  colnames(var.impur.dat) &lt;- c(&quot;var&quot;, &quot;impurity&quot;)</span><br><span class="line">  </span><br><span class="line">  for(i in 1:(ncol(dat)-1))&#123;</span><br><span class="line">    var.impur.dat[i,1] &lt;- predictors[i]</span><br><span class="line">    if(fun == &quot;gini&quot;)&#123;</span><br><span class="line">      #the least score of gini is the best split </span><br><span class="line">      var.impur.dat[i,2] &lt;- min(var.impurity(dat[,i], dat, gini)$impurity)</span><br><span class="line">    &#125;</span><br><span class="line">    if(fun == &quot;entropy&quot;)&#123;</span><br><span class="line">      #the greates score of entropy is the best split</span><br><span class="line">      var.impur.dat[i,2] &lt;- max(var.impurity(dat[,i], dat, entropy)$impurity)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  return(var.impur.dat)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#give you the best predictor to split or the top of the tree</span><br><span class="line">topTree.predictor &lt;- function(x,fun)&#123;</span><br><span class="line">  if(fun == &quot;entropy&quot;)&#123;</span><br><span class="line">    return(which.max(impurity.fun(x, &quot;entropy&quot;)[,2]))</span><br><span class="line">  &#125;</span><br><span class="line">  if(fun == &quot;gini&quot;)&#123;</span><br><span class="line">    return(which.min(impurity.fun(x, &quot;gini&quot;)[,2]))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#The best split point associated with the best predictor</span><br><span class="line">impurityOfbest &lt;- function(dat, best.pred, fun)&#123;</span><br><span class="line">  if(fun == &quot;entropy&quot;)&#123;</span><br><span class="line">    impurity.pred &lt;- var.impurity(dat[,best.pred], dat, entropy)$adj.avg[which.max(var.impurity(dat[,best.pred], dat, entropy)$impurity)]</span><br><span class="line">  &#125;</span><br><span class="line">  if(fun == &quot;gini&quot;)&#123;</span><br><span class="line">    impurity.pred &lt;- var.impurity(dat[,best.pred], dat, gini)$adj.avg[which.min(var.impurity(dat[,best.pred], dat, gini)$impurity)]</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  print(paste0(&quot;Best predictor, which is top tree node is &quot;, colnames(dat)[best.pred], &quot; with best split is  &quot;, impurity.pred, &quot; by the metric, &quot;, fun))</span><br><span class="line">  return(impurity.pred)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#by Entropy metric, we want to find the maximum entropy score</span><br><span class="line">fun &lt;- &quot;entropy&quot;</span><br><span class="line">target &lt;- &quot;Species&quot;</span><br><span class="line"></span><br><span class="line">imp.pred &lt;- topTree.predictor(iris1, fun) #This is the best predictor that splits our target the best. </span><br><span class="line">best.impur &lt;- impurityOfbest(iris1, imp.pred, fun) #This is the best split point for the best predictor. </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Let&apos;s see how well this value calculated by the function predicts</span><br><span class="line">table(iris1[,imp.pred] &lt; best.impur, iris1$Species)</span><br><span class="line">#perfectly predicted in training set</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pred.by&lt;- as.factor(ifelse(iris1[,imp.pred] &lt; best.impur, &quot;setosa&quot;,&quot;versiclor&quot;))</span><br><span class="line"></span><br><span class="line">t1 &lt;- iris1 %&gt;% </span><br><span class="line">  ggplot(aes(x=Petal.Width, y=Petal.Length ,col=Species)) + </span><br><span class="line">  geom_jitter() +</span><br><span class="line">  ggtitle(&quot;Actual&quot;)</span><br><span class="line">t2 &lt;- iris1 %&gt;% </span><br><span class="line">  ggplot(aes(x=Petal.Width, y=Petal.Length ,col=pred.by)) + </span><br><span class="line">  geom_jitter() +</span><br><span class="line">  geom_vline(xintercept =  best.impur, colour=&quot;blue&quot;, linetype=&quot;dashed&quot;) + </span><br><span class="line">  annotate(geom=&quot;text&quot;, label=best.impur, x=best.impur, y=0, vjust=-1) +</span><br><span class="line">  ggtitle(&quot;Predicted&quot;)</span><br><span class="line"></span><br><span class="line">grid.arrange(t1,t2)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/19/DecisionTree/Prediction-1.png" title="[tree1]">

















        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-15T09:07:27.000Z">2019-09-15</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/DNN-Optimizers/">DNN-Optimizers</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    6 minutes read (About 848 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/15/DNN-Optimizer/">Optimizers</a>
            
        </h1>
        <div class="content">
            <h2 id="Deep-Neural-Networks"><a href="#Deep-Neural-Networks" class="headerlink" title="Deep Neural Networks"></a>Deep Neural Networks</h2><h3 id="Simple-Intro-to-DNN"><a href="#Simple-Intro-to-DNN" class="headerlink" title="Simple Intro to DNN"></a>Simple Intro to DNN</h3><p> DNN(Deep Neural Networks) is a deep structured model of ANN(Artificial Neural Networks), which is a type of machine learning inspired by biological neural networks. </p>
<p> DNN is used and applied widely in many fields such as computer vision, speech recognition, natural language processing, and bioinformatics. </p>
<p> For my understand, ANN is to stack many models with various weights and biases such as logistic regression models, and DNN is a complex body that consist of huge amount of models. Therefore, some DNN model has more than 20 million parameters such as Xception invented from Google. </p>
<p>However, DNN models have very deep and complex structures. As any other complexity in this world brings about problems, DNN has some problems that has to be solved such as slow computational time, vanishing/exploding gradients problems. </p>
<p>I am going to disscuss about faster optimizers rather than Stochastic Gradient Descent, which is one of ways to speed up training time for DNN. </p>
<h3 id="SGD-Stochastic-Gradient-Descent"><a href="#SGD-Stochastic-Gradient-Descent" class="headerlink" title="SGD (Stochastic Gradient Descent)"></a>SGD (Stochastic Gradient Descent)</h3><p>Let’s begin with talking about what the SGD is. </p>
<p>Regular Gradient Descent is to simply updates the weights by calculating and subtracting the gradient of cost function with regards to the weights. </p>
<p>Definition: </p>
<p>$$\theta \leftarrow \theta - \eta\Delta_{\theta}\mathcal{J}(\theta)$$</p>
<p>The formula in the definition is to update the weights by subtracting or adding the gradients of cost function to take small steps down the slope. </p>
<p>So SGD does not care about the previous weights or gradients, but it only cares about current weights and gradients. This is where the Momentum optimization is introduced. </p>
<h3 id="SGD-with-Momentum-optimization"><a href="#SGD-with-Momentum-optimization" class="headerlink" title="SGD with Momentum optimization"></a>SGD with Momentum optimization</h3><p>SGD with Momentum Optimization is simply added some vector in the process of SGD, which is called momentum vector. </p>
<p>Definition:</p>
<p>$$\mathcal{m} \leftarrow \beta\mathcal{m} - \eta\Delta_{\theta} \mathcal{J}(\theta)$$</p>
<p>$$\theta \leftarrow \theta + \mathcal{m}$$</p>
<p>We update the weight by updating the momenum vector as above. Let’s imagine that we starts with the first initialized weights at the first iteration. The moment vector, $\mathcal{m}$, is started with zero, then the weights, $\theta$, will be just updated by the gradients as Gradient Decsent does. At the second iteration, the momentum vector is the gradients calculated at the first iteration. We are updating the gradients of the first iteration, which is momentum vector, by subtracting or adding the gradients at the current step, and adding the updated the gradients or the momentum vector. At the third iteration, the momentum vector would be the updated momentum vector at the second iteration, which is calculated by the first gradients. We update the wegiths by updating and adding the momentum vector. </p>
<p>To prevent the momentum vector growing too large or too small, we slightly reduced the value by a hyperparameter, $\beta$, which is called momentum. If the momentum, $\beta = 0$, then it’s the same as SGD. The typical value of momentum is 0.9. </p>
<h3 id="Nesterov-Accelerated-Gradient"><a href="#Nesterov-Accelerated-Gradient" class="headerlink" title="Nesterov Accelerated Gradient"></a>Nesterov Accelerated Gradient</h3><p>A new Momentum optimization is introduced, which is called Nesterov Accelerated Gradient. In the original Momentum Optimization, we update the momenum vector at the local position of the current weight; however, in NAG optimization, we update the momentum vector with the gradient that is measured at $\theta + \beta\mathcal{m}$. </p>
<p>Definition:</p>
<p>$$\mathcal{m} \leftarrow \beta\mathcal{m} - \eta\Delta_{\theta}\mathcal{J}(\theta + \beta\mathcal{m})$$</p>
<p>$$\theta \leftarrow \theta + \mathcal{m}$$</p>
<p>Since this gradient has the information of both of the weight and the previous momentum, this will help update the gradient more accurate, improve the directions to the optimum, and eventually converges faster. </p>
<h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>AdaGrad is the different version of gradient descent optimization than the momentum based optimization. It’s scaling down the gradient vector by dividing some factors measured along the steepness of the gradient. </p>
<p>Definition:</p>
<p>$$s \leftarrow s + \Delta_{\theta}\mathcal{J}(\theta)\otimes\Delta_{\theta}\mathcal{J}(\theta)$$</p>
<p>$$\theta \leftarrow \theta - \eta\Delta_{\theta}\mathcal{J}(\theta)\oslash\sqrt{s + \epsilon}$$</p>
<p>In the first step, we update the factor s by adding the element-wise multiplication of the gradient elements, that is the square of the gradients. It will be the same as $s_{i} \leftarrow + (\partial \mathcal{J}(\theta)/\partial \Theta_{i})^2$ for each element $s_{i}$. The vector, s, will accumulate the squares of the partial derivative of the cost function with regards to parameter $\Theta_{i}$. Therefore, if the gradient of the cost function is big, the factor s will get larger and larger at each iteration. </p>
<p>In the second step, the gradient of the cost function will be divided by the square root of the factor that is updated with the square of the gradients in the first step. The $\epsilon$ is the smoothing term to prevent dividing by zero, if the factor, s, is zero. </p>
<p>To sum up, if the direction to the global optimum is steep at the $i^{th}$ iteration, then the factor gets larger and so the term, $\eta\Delta_{\theta}\mathcal{J}(\theta)\oslash\sqrt{s + \epsilon}$,  will be smaller. However, because it’s scaling down the gradient quickly the iterations, when it’s reaching to the optimum, the gradient will be too small. </p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/09/15/GaussianProcess/" class="image is-7by1">
            <img class="thumbnail" src="/images/GP image.gif" alt="GaussianProcess">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-15T09:07:27.000Z">2019-09-15</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Gaussian-Process/">Gaussian Process</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    12 minutes read (About 1840 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/15/GaussianProcess/">GaussianProcess</a>
            
        </h1>
        <div class="content">
            <h2 id="Introduction-to-Gaussian-Process"><a href="#Introduction-to-Gaussian-Process" class="headerlink" title="Introduction to Gaussian Process"></a>Introduction to Gaussian Process</h2><p>In this post, I will discuss about Gaussian Process, which employs Gaussian distribution (also often called normal distribution that plays an important role in statistics. The previous posts, such as CLT, Cholesky Decomposition, and Schur Complement, are posted because they are needed to understand the Gaussian Process. </p>
<p>Before I start with Gaussian Process, some important properties of Gaussian Distribution will be addressed. </p>
<h3 id="large-Gaussian-large-Distribution"><a href="#large-Gaussian-large-Distribution" class="headerlink" title="$\large{Gaussian}$ $\large{ Distribution}$"></a>$\large{Gaussian}$ $\large{ Distribution}$</h3><p>If we have random vector $X$ following Gaussian Distribution, then we have </p>
<p>$$X = \begin{bmatrix} X_1 \\ X_2 \\ … \\ X_n \end{bmatrix} \sim \mathcal{N}(\mu, \Sigma)$$</p>
<p>where $\Sigma = Cov(X_i, X_j) = E[(X_i - \mu_i)(X_j - \mu_j)^{T}]$</p>
<p>Now, Suppose we have two vectors, $X$ and $Y$ that are subsets of a random variable, Z, with the following; </p>
<p>$$P_Z = P_{X,Y} = \begin{bmatrix} X \\ Y \end{bmatrix} \sim \mathcal{N}(\mu, \Sigma) = \mathcal{N}(\begin{bmatrix} \mu_X \\ \mu_Y \end{bmatrix}, \begin{bmatrix} \Sigma_{XX} &amp; \Sigma_{XY} \\ \Sigma_{YX} &amp; \Sigma_{YY} \end{bmatrix})$$</p>
<p>Then, the following properties holds; </p>
<h3 id="1-Normalization"><a href="#1-Normalization" class="headerlink" title="1. Normalization"></a>1. Normalization</h3><p>The density function will be </p>
<p>$$\int_{Z} p(Z;\mu,\Sigma) dz = 1$$</p>
<p>where $Z = \begin{bmatrix} X \\ Y \end{bmatrix}$</p>
<h3 id="2-Marginalization"><a href="#2-Marginalization" class="headerlink" title="2. Marginalization"></a>2. Marginalization</h3><p>The maginal densities will be</p>
<p>$$p(X) = \int_{Y} p(X,Y; \mu, \Sigma) dy \\ p(Y) = \int_{X} p(X,Y; \mu, \Sigma) dx$$</p>
<p>which are Gaussian, therefore</p>
<p>$$X \sim \mathcal{N}(\mu_X, \Sigma_{XX}) \\ Y \sim \mathcal{N}(\mu_Y, \Sigma_{YY})$$</p>
<h3 id="3-Conditioning"><a href="#3-Conditioning" class="headerlink" title="3. Conditioning"></a>3. Conditioning</h3><p>The conditional densities will also be Gaussian, and the conditional expected value and covariance are calculated with the properties of Schur Complement as the following. </p>
<p>Refer to the wiki, <a href="https://en.wikipedia.org/wiki/Schur_complement" target="_blank" rel="noopener">Schur Complement</a>. </p>
<p>The conditional expectation and covariance of X given Y is the Schur Complement of C in $\Sigma$ where if $\Sigma$ is defined as </p>
<p>$$\Sigma = \begin{bmatrix} A &amp; B \\ B^{T} &amp; C \end{bmatrix}$$ </p>
<p>$$Cov(X|Y) = A-BC^{-1}B^{T} \\ E(X|Y) = E(X) + BC^{-1}(Y-E(Y))$$</p>
<p>Hence, the conditional covariance of X given Y will be</p>
<p>$$P_{X,Y} = \mathcal{N}(\begin{bmatrix} \mu_X \\ \mu_Y \end{bmatrix}, \begin{bmatrix} \Sigma_{XX} &amp; \Sigma_{XY} \\ \Sigma_{YX} &amp; \Sigma_{YY} \end{bmatrix})$$</p>
<p>$$X | Y \sim \mathcal{N}(\mu_X + \Sigma_{XY}\Sigma_{YY}^{-1}(Y-\mu_Y), \Sigma_{XX} - \Sigma_{XY}\Sigma_{YY}^{-1}\Sigma_{YX}) \\<br>Y | X \sim \mathcal{N}(\mu_Y + \Sigma_{YX}\Sigma_{XX}^{-1}(X-\mu_X), \Sigma_{YY} - \Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY})$$</p>
<h3 id="4-Summation"><a href="#4-Summation" class="headerlink" title="4. Summation"></a>4. Summation</h3><p>The sum of independent Gaussian random variables is also Gaussian;</p>
<p>Let $y \sim \mathcal{N}(\mu, \Sigma)$ and $z \sim \mathcal{N}(\mu’,\Sigma’)$</p>
<p>Then, </p>
<p>$$y + z \sim \mathcal{N}(\mu + \mu’, \Sigma+\Sigma’)$$</p>
<p>We see some of important properties of Gaussian distribution. </p>
<p>Now, let’s change our focus to linear regression problem. </p>
<p>Typical linear regression equation is defined as, </p>
<p>$$y = ax + b$$</p>
<p>Or, </p>
<p>$$y = \beta_0 + \beta_1X_1 + … + \beta_pX_p + \epsilon$$</p>
<p>And, it can be expressed by</p>
<p>$$y = f(x) + \epsilon$$</p>
<p>where $f(x) = \beta_0 + \beta_1X_1 + … + \beta_pX_p$. </p>
<p>Here, an important assumption of linear regression is that the error term, $\epsilon$ is normally distributed with mean zero.  </p>
<p>If we consider repeated sampling from our population, for large sample sizes, the distribution of the ordinary least squared estimates of the regression coefficients follow a normal distribution by the Central Limit Theorem. </p>
<p>Refer to the previous post, <a href="https://davidkwon91.github.io/2019/09/09/CLT/" target="_blank" rel="noopener">Central Limit Theorem</a></p>
<p>Therefore, the $\epsilon$ follows Gaussian Distribution. </p>
<p>We might then think about what if our $f(x)$ follows Gaussian distribution. </p>
<p>Then, in the above equation, $y = f(x) + \epsilon$ will follow Gaussian distribution by the property that the sum of two independent Gaussian distribution is Gaussian distribution. </p>
<p>Here is the start of the Gaussian Process. </p>
<h2 id="large-Gaussian-large-Process"><a href="#large-Gaussian-large-Process" class="headerlink" title="$\large{Gaussian}$ $\large{ Process}$"></a>$\large{Gaussian}$ $\large{ Process}$</h2><p>A Gaussian Process is one of stochastic process that is a collection of random variables, ${f(x) : x \in \mathcal{X}}$, indexed by elements from some set $\mathcal{X}$, known as the index set, and GP is such stochastic process that any finite subcollection of random variables has a multivariate Gaussian distribution. </p>
<p>Here, we assume the $f(x)$ follows Gaussian distribution with some $\mu$ and $\Sigma$. </p>
<p>The $\mu$ and $\Sigma$ are called as “mean function” and “covariance function” with the notation, </p>
<p>$$f(.) \sim \mathcal{GP}(m(.), k(.,.))$$</p>
<p>where $k(.,.)$ is covariance function and a kernel function. </p>
<p>Hence, we have the equation, </p>
<p>$$y^{i} = f(x^{i}) + \epsilon^{i}$$</p>
<p>where the error term (“noise”), $\epsilon^{i}$ with independent $\mathcal{N}(0, \sigma^2)$ distribution. </p>
<p>We assume the prior distribution over the function $f(.)$ with mean zero Gaussian; </p>
<p>$$f(.) \sim \mathcal{GP}(0, k(.,.))$$</p>
<p>for some valid kernel function $k(.,.)$. </p>
<p>Now, we can convert the $\mathcal{GP}$ prior $f(.)$ into a $\mathcal{GP}$ posterior function after having some data to make predictions $f_p$. </p>
<p>Then, the two functions for the training ($f(.)$) and test ($f_p(.)$), will follows</p>
<p>$$\begin{bmatrix} f \\ f_p \end{bmatrix}|X,X_p \sim \mathcal{N}\big(0, \begin{bmatrix} K(X,X) &amp; K(X, X_p)  \\  K(X_p, X) &amp; K(X_p, X_p) \end{bmatrix}\big)$$</p>
<p>Now, with $\epsilon$ and $\epsilon_p$, </p>
<p>$$\begin{bmatrix} \epsilon \\ \epsilon_p\end{bmatrix} \sim \mathcal{N}(0,\begin{bmatrix} \sigma^2I &amp; 0 \\ 0^{T} &amp; \sigma^2I\end{bmatrix})$$</p>
<p>As mentioned above, the sum of two independent Gaussian distribution is also Gaussian, then eventually we have,</p>
<p>$$\begin{bmatrix} y \\ y_p \end{bmatrix} | X, X_p = \begin{bmatrix} f \\ f_p \end{bmatrix} + \begin{bmatrix} \epsilon \\ \epsilon_p\end{bmatrix} \sim \mathcal{N}(0, \begin{bmatrix} K(X,X) + \sigma^2I &amp; K(X,X_p) \\ K(X_p, X) &amp; K(X_p,X_p)+\sigma^2I \end{bmatrix})$$</p>
<p>Here, we generate the prior distributions, which are multivariate normal distributions, with standard multivariate normal distribution and the Cholesky factor as the following. </p>
<p>Refer to my previous post for Cholesky Decomposition, <a href="https://davidkwon91.github.io/tags/CholeskyDecomposition/" target="_blank" rel="noopener">Cholesky Decomposition post</a>. </p>
<p>1) Compute covariance function, $\Sigma$, and Cholesky Decomposition for $\Sigma = LL^{T}$</p>
<p>2) Generate standard multivariate normal distribution, $u \sim \mathcal{N}(0,1)$</p>
<p>3) Compute $x = \mu + Lu$, then $x$ will the prior distribution for $\mathcal{GP}$.  </p>
<p>Back to our prediction with GP, we have </p>
<p>$$y_p|y,X,X_p \sim \mathcal{N}(\mu_p, \Sigma_p)$$ </p>
<p>by the rules for conditioning Gaussians as we have done above. </p>
<p>Then, we have </p>
<p>$$\mu_p = K(X_p, X)K((X,X)+\sigma^2I)^{-1}y \\ \Sigma_p = K(X_p, X_p) + \sigma^2I - K(X_p,X)(K(X,X)+\sigma^2I)^{-1}K(X,X_p)$$</p>
<p>Here, the conditional mean and covariance function are calculated by the Schur Complement as shown above, and since the mean is zero vector, the mean function is defined as the above. </p>
<p>Conditional expectation and covariance with Schur Complement again, </p>
<p>$$\Sigma = \begin{bmatrix} A &amp; B \\ B^{T} &amp; C \end{bmatrix}$$ </p>
<p>$$Cov(X|Y) = A-BC^{-1}B^{T} \\ E(X|Y) = E(X) + BC^{-1}(Y-E(Y))$$</p>
<p>Here, the $E(X)$ and $E(Y) = 0$ for $\mathcal{GP}$, thus we have just $BC^{-1}Y$ term, which is $\mu_p = K(X_p, X)K((X,X)+\sigma^2I)^{-1}y$</p>
<h3 id="Kernel-Function-K"><a href="#Kernel-Function-K" class="headerlink" title="Kernel Function, $K(.,.)$?"></a>Kernel Function, $K(.,.)$?</h3><p>Commonly used kernel function for $\mathcal{GP}$ is <em>squared exponential kernel function</em>, which is often called Gaussian kernel. </p>
<p>It’s defined as, </p>
<p>$$K(X,X_p) = exp(-\frac{1}{2\nu} ||X-X_p||^2) = exp(-\frac{1}{2\nu} (X-X_p)^{T}(X-X_p))$$</p>
<p>where the $\nu$ is the free parameter for bandwidth of the function smoothness. </p>
<p>Here is the implementation of Gaussian Process in R. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">xcor &lt;- seq(-5,4.9,by=0.1) #index or coordinate for our data</span><br><span class="line"></span><br><span class="line">#Kernel Function (Squared exponential kernel function or Gaussian kernel function)</span><br><span class="line">GP.kernel &lt;- function(x1,x2,l)&#123;</span><br><span class="line">  mat &lt;- matrix(rep(0, length(x1)*length(x2)), nrow=length(x1))</span><br><span class="line">  for (i in 1:length(x1)) &#123;</span><br><span class="line">    for (j in 1:length(x2)) &#123;</span><br><span class="line">      mat[i,j] &lt;- (x1[i]-x2[j])^2</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  kern &lt;- exp(-(0.5*mat)/l) #l is free parameter that can be seen as bandwidth</span><br><span class="line">  return(kern)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Zero-mean and covariance function by the squared exponential kernel function with the index to create prior distribution</span><br><span class="line">mu &lt;- rep(0,length(xcor))</span><br><span class="line">cov &lt;- GP.kernel(xcor,xcor,1) #nu is 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#3 prior distributions</span><br><span class="line">dat &lt;- data.frame(x=xcor, first=mvrnorm(1,mu,cov), sec=mvrnorm(1,mu,cov), thi=mvrnorm(1,mu,cov))</span><br><span class="line">#mvrnorm function is same with the using of Cholesky factor of the covariance function multiplied by the standard multivariate normal distribution added mu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#graph for the 3 priors</span><br><span class="line">dat %&gt;% ggplot() + </span><br><span class="line">  geom_line(aes(x=xcor,y=first, colour=&quot;red&quot;)) + </span><br><span class="line">  geom_line(aes(x=xcor,y=sec, colour=&quot;blue&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=thi, colour=&quot;green&quot;))</span><br></pre></td></tr></table></figure>

<img src="/2019/09/15/GaussianProcess/GP-1.png" title="[GP]">


<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#Assume we have a data</span><br><span class="line"></span><br><span class="line">#5 elements</span><br><span class="line">f &lt;- data.frame(x=c(-4,-3,-1,0,2),</span><br><span class="line">                y=c(-2,0,1,2,-1))</span><br><span class="line">x &lt;- f$x</span><br><span class="line">k.xx &lt;- GP.kernel(x,x,1)</span><br><span class="line">k.xxs &lt;- GP.kernel(x,xcor,1)</span><br><span class="line">k.xsxs &lt;- GP.kernel(xcor,xcor,1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f.star.bar &lt;- t(k.xxs)%*%solve(k.xx)%*%f$y #mean function</span><br><span class="line">cov.f.star &lt;- k.xsxs - t(k.xxs)%*%solve(k.xx)%*%k.xxs #covariance function</span><br><span class="line"></span><br><span class="line">#posterior distribution</span><br><span class="line">dat1 &lt;- data.frame(x=xcor, </span><br><span class="line">                  first=mvrnorm(1,f.star.bar,cov.f.star), </span><br><span class="line">                  sec=mvrnorm(1,f.star.bar,cov.f.star), </span><br><span class="line">                  thi=mvrnorm(1,f.star.bar,cov.f.star))</span><br><span class="line"></span><br><span class="line">#graph for 3 posterior distribution when nu is 1</span><br><span class="line">dat1 %&gt;% ggplot() + </span><br><span class="line">  geom_line(aes(x=xcor,y=first, colour=&quot;red&quot;)) + </span><br><span class="line">  geom_line(aes(x=xcor,y=sec, colour=&quot;blue&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=thi, colour=&quot;green&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=f.star.bar),colour=&quot;black&quot;)+</span><br><span class="line">  geom_errorbar(data=f, aes(x=x, y=NULL, ymin=y-2*0.1, ymax=y+2*0.1),width=0.2)+</span><br><span class="line">  geom_point(data=f,aes(x=x,y=y)) +</span><br><span class="line">  ggtitle(&quot;Posterior from Prior x likelihood/evidence (given x,x*,y)&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/15/GaussianProcess/GP-2.png" title="[GP]">



<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mu &lt;- rep(0,length(xcor))</span><br><span class="line">cov &lt;- GP.kernel(xcor,xcor,0.1) #nu is 0.1</span><br><span class="line"></span><br><span class="line">dat &lt;- data.frame(x=xcor, first=mvrnorm(1,mu,cov), sec=mvrnorm(1,mu,cov), thi=mvrnorm(1,mu,cov))</span><br><span class="line"></span><br><span class="line">dat %&gt;% ggplot() + </span><br><span class="line">  geom_line(aes(x=xcor,y=first, colour=&quot;red&quot;)) + </span><br><span class="line">  geom_line(aes(x=xcor,y=sec, colour=&quot;blue&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=thi, colour=&quot;green&quot;)) +</span><br><span class="line">  ggtitle(&quot;When sigma is 0.1&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mu &lt;- rep(0,length(xcor))</span><br><span class="line">cov &lt;- GP.kernel(xcor,xcor,10) #nu is 10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dat &lt;- data.frame(x=xcor, first=mvrnorm(1,mu,cov), sec=mvrnorm(1,mu,cov), thi=mvrnorm(1,mu,cov))</span><br><span class="line"></span><br><span class="line">dat %&gt;% ggplot() + </span><br><span class="line">  geom_line(aes(x=xcor,y=first, colour=&quot;red&quot;)) + </span><br><span class="line">  geom_line(aes(x=xcor,y=sec, colour=&quot;blue&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=thi, colour=&quot;green&quot;)) +</span><br><span class="line">  ggtitle(&quot;When sigma is 10&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/15/GaussianProcess/GP-3.png" title="[GP]">
<img src="/2019/09/15/GaussianProcess/GP-5.png" title="[GP]">









<p>Reference: </p>
<p><a href="http://krasserm.github.io/2018/03/19/gaussian-processes/" target="_blank" rel="noopener">Gaussian Process</a><br><a href="http://i-systems.github.io/HSE545/machine%20learning%20all/14%20Gaussian%20Process%20Regression/reference_files/cs229-gaussian_processes.pdf" target="_blank" rel="noopener">Gaussian Process</a><br><a href="https://distill.pub/2019/visual-exploration-gaussian-processes/#DimensionSwap" target="_blank" rel="noopener">A Visual Exploration of Gaussian Process</a><br><a href="http://www0.cs.ucl.ac.uk/staff/J.Shawe-Taylor/courses/ATML-1.pdf" target="_blank" rel="noopener">Gaussian Process: A Basic Properties and GP regression</a><br><a href="https://www.r-bloggers.com/gaussian-process-regression-with-r/" target="_blank" rel="noopener">Gaussian Process regression with R</a></p>
<p>and my previous posts,</p>
<p><a href="https://davidkwon91.github.io/tags/CholeskyDecomposition/" target="_blank" rel="noopener">Cholesky Decomposition</a><br><a href="https://davidkwon91.github.io/tags/CentralLimitTheorem/" target="_blank" rel="noopener">Central Limit Theorem</a><br><a href="https://davidkwon91.github.io/tags/CentralLimitTheorem/" target="_blank" rel="noopener">Schur Complement</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/08/29/logit/" class="image is-7by1">
            <img class="thumbnail" src="/images/logisticRegression.png" alt="Logit - Logistic Regression">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-08-29T05:48:09.000Z">2019-08-29</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Logistic-Regression-I/">Logistic Regression I</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    7 minutes read (About 1096 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/08/29/logit/">Logit - Logistic Regression</a>
            
        </h1>
        <div class="content">
            <h2 id="Logistic-Regression-from-Logit"><a href="#Logistic-Regression-from-Logit" class="headerlink" title="Logistic Regression from Logit"></a>Logistic Regression from Logit</h2><h3 id="1-What-is-Logit"><a href="#1-What-is-Logit" class="headerlink" title="1. What is Logit?"></a>1. What is Logit?</h3><p>To discuss logit, we need to know what the odds is. </p>
<p>Simply say, Odds is $\frac{p}{1-p}$, where $p$ is the probability of a binary outcome. </p>
<p>Since the odds has different properties than probability, it allows us to make new functions or new graphs. </p>
<p>With the properties of odds, logarithm of odds is popularly used in machine learning industry, which is called logit. </p>
<p>Logit (also log odds) = $log(\frac{p}{1-p})$. </p>
<p>Let logit be defined as $\theta$, then </p>
<p>$$\theta = log(\frac{p}{1-p})$$</p>
<p>We can express this in terms of $p$ as the following,</p>
<p>$$e^{\theta} = \frac{p}{1-p} \\ (1-p)e^{\theta} = p \\ e^{\theta} = p + p e^{\theta} \\ e^{\theta} = p(1+e^{\theta}) \\ \therefore p = \frac{e^{\theta}}{(1+e^{\theta})} = \frac{1}{1+e^{-\theta}}$$</p>
<p>This is the logistic function, which is the special case of sigmoid function that is widely used for classification, such as logistic regression, tree-based algorithm for classification, or deep learning.<br>The logistic function is the CDF of the logistic distribution. </p>
<h3 id="2-Logistic-Distribution"><a href="#2-Logistic-Distribution" class="headerlink" title="2. Logistic Distribution"></a>2. Logistic Distribution</h3><p>A property of the logistic distribution is that the logistic distribution is very similar with the normal distribution except for the kurtosis. </p>
<p>The PDF of the logistic distribution is defined as, </p>
<p>$$f(x; \mu, s) =  \frac{e^{-\frac{(x-\mu)}{s}}}{s(1+e^{-\frac{x-\mu}{s}})^2}$$</p>
<p>The CDF of the logistic distribution is defined as,</p>
<p>$$F(x; \mu, s) = \frac{1}{1+e^{-\frac{x-\mu}{s}}}$$</p>
<p>Some simulations are performed in <a href="https://core.ac.uk/download/pdf/41787448.pdf" target="_blank" rel="noopener">this paper</a>, and the simulation was to measure the absolute deviation between the cumulative standard normal distribution and the cumulative logistic distribution with mean zero and variance one. </p>
<p>By <a href="https://core.ac.uk/download/pdf/41787448.pdf" target="_blank" rel="noopener">the paper</a>, “the cumulative logistic distribution with mean zero and variance one is known as $(1+e^{-\nu x})^{-1}$ “<br>And, the paper also states that the kurtosis of the logistic distribution is effected by the parameter coefficient, $\nu$.<br>With the $\nu = 1.702$, the deviation is 0.0095. </p>
<p>As a summary, the logistic distribution approximately follows the normal distribution with the variance $\sigma^2$, and we could use the property of the normal distribution as the following; </p>
<p>By <a href="https://en.wikipedia.org/wiki/Standard_deviation#targetText=If%20a%20data%20distribution%20is,deviations%20(%CE%BC%20%C2%B1%203%CF%83)." target="_blank" rel="noopener">wikipedia</a>, </p>
<p>“If a data distribution is approximately normal, then about 68 percent of the data values are within one standard deviation of the mean (mathematically, $\mu \pm \sigma$, where $\mu$ is the arithmetic mean), about 95 percent are within two standard deviations ($\mu \pm 2\sigma$), and about 99.7 percent lie within three standard deviations ($\mu \pm 3\sigma$).”</p>
<h3 id="3-Logistic-Regression"><a href="#3-Logistic-Regression" class="headerlink" title="3. Logistic Regression"></a>3. Logistic Regression</h3><p>In a binary logistic regression, the outcome is binary, which can be said that it follows binomial distribution.</p>
<p>Let $y_i$ = number of successes in $m_i$ trials of a binomial process where $i = 1,…,n$, and we have a single predictor, $x_i$.</p>
<p>Then, </p>
<p>$$y_i | x_i \sim Bin(m_i, \theta(x_i))$$</p>
<p>By the definition of binomial distribution,</p>
<p>$$P(Y_i = y_i | x_i) = \binom{m_i}{y_i}\theta(x_i)^{y_i}(1-\theta(x_i))^{m_i-y_i}$$</p>
<p>As I discussed in this <a href="https://davidkwon91.github.io/categories/ML/Likelihood/" target="_blank" rel="noopener">post, Likelihood</a>, we can find the optimal distribution for the given data. </p>
<p>$$\mathcal{L}(\theta | x) = \mathcal{L}(distribution | data)$$</p>
<p>And with the optimal distribution given by the maximum likelihood, we can predict the probability.</p>
<p>$$P(data | distribution)$$</p>
<p>Then, the likelihood function for Logistic Regression will be </p>
<p>$$\mathcal{L} = \prod_{i=1}^{n} P(Y_i = y_i | x_i) \\ = \prod_{i=1}^{n}\binom{m_i}{y_i}\theta(x_i)^{y_i}(1-\theta(x_i))^{m_i-y_i}$$</p>
<p>We take log for the likelihood function to differentiate easier because maximizing likelihood is the same with maximizing log-likelihood. </p>
<p>The log-likelihood function will be</p>
<p>$$log(\mathcal{L}) =  \prod_{i=1}^{n}[log(\binom{m_i}{y_i}) + log(\theta(x_i)^{y_i}) + log((1-\theta(x_i))^{m_i - y_i})] \\ = \prod_{i=1}^{n}[y_{i} log(\theta(x_i)) + (m_i - y_i)log(1-\theta(x_i)) + log(\binom{m_i}{y_i})] \\ = \prod_{i=1}^{n}[y_{i} log(\frac{\theta(x_i)}{1-\theta(x_i)}) + m_{i} log(1-\theta(x_i)) + log(\binom{m_i}{y_i})] \\ = \prod_{i=1}{n}[y_{i}(\beta_{0} + \beta_{1}x_{i}) - m_{i}log(1+exp(\beta_{0}+\beta_{1}x_{i})) + log(\binom{m_i}{y_i})]$$</p>
<p>Here, the $\theta(x_i)$, which is the probability, is defined as</p>
<p>$$\theta(x_i) = \frac{1}{1+e^{-(\beta_0 + \beta_1 x_i)}}$$</p>
<p>And, the linear combination of $\beta$ and $x$ will be the log odds as the following,</p>
<p>$$log(\frac{\theta(x_i)}{1-\theta(x_i)}) = \beta_0 + \beta_1 x_i$$</p>
<p>Just taking the derivative on the log-likelihood function and setting this as zero is hard to solve the problem, so the parameter $\beta_0$ and $\beta_1$ can be estimated by some other optimization method, such as Newton’s method. </p>
<h3 id="4-Wald-Test-in-Logistic-Regression"><a href="#4-Wald-Test-in-Logistic-Regression" class="headerlink" title="4. Wald Test in Logistic Regression"></a>4. Wald Test in Logistic Regression</h3><p>Wald Test uses the property of the logistic distribution. </p>
<p>As mentioned above, the logistic distribution is almost approximately standard normal distribution. </p>
<p>Hence, the Wald test statistic is used to test </p>
<p>$$H_0 : \beta_1 = 0$$ </p>
<p>in Logistic regression. </p>
<p>The Wald test statistic is defined as</p>
<p>$$Z = \frac{\hat{\beta_1} - \beta_1}{estimated \qquad se(\hat{\beta_1})}$$</p>
<p>In the test, $\beta_1 = 0$ as the null hypothesis, and this follows standard normal distribution. </p>
<p>Hence, </p>
<p>$$Z = \frac{\hat{\beta_1}}{\hat{se}(\hat{\beta_1})} \sim \mathcal{N}(0,1)$$</p>
<p>As mentioned above, the logistic distribution is approximately following the normal distribution with variance, $\sigma^2$.<br>Hence, dividing by the standard error for the estimated paramter gives you standard normal distribution, $\mathcal{N}(0,1)$.</p>
<p>We can determine the coefficient significant with the Wald Test for the Logistic Regression. </p>
<h3 id="4-Interpretation-for-coefficients-of-Logistic-Regression"><a href="#4-Interpretation-for-coefficients-of-Logistic-Regression" class="headerlink" title="4. Interpretation for coefficients of Logistic Regression."></a>4. Interpretation for coefficients of Logistic Regression.</h3><p>Unlike Linear Regression, the linear combination of coefficients of Logistic regression is logit. </p>
<p>Hence, we can interpret the coefficient in different way. </p>
<p>For example, with the single predictor as used above, the logit is</p>
<p>$$log(\frac{p}{1-p}) = \beta_0 + \beta_1 x_i$$</p>
<p>If we take exponential both sides, </p>
<p>$$\frac{p}{1-p} = e^{(\beta_0 + \beta_1 x_i)}$$</p>
<p>Then, we can say as following,</p>
<p>“For one-unit in $x_i$ increase, it is expected to see ..% increase in odds of success, $p$”.</p>
<p>Reference:<br><em>A Modern Approach to Regression With R by Simon J. Sheater, Springer</em><br><a href="https://core.ac.uk/download/pdf/41787448.pdf" target="_blank" rel="noopener">A logistic approximation to the cumulative normal distribution</a><br><a href="https://en.wikipedia.org/wiki/Standard_deviation#targetText=If%20a%20data%20distribution%20is,deviations%20(%CE%BC%20%C2%B1%203%CF%83)." target="_blank" rel="noopener">Standard Deviation</a><br><a href="https://en.wikipedia.org/wiki/Logistic_distribution" target="_blank" rel="noopener">Logistic Distribution</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/08/29/Likelihood/" class="image is-7by1">
            <img class="thumbnail" src="/images/likelihood.png" alt="Likelihood">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-08-29T03:50:08.000Z">2019-08-29</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Likelihood/">Likelihood</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    7 minutes read (About 1026 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/08/29/Likelihood/">Likelihood</a>
            
        </h1>
        <div class="content">
            <h2 id="Likelihood-vs-Probability"><a href="#Likelihood-vs-Probability" class="headerlink" title="Likelihood vs Probability"></a>Likelihood vs Probability</h2><p>Likelihood Definition (Wikipedia) : In statistics, the likelihood function expresses how probable a given set of observations is for different values of statistical parameters. It is equal to the joint probability distribution of the random sample evaluated at the given observations, and it is, thus, solely a functoin of paramters that index the family of those probability distributions. </p>
<h2 id="1-What-is-Likelihood"><a href="#1-What-is-Likelihood" class="headerlink" title="1. What is Likelihood?"></a>1. What is Likelihood?</h2><p>Likelihoods are the y-axis values for fixed data points with distribution that can be moved, as the following; </p>
<p>$$\mathcal{L}(\theta | x) = \mathcal{L}(distribution | data)$$</p>
<p>Whereas, Probabilities are the areas under a fixed distribution.</p>
<p>$$P(data | distribution)$$</p>
<p>If we find the maximum y-axis value for fixed data points by moving the distribution, we find the optimal distribution for the fixed data.<br>Therefore, we can use the likelihood function to fit a optimal distribution on the fixed data by finding maximum likelihood estimator. </p>
<h2 id="2-Maximum-Likelihood"><a href="#2-Maximum-Likelihood" class="headerlink" title="2. Maximum Likelihood?"></a>2. Maximum Likelihood?</h2><p>To find the maximum likelihood estimator, we might find a function for likelihoods given fixed data points, and take a derivative for the function, and set the derivatives as 0. </p>
<h3 id="Example-1-Exponential-distribution"><a href="#Example-1-Exponential-distribution" class="headerlink" title="Example 1: Exponential distribution"></a>Example 1: Exponential distribution</h3><p>The Probability Density Function of Exponential Distribution is defined as the following;</p>
<p>$$f(x; \lambda) = \lambda e ^ {- \lambda x} \qquad if  \qquad x \geq 0$$</p>
<p>Otherwise, </p>
<p>$$f(x; \lambda) = 0$$</p>
<p>for $\lambda &gt;0$, which is rate parameter of the distribution. </p>
<p>Hence, the exponential distribution will be shaped by the $\lambda$. </p>
<p>Therefore, we want to find the maximum likelihood of $\lambda$ for given data. </p>
<p>$$\mathcal{L}(\lambda | x_1, x_2, … , x_n) =  \mathcal{L}(\lambda | x_1)\mathcal{L}(\lambda | x_2)…\mathcal{L}(\lambda | x_n)    $$</p>
<p>$$= \lambda e ^ {- \lambda x_1}\lambda e ^ {- \lambda x_2}…\lambda e ^ {- \lambda x_n} \\ = \lambda^{n}(e ^ {-\lambda  (x_1 + x_2 + … + x_n)})$$</p>
<p>Now, take a derivative for the likelihood function to find maximum likelihood estimator of $\lambda$.</p>
<p>$$ \frac{d}{d\lambda} \mathcal{L}(\lambda | x_1, x_2, … , x_n) = \frac{d}{d\lambda} \lambda^{n} (e ^ {-\lambda * (x_1 + x_2 + … + x_n)})$$</p>
<p>Before derivitaves, there is more easier way to differentiate this function, which is to take a lograithm on the function.<br>Take a logarithm on the likelihood function, then this becomes the log likelihood function. </p>
<p>$$log (\mathcal{L}(\lambda | x_1, x_2, … , x_n)) =  log(\mathcal{L}(\lambda | x_1)\mathcal{L}(\lambda | x_2)…\mathcal{L}(\lambda | x_n))<br>$$</p>
<p>Which makes easier differentiate the function. </p>
<p>$$ \frac{d}{d\lambda} log(\mathcal{L}(\lambda | x_1, x_2, … , x_n)) = \frac{d}{d\lambda} log(\lambda^{n}(e ^ {-\lambda * (x_1 + x_2 + … + x_n)}))$$</p>
<p>$$= \frac{d}{d\lambda} (log(\lambda ^ {n}) + log(\lambda^{n}(e ^ {-\lambda * (x_1 + x_2 + … + x_n)}))) \\ = \frac{d}{d\lambda} (n log(\lambda) - \lambda (x_1 + x_2 + … + x_n)) \\ = n \frac{1}{\lambda} - (x_1 + x_2 + … + x_n)$$</p>
<p>Set this as zero, </p>
<p>$$n \frac{1}{\lambda} - (x_1 + x_2 + … + x_n) = 0$$</p>
<p>Which gives you, </p>
<p>$$ \frac{(x_1 + x_2 + … + x_n)}{n} = \mu = \frac{1}{\lambda}$$</p>
<p>Therefore, the maximum likelihood estimator of $\lambda$ is</p>
<p>$$\lambda = \frac{1}{\mu}$$.</p>
<p>And, this connects to the concept of the maximum entropy probability distribution.</p>
<p>“The exponential distribution is the maximum entropy distribution among all continuous distribution supported in $[0, \infty]$ that have a specified mean of $\frac{1}{\lambda}$”</p>
<h3 id="Example-2-Normal-Distribution"><a href="#Example-2-Normal-Distribution" class="headerlink" title="Example 2: Normal Distribution"></a>Example 2: Normal Distribution</h3><p>In normal distribution (often called Gaussian distribution), the mean $\mu$ and the variance $\sigma^2$ will be the parameter that the determines how the normal distribution looks like. With $\mu$ and $\sigma^2$, we will find the maximum likelihood estimator for the normal distribution given fixed data. </p>
<p>The PDF of normal distribution is defined as,</p>
<p>$$f(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$</p>
<p>The Likelihood of $\mu$ and $\sigma$ for normal distribution will be, </p>
<p>$$\mathcal{L}(\mu, \sigma|x_1, x_2, … , x_n) = \mathcal{L}(\mu, \sigma^2|x_1)\mathcal{L}(\mu, \sigma^2|x_2)…\mathcal{L}(\mu, \sigma^2|x_n)$$</p>
<p>$$= \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_1-\mu)^2}{2\sigma^2}}\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_2-\mu)^2}{2\sigma^2}}…\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_n-\mu)^2}{2\sigma^2}}$$</p>
<p>Before take a derivatives, we take a log on the function as done in exponential. </p>
<p>$$log(\mathcal{L}(\mu, \sigma|x_1, x_2, … , x_n))= log(\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_1-\mu)^2}{2\sigma^2}}\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_2-\mu)^2}{2\sigma^2}}…\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x_n-\mu)^2}{2\sigma^2}})$$</p>
<p>By simplifying the log likelihood function with the properties of logarithm,</p>
<p>$$= \sum_{i=1}^{n}(log(\frac{1}{\sqrt{2\pi\sigma^2}}) -\frac{(x_i-\mu)^2}{2\sigma^2}) \\ = -\frac{n}{2}log(2\pi) - nlog(\sigma) - \sum_{i=1}^{n}\frac{(x_i-\mu)^2}{2\sigma^2}$$</p>
<p>Now, take the derivative with respect to the $\mu$,</p>
<p>$$\frac{\partial}{\partial \mu} log(\mathcal{L}(\mu, \sigma|x_1, x_2, … , x_n)) \\ = 0 - 0 + \sum_{i=1}^{n} \frac{(x_i-\mu)}{\sigma^2} \\ = \sum_{i=1}^{n} \frac{x_i}{\sigma^2} - n\mu$$</p>
<p>Set this as 0, </p>
<p>$$\sum_{i=1}^{n} \frac{x_i}{\sigma^2} - n\mu = 0$$</p>
<p>Since $\sigma^2$ is constant here, </p>
<p>$$ \sum_{i=1}^{n} x_i = n\mu$$</p>
<p>Hence,</p>
<p>$$\mu = \sum_{i=1}^{n} x_i / n$$<br>which is the mean of the measurements. </p>
<p>Taking derivative with respect to the $\sigma$, </p>
<p>$$\frac{\partial}{\partial \sigma} log(\mathcal{L}(\mu, \sigma|x_1, x_2, … , x_n)) = \frac{\partial}{\partial \sigma} (-\frac{n}{2}log(2\pi) - nlog(\sigma) - \sum_{i=1}^{n}\frac{(x_i-\mu)^2}{2\sigma^2}) \\ = 0 - \frac{n}{\sigma} + \sum_{i=1}^{n} \frac{(x_i - \mu)^2}{\sigma^3}$$</p>
<p>Set this as 0,</p>
<p>$$0 - \frac{n}{\sigma} + \sum_{i=1}^{n} \frac{(x_i - \mu)^2}{\sigma^3} = 0 \\ n = \sum_{i=1}^{n} \frac{(x_i - \mu)^2}{\sigma^2} \\ \sigma^2 = \sum_{i=1}^{n} \frac{(x_i - \mu)^2}{n} \\ \sigma = \sum_{i=1}^{n} \sqrt{\frac{(x_i - \mu)^2}{n}}$$<br>which is the standard deviation of the measurements. </p>
<p>In conclusion, </p>
<p>the mean of the data is the maximum likelihood estimate for where the center of the distribution should be, </p>
<p>the standard deviation of the data is the maximum likelihood estimate for how wide the curve of the distribution should be. </p>
<p>This also connects to the maximum entropy probability distribution as the following; </p>
<p>The normal distribution has maximum entropy among all real-valued distribution supported on $(-\infty, \infty)$ with a specified variance $\sigma^2$.</p>
<p>Reference:<br><a href="https://en.wikipedia.org/wiki/Likelihood_function" target="_blank" rel="noopener">Likelihood(wiki)</a><br><a href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution" target="_blank" rel="noopener">Maximum Entropy Probability Distribution(wiki)</a><br><a href="https://www.youtube.com/watch?v=pYxNSUDSFH4" target="_blank" rel="noopener">Related to Likelihood for distributions (StatQuest with Josh Starmer)</a></p>

        </div>
        
        
        
    </div>
</div>








</div>
                




<div class="column is-4-tablet is-4-desktop is-4-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                        <img class="image is-128x128 has-mb-6" src="/images/David.png" alt="Yongbock (David) Kwon">
                    
                    
                    <p class="is-size-4 is-block">
                        Yongbock (David) Kwon
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        The trick in life isn&#39;t getting what you want, it&#39;s wanting it after you get it
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>Seoul, Korea</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Posts
                    </p>
                    <p class="title has-text-weight-normal">
                        19
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Categories
                    </p>
                    <p class="title has-text-weight-normal">
                        21
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Tags
                    </p>
                    <p class="title has-text-weight-normal">
                        34
                    </p>
                </div>
            </div>
        </nav>
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/DavidKwon91" target="_blank">
                Follow</a>
        </div>
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Github" href="https://github.com/DavidKwon91">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Linkedin" href="https://www.linkedin.com/in/yongbock-david-kwon-7a195994/">
                
                <i class="fab fa-linkedin"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        
    
        


    
        
<div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Categories
            </h3>
            <ul class="menu-list">
            <li>
        <a class="level is-marginless" href="/categories/Language/">
            <span class="level-start">
                <span class="level-item">Language</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/Language/Python-in-R/">
            <span class="level-start">
                <span class="level-item">Python in R</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/ML/">
            <span class="level-start">
                <span class="level-item">ML</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">9</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/ML/Classification/">
            <span class="level-start">
                <span class="level-item">Classification</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/DNN-Optimizers/">
            <span class="level-start">
                <span class="level-item">DNN-Optimizers</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Decision-Tree/">
            <span class="level-start">
                <span class="level-item">Decision Tree</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Gaussian-Process/">
            <span class="level-start">
                <span class="level-item">Gaussian Process</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Likelihood/">
            <span class="level-start">
                <span class="level-item">Likelihood</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Logistic-Regression-I/">
            <span class="level-start">
                <span class="level-item">Logistic Regression I</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Logistic-Regression-II/">
            <span class="level-start">
                <span class="level-item">Logistic Regression II</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Monty-Hall/">
            <span class="level-start">
                <span class="level-item">Monty Hall</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Principal-Components-Analysis/">
            <span class="level-start">
                <span class="level-item">Principal Components Analysis</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/Math/">
            <span class="level-start">
                <span class="level-item">Math</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">4</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/Math/CLT/">
            <span class="level-start">
                <span class="level-item">CLT</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Math/Cholesky-Decomposition/">
            <span class="level-start">
                <span class="level-item">Cholesky Decomposition</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Math/Histogram/">
            <span class="level-start">
                <span class="level-item">Histogram</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Math/Schur-Complement/">
            <span class="level-start">
                <span class="level-item">Schur Complement</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/Today/">
            <span class="level-start">
                <span class="level-item">Today</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">4</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/Today/August/">
            <span class="level-start">
                <span class="level-item">August</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Today/Feb-2020/">
            <span class="level-start">
                <span class="level-item">Feb 2020</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Today/September/">
            <span class="level-start">
                <span class="level-item">September</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">2</span>
            </span>
        </a></li></ul></li>
            </ul>
        </div>
    </div>
</div>
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Tag Cloud
        </h3>
        <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Bayes-Theorem/" style="font-size: 10px;">Bayes Theorem</a> <a href="/tags/CentralLimitTheorem/" style="font-size: 10px;">CentralLimitTheorem</a> <a href="/tags/CharlesBukowski/" style="font-size: 10px;">CharlesBukowski</a> <a href="/tags/CholeskyDecomposition/" style="font-size: 10px;">CholeskyDecomposition</a> <a href="/tags/Classification/" style="font-size: 10px;">Classification</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/Decision-Tree/" style="font-size: 10px;">Decision Tree</a> <a href="/tags/Diary/" style="font-size: 10px;">Diary</a> <a href="/tags/Entropy/" style="font-size: 10px;">Entropy</a> <a href="/tags/GaussianProcess/" style="font-size: 10px;">GaussianProcess</a> <a href="/tags/Gini-Index/" style="font-size: 10px;">Gini Index</a> <a href="/tags/Histogram/" style="font-size: 10px;">Histogram</a> <a href="/tags/Likelihood/" style="font-size: 13.33px;">Likelihood</a> <a href="/tags/LogLikelihood/" style="font-size: 10px;">LogLikelihood</a> <a href="/tags/Logistic-Regression-II/" style="font-size: 10px;">Logistic Regression II</a> <a href="/tags/Logit/" style="font-size: 10px;">Logit</a> <a href="/tags/ML/" style="font-size: 20px;">ML</a> <a href="/tags/Matrix/" style="font-size: 16.67px;">Matrix</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Month-Hall-Problem/" style="font-size: 10px;">Month Hall Problem</a> <a href="/tags/Newton-s-Method/" style="font-size: 10px;">Newton's Method</a> <a href="/tags/Odds/" style="font-size: 10px;">Odds</a> <a href="/tags/Optimizers/" style="font-size: 10px;">Optimizers</a> <a href="/tags/PCA/" style="font-size: 10px;">PCA</a> <a href="/tags/Poem/" style="font-size: 10px;">Poem</a> <a href="/tags/Probability/" style="font-size: 10px;">Probability</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD</a> <a href="/tags/SchurComplement/" style="font-size: 10px;">SchurComplement</a> <a href="/tags/Today/" style="font-size: 16.67px;">Today</a> <a href="/tags/Unsupervised-Learning/" style="font-size: 10px;">Unsupervised Learning</a>
    </div>
</div>

    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Recent
        </h3>
        
        <article class="media">
            
            <a href="/2020/02/01/new/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/thumbnail.svg" alt="200202">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-02-01T05:45:12.000Z">2020-02-01</time></div>
                    <a href="/2020/02/01/new/" class="title has-link-black-ter is-size-6 has-text-weight-normal">200202</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/Today/">Today</a> / <a class="has-link-grey -link" href="/categories/Today/Feb-2020/">Feb 2020</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2020/01/02/logistic/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/sigmoid.png" alt="Logistic Regression - Newton&#39;s method">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2020-01-02T03:20:29.000Z">2020-01-02</time></div>
                    <a href="/2020/01/02/logistic/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Logistic Regression - Newton&#39;s method</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/ML/">ML</a> / <a class="has-link-grey -link" href="/categories/ML/Logistic-Regression-II/">Logistic Regression II</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/10/24/PCA/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/pca.png" alt="PCA">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-10-24T06:08:47.000Z">2019-10-24</time></div>
                    <a href="/2019/10/24/PCA/" class="title has-link-black-ter is-size-6 has-text-weight-normal">PCA</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/ML/">ML</a> / <a class="has-link-grey -link" href="/categories/ML/Principal-Components-Analysis/">Principal Components Analysis</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/10/16/AUROC/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/auroc.png" alt="Classfication">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-10-16T04:04:57.000Z">2019-10-16</time></div>
                    <a href="/2019/10/16/AUROC/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Classfication</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/ML/">ML</a> / <a class="has-link-grey -link" href="/categories/ML/Classification/">Classification</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/10/07/Reticulate/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/RorPython.jpeg" alt="Reticulate">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-10-07T03:00:23.000Z">2019-10-07</time></div>
                    <a href="/2019/10/07/Reticulate/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Reticulate</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/Language/">Language</a> / <a class="has-link-grey -link" href="/categories/Language/Python-in-R/">Python in R</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            Archives
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2020/02/">
                <span class="level-start">
                    <span class="level-item">February 2020</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2020/01/">
                <span class="level-start">
                    <span class="level-item">January 2020</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2019/10/">
                <span class="level-start">
                    <span class="level-item">October 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2019/09/">
                <span class="level-start">
                    <span class="level-item">September 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">9</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2019/08/">
                <span class="level-start">
                    <span class="level-item">August 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Tags
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/AUC/">
                        <span class="tag">AUC</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Bayes-Theorem/">
                        <span class="tag">Bayes Theorem</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/CentralLimitTheorem/">
                        <span class="tag">CentralLimitTheorem</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/CharlesBukowski/">
                        <span class="tag">CharlesBukowski</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/CholeskyDecomposition/">
                        <span class="tag">CholeskyDecomposition</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Classification/">
                        <span class="tag">Classification</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/DNN/">
                        <span class="tag">DNN</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Decision-Tree/">
                        <span class="tag">Decision Tree</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Diary/">
                        <span class="tag">Diary</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Entropy/">
                        <span class="tag">Entropy</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/GaussianProcess/">
                        <span class="tag">GaussianProcess</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Gini-Index/">
                        <span class="tag">Gini Index</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Histogram/">
                        <span class="tag">Histogram</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Likelihood/">
                        <span class="tag">Likelihood</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/LogLikelihood/">
                        <span class="tag">LogLikelihood</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Logistic-Regression-II/">
                        <span class="tag">Logistic Regression II</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Logit/">
                        <span class="tag">Logit</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/ML/">
                        <span class="tag">ML</span>
                        <span class="tag is-grey">14</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Matrix/">
                        <span class="tag">Matrix</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Metric/">
                        <span class="tag">Metric</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Month-Hall-Problem/">
                        <span class="tag">Month Hall Problem</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Newton-s-Method/">
                        <span class="tag">Newton&#39;s Method</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Odds/">
                        <span class="tag">Odds</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Optimizers/">
                        <span class="tag">Optimizers</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/PCA/">
                        <span class="tag">PCA</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Poem/">
                        <span class="tag">Poem</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Probability/">
                        <span class="tag">Probability</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Python/">
                        <span class="tag">Python</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/R/">
                        <span class="tag">R</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/ROC/">
                        <span class="tag">ROC</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/SVD/">
                        <span class="tag">SVD</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/SchurComplement/">
                        <span class="tag">SchurComplement</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Today/">
                        <span class="tag">Today</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Unsupervised-Learning/">
                        <span class="tag">Unsupervised Learning</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.svg" alt="David Kwon" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2020 David Kwon&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Creative Commons" href="https://creativecommons.org/">
                        
                        <i class="fab fa-creative-commons"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/">
                        
                        <i class="fab fa-creative-commons-by"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("en");</script>

<script>
var IcarusThemeSettings = {
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>



    
    
    
    <script src="/js/animation.js"></script>
    

    
    
    
    <script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
    <script src="/js/gallery.js" defer></script>
    

    
    

<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


    
    
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>

    
    

<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>


    
    

    
    
    
    

    
    
    
    
    


<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>