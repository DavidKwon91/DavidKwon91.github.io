{"pages":[{"title":"About","text":"This is David Kwon! Email: husiew140@gmail.com Phone: +82 10-5413-6780","link":"/about/index.html"}],"posts":[{"title":"190827","text":"August 27th, 2019 Today’s work Finalize creating blog Density plot -&gt; Kernel Density Plot Join Kaggle Competition - IEEE Fraud Detection, and skim the Datasets Resume Convex Optimization Course - review convex optimization problem, start Duality 오늘 할일 블로그 개설 Density Plot 에 대해 공부 -&gt; Kernel Density Plot(커널 밀도 함수) Join Kaggle Competition - IEEE Fraud Detection, 데이터셋 훑어보기 Convex 수업, 이전 내용 복습, Duality 강의 시작","link":"/2019/08/27/190827/"},{"title":"190828","text":"August 28th, 2019 Today’s work Convex Optimization Course - Duality; weak and strong duality IEEE Kaggle - Data Organization; convert data by Data Description - link Study LightGBM with the video presented by Mateusz Susik from McKinsey - link XGBoost by Tianqi Chen- link 오늘 할 일 Convex Optimization 수업 - Duality; weak and strong duality IEEE 카글 대회 - 데이터 정리; 데이터 Description에 따라 변수들 변환 - link LightGBM 원리 공부 - 프레젠테이션 Mateusz Susik from McKinsey - link XGBoost -&gt; LightGBM Update: Duality - Form Lagrange $\\mathcal{L}(x,\\lambda,\\nu)$ = $f_0 (x) + \\sum_{i=1}^{m} \\lambda_i f_i(x) + \\sum_{i=1}^{p} \\nu_i h_i (x)$ Set gradient for $x$ equal to zero to minimize $\\mathcal{L}$ $\\nabla_x \\mathcal{L}$ = 0 Plug it in $\\mathcal{L}$ to get the Lagrangian dual function; $g(\\lambda, \\nu)$ = $\\inf_{x\\in D} \\mathcal{L}(x, \\lambda, \\nu)$ which is a concave function, can be $-\\infty$ for some $\\lambda, \\nu$ Lagrangian dual function is a concave function, since the Lagrangian form is affine function, and infimum of any family of affine is concave. LightGBM - XGBoost (either histogram implementation available) One of the method used in LightGBM is ‘Graident-based one-side sampling’ that is the biggest benefit of LightGBM. This method is to concentrate on data points with large gradients and ignore data points with small graidents (close to local minima). LightGBM - XGBoost(둘 다 histogram implementation 가능) 가장 큰 장점은 Gradient-based one-side sampling 이라는 방법으로, gradient가 큰 데이터 포인트들을 집중하며, small gradients (Gradients가 작다는 것은 local minima에 가깝다는것이고, 그 말은 즉 Residual 혹은 loss 가 적다는것) 들은 무시하기에 training 속도가 굉장히 빠르다는 것.","link":"/2019/08/28/190828/"},{"title":"Starting Blog","text":"Welcome to David’s blog!I am excited to start my first blog. This is Yongbock (David) Kwon who likes studying statistics, especially machine learning. I love to listen to musics! I started this blog to record what I study, what I listen to, and what I think in my daily life, because I have a bad memory. And, the record is the one that is able to get over against the river of time as a human. Hence, this blog will be my study notes, my diary, and the representation of myself. Let’s begin! 제 블로그에 오신걸 환영합니다!첫 블로그인지라 많이 긴장되네요! 현재 머신러닝을 공부중에 있으며, 음악듣는걸 좋아합니다. 기억력이 좋지 않은 편이라, 무엇을 공부했고, 무슨 음악을 들으며, 일상중에 무슨 생각을 하는지에 대해 기록해보고자 블로그를 만들었습니다. 즉, 제 공부 노트이자, 일기장이자, 권용복이 누구인지 보여주는 블로그입니다!","link":"/2019/08/27/Starting-blog/"}],"tags":[{"name":"Today","slug":"Today","link":"/tags/Today/"}],"categories":[{"name":"Today","slug":"Today","link":"/categories/Today/"},{"name":"August","slug":"Today/August","link":"/categories/Today/August/"}]}