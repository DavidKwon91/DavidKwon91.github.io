<!DOCTYPE html>
<html  lang="en">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 3.9.0" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>Tag: ML - David Kwon</title>


    <meta property="og:type" content="website">
<meta property="og:title" content="David Kwon">
<meta property="og:url" content="http://yoursite.com/tags/ML/index.html">
<meta property="og:site_name" content="David Kwon">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/images/og_image.png">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="David Kwon">
<meta name="twitter:image" content="http://yoursite.com/images/og_image.png">








<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css">


    
    
    
    <style>body>.footer,body>.navbar,body>.section{opacity:0}</style>
    

    
    
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">
    

    
    

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">


    
    
    
    

<link rel="stylesheet" href="/css/back-to-top.css">


    
    

    
    
    
    

    
    
<link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

    
    
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-2-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/logo.svg" alt="David Kwon" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">Home</a>
                
                <a class="navbar-item"
                href="/archives">Archives</a>
                
                <a class="navbar-item"
                href="/categories">Categories</a>
                
                <a class="navbar-item"
                href="/tags">Tags</a>
                
                <a class="navbar-item"
                href="/about">About</a>
                
            </div>
            
            <div class="navbar-end">
                
                    
                    
                    <a class="navbar-item" target="_blank" title="Download on GitHub" href="https://github.com/DavidKwon91">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                    
                
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-8-widescreen has-order-2 column-main"><div class="card">
    <div class="card-content">
        <nav class="breadcrumb" aria-label="breadcrumbs">
        <ul>
            <li><a href="/tags">Tags</a></li>
            <li class="is-active"><a href="#" aria-current="page">ML</a></li>
        </ul>
        </nav>
    </div>
</div>

    <div class="card">
    
    <div class="card-image">
        <a href="/2019/10/24/PCA/" class="image is-7by1">
            <img class="thumbnail" src="/images/pca.png" alt="PCA">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-10-24T06:08:47.000Z">2019-10-24</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Principal-Components-Analysis/">Principal Components Analysis</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    12 minutes read (About 1767 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/10/24/PCA/">PCA</a>
            
        </h1>
        <div class="content">
            <h2 id="Introduction-to-Principal-Components-Analysis"><a href="#Introduction-to-Principal-Components-Analysis" class="headerlink" title="Introduction to Principal Components Analysis"></a>Introduction to Principal Components Analysis</h2><p>PCA (Principal Components Analysis) is a popular statistical method to reduce dimensions when the dimension of a dataset is huge, especially, when $n \geq m$ in $m \times n$ matrix or dataset.</p>
<p>Before digging into PCA, I start with some basic of linear algebra. </p>
<h4 id="1-Numerator-and-Denominator-Layout-in-Matrix-Differentiation"><a href="#1-Numerator-and-Denominator-Layout-in-Matrix-Differentiation" class="headerlink" title="1. Numerator and Denominator Layout in Matrix Differentiation"></a>1. Numerator and Denominator Layout in Matrix Differentiation</h4><p>In matrix differentiation, there are two notational convention to express the derivative of a vector with respect to a vector, that is $\frac{\partial y}{\partial x}$, which are “Numerator Layout” and “Denominator Layout”.</p>
<p>It doesn’t matter which way you would like to use, but make sure you stick with a notational convention if you start with a particular convention. </p>
<p>Let $x$ be a $n \times 1$ vector, $y$ be a $m \times 1$ vector. </p>
<p>Then, for $\frac{\partial y}{\partial x}$,</p>
<p>Numerator Layout: $m \times n$ matrix as following; </p>
<img src="/2019/10/24/PCA/numlay.png" title="[table]">

<p>Denominator Layout: $n \times m$ matrix as following;</p>
<img src="/2019/10/24/PCA/denomlay.png" title="[table]">

<p>The different kinds of two notational convention can be found from <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Vector-by-vector_identities" target="_blank" rel="noopener">Wiki</a></p>
<p>Most of books or papers don’t state which convention they use, but the Denominator Layout is mostly used in many papers or software. This article is going to use Denominator Layout in this article. </p>
<h4 id="2-Variance-Covariance-Matrix"><a href="#2-Variance-Covariance-Matrix" class="headerlink" title="2. Variance-Covariance Matrix"></a>2. Variance-Covariance Matrix</h4><p>Variance-Covariance Matrix, called just Covariance Matrix, of a matrix $X$ can be expressed in two ways as following;</p>
<p>$$S = \frac{1}{n-1} XX^T \tag{1}$$</p>
<p>or</p>
<p>$$S = \frac{1}{n-1} X^{T}X \tag{2}$$</p>
<p>where $X$ is the $m \times n$ matrix. </p>
<p>I will make the data be centered by subtracting the mean of the data for later convenience.  </p>
<p>The (1) equation will give you the covariance matrix by row vectors as following;</p>
<p>$$S_{n \times n} = \frac{1}{n-1} X_{n \times 1} X_{1 \times n}^T = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \mu)(x_i - \mu)^T \tag{3}$$</p>
<p>where $\mu$ is the row mean vector and $S$ will be the $n \times n$ square matrix. </p>
<p>The (2) equation will give you the covariance matrix by column vectors as following;</p>
<p>$$S_{m \times m} = \frac{1}{n-1} X_{m \times 1}^{T}X_{1 \times m} = \frac{1}{n-1} \sum_{i=1}^{m} (x_i - \mu)^{T}(x_i - \mu) \tag{4}$$</p>
<p>where $\mu$ is the column mean vector and $S$ will be the $m \times m$ square matrix.</p>
<h6 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h6><p>Let $X$ be $2 \times 3$ matrix as the following; </p>
<p>$$X = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{bmatrix}$$</p>
<p>Then, for (1) equation, </p>
<p>The row vectors will be $3 \times 1$ vector as following;</p>
<p>$X_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ and $X_2 = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}$.</p>
<p>Then, the covariance matrix will be</p>
<p>$$S_{3 \times 3} = \frac{1}{n-1} X_{3 \times 1} X_{1 \times 3}^{T}$$.</p>
<p>For (2) equation, </p>
<p>The column vectors will be $1 \times 2$ vectors as following;</p>
<p>$X_1 = \begin{bmatrix} 1 &amp; 4 \end{bmatrix}$, $X_2 = \begin{bmatrix} 2 &amp; 5 \end{bmatrix}$, $X_3 = \begin{bmatrix} 3 &amp; 6 \end{bmatrix}$.</p>
<p>Then, the covariance matrix will be </p>
<p>$$S_{2 \times 2} = \frac{1}{n-1} X_{2 \times 1}^{T} X_{1 \times 2}$$</p>
<p>In this paper, we will need the (2) covariance matrix since mostly columns are the predictors, and covariance matrix is calculated by the columns. </p>
<h4 id="3-Intuition-on-Eigenvalues-and-Eigenvectors"><a href="#3-Intuition-on-Eigenvalues-and-Eigenvectors" class="headerlink" title="3. Intuition on Eigenvalues and Eigenvectors"></a>3. Intuition on Eigenvalues and Eigenvectors</h4><p>Formal Definition of Eigenvalues and Eigenvectors:</p>
<p>Let A be $n \times n$ a sqaure matrix, then $v$ and $\lambda$ is the Eigenvector and the Eigenvalue of $A$, respectively, if</p>
<p>$$Av = \lambda v \tag{5}$$</p>
<p>where $v$ is a non-zero vector and $\lambda$ can be any scalar. </p>
<p>Geometrical meaning of Eigenvectors and Eigenvalues would be..</p>
<p>From <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors" target="_blank" rel="noopener">Wiki</a>,</p>
<p>Eigenvectors can be seen the scaled direction vectors in the difrection of the particular vector; therefore, the direction of the vector is preserved, but we are just scaling the lengths.<br>Eigenvalue can be seen the factor by which it is stretched. </p>
<p>For equation (5), it is equivalent to the following;</p>
<p>$$(A - \lambda I)v = 0$$</p>
<p>where $I$ is the $n \times n$ identity matrix, and $0$ is the zero vector. </p>
<p>Also, if the $(A - \lambda I)$ is invertible, then $v$ can be zero-vector; therefore, in order to have a non-zero vector, $v$, the $(A - \lambda I)$ must be not invertible. It leads the determinant of $(A - \lambda I)$ must be zero as the following; </p>
<p>$$det(A - \lambda I) = 0$$</p>
<h4 id="4-Eigendecomposition-or-Spectral-Decomposition"><a href="#4-Eigendecomposition-or-Spectral-Decomposition" class="headerlink" title="4. Eigendecomposition or Spectral Decomposition"></a>4. Eigendecomposition or Spectral Decomposition</h4><p>Let $A$ be $n \times n$ a sqaure matrix with its rank is $n$, $rank(A) = n$, which is equivalent to have the number of linearly independent of columns or rows. </p>
<p>Then, $A$ can be factorized as</p>
<p>$$A_{n \times n} = Q_{n \times n} \Lambda_{n \times n} Q_{n \times n}^{-1}$$</p>
<p>where $Q$ is the $n \times n$ square matrix that the $i$th column is the eigenvector of A, and the $\Lambda$ is the diagonal matrix that the diagonal elements are the eigenvalues of A, $\Lambda_{ii} = \lambda_i$.</p>
<p>If the $Q$ is orthogonal matrix, then $A$ is equivalent to the following;</p>
<p>$$A = Q \Lambda Q^{-1} = Q \Lambda Q^T \tag{6}$$</p>
<h4 id="5-SVD-Singular-Value-Decomposition"><a href="#5-SVD-Singular-Value-Decomposition" class="headerlink" title="5. SVD (Singular Value Decomposition)"></a>5. SVD (Singular Value Decomposition)</h4><p>Let $A$ be $m \times n$ a rectangular matrix, then $A$ can be factorized as,</p>
<p>$$A_{m \times n} = U_{m \times m} \Sigma_{m \times n} V_{n \times n}^{T}$$</p>
<p>where $U$ and $V$ is orthonormal square matrix, which means $U^{T}U = V^{T}V = I$, and $\Sigma$ is the diagonal matrix. </p>
<p>Then, the square matrix, </p>
<p>$$A_{m \times n}A_{n \times m}^{T} = (U \Sigma V^T) (U \Sigma V^T)^T \\ = (U \Sigma V^T) (V \Sigma^{T} U^{T}) \\ = U(\Sigma \Sigma^{T}) U^{T} \tag{7}$$</p>
<p>which gives us $U$ is the eigenvector of $AA^T$ by the definition of Eigendecomposition and the fact that we assume that $U$ is the orthonormal matrix, and $U$ is called left-singular vector. </p>
<p>$$A_{n \times m}^{T}A_{m \times n} = (U \Sigma V^T)^{T} (U \Sigma V^T) \\ = (V \Sigma^{T} U^{T}) (U \Sigma V^T) \\ = V (\Sigma^{T} \Sigma) V^{T} \tag{8}$$</p>
<p>which gives us $V$ is the eigenvector of $A^{T}A$ by the definition of Eigendecomposition and the fact that we assume that $V$ is the orthonormal matrix, and $V$ is called right-singular vector. </p>
<p>Also, $\Sigma \Sigma^{T} = \Sigma^{T} \Sigma$ is the diagonal matrix that the diagonal elements are the eigenvalues. </p>
<h4 id="6-PCA-Principal-Components"><a href="#6-PCA-Principal-Components" class="headerlink" title="6. PCA (Principal Components)"></a>6. PCA (Principal Components)</h4><p>The goal of the PCA is to identify the most meaningful basis to re-express a dataset. </p>
<p>Let $X$ be $m \times n$ matrix, $Y$ be another $m \times n$ matrix related by a linear transformation $P_{n \times n}$. </p>
<p>Then,</p>
<p>$$Y = XP \\ = \begin{bmatrix} x_{11} &amp; \dots &amp; x_{1n}  \\ \vdots &amp; \ddots \\ x_{m1} &amp; \dots &amp; x_{mn} \end{bmatrix}  \begin{bmatrix} p_{11} &amp; \dots &amp; p_{1n}  \\ \vdots &amp; \ddots \\ p_{n1} &amp; \dots &amp; p_{nn} \end{bmatrix}$$</p>
<p>which gives us the dot product of $X$ and $P$. </p>
<p>The dot product can be interpreted as a linear transformation or projecting $X$ onto another basis by $P$.</p>
<p>In this case, let’s see the covariance matrix of $Y$. Notice that the covariance matrix of $Y$ or the covariance matrix of $X$ are centered as stated above (3) or (4). </p>
<p>$$C_{Y} = \frac{1}{n-1} Y^{T}Y \\ = \frac{1}{n-1} (XP)^{T}(XP) \\ = \frac{1}{n-1}(P^{T}X^{T}XP) \\ = P^{T}C_{X} P$$</p>
<p>If we prove that the matrix $P$ is orthonormal matrix, then we can see </p>
<p>$$C_{Y} =  P^{T}C_{X} P =  P^{-1}C_{X} P$$</p>
<p>by (6)</p>
<p>and hence, it implies that $P$ is the eigenvector matrix of covariance matrix of $X$. </p>
<ol>
<li>First Approach to prove $P$ is orthonormal. </li>
</ol>
<p>We know that $C_{Y}$ is the symmetric matrix since it is the covariance matrix of $Y$, </p>
<p>then, $C_{Y} = C_{Y}^{T}$ is true. </p>
<p>$$C_{Y}^{T} = (P^{-1} C_{X} P)^{T} \\ = (C_{X})^{T} (P^{-1})^{T} = C_{Y} $$</p>
<p>Therefore, </p>
<p>$$P^{T} (C_{X})^{T} (P^{-1})^{T} =  P^{T}C_{X} P$$</p>
<p>We know that $C_X = C_{X}^{T}$ since the covariance matrix of $X$ is symmetric matrix.</p>
<p>Finally, </p>
<p>$$P^{T} C_{X} (P^{-1})^{T} =  P^{T}C_{X} P \\ (P^{-1})^{T} = P$$</p>
<p>Now, we can see the $P$ is orthonormal matrix. </p>
<p>This proof implies that we can get the $P$ from SVD by calculating $V$. </p>
<p>The covariance matrix of $X$ is.. </p>
<p>$$C_X = \frac{1}{n-1} X^{T}X$$ </p>
<p>By (8) in SVD, $X^{T}X$ is.. </p>
<p>$$X^{T}X = V (\Sigma_{X}^{T} \Sigma_{X}) V^{T}$$</p>
<p>Therefore, </p>
<p>$$V^{T} = P$$ </p>
<ol start="2">
<li>Second Approach to prove $P$ is orthonormal (this proof was provided by a member of my study group)</li>
</ol>
<p>We have </p>
<p>$$Y = XP$$</p>
<p>where $Y$ is the projected values for $X$ by linear tranformation of $P$. </p>
<p>Suppose we have $U$ as the following;</p>
<p>$$YU = X_{recovered}$$ </p>
<p>where $X_{recovered}$ is the re-proejcted values onto original values from the basis where the dataset are projected onto. </p>
<p>Then, </p>
<p>$$XPU = X_{recovered}$$</p>
<p>Therefore, </p>
<p>$$PU = I \tag{9}$$</p>
<p>Let $x$ is column vectors from $X$, and $\hat{x}$ is column vectors from $Y$.</p>
<p>$$x_{n \times 1} \in X_{m \times n}, \hat{x_{n \times 1}} \in Y_{m \times n}$$</p>
<p>The loss function of PCA is</p>
<p>$$\arg\min_{\hat{x}} ||x_{n \times 1} - U_{n \times n}\hat{x_{n \times 1}}||$$</p>
<p>To optimize, we differentiate the equation with respect to $\hat{x}$. </p>
<p>$$\frac{\partial}{\partial \hat{x}} ||x - U\hat{x}|| \\ = \frac{\partial}{\partial \hat{x}} (x-U\hat{x})^{T}(x-U\hat{x}) \\ = \frac{\partial}{\partial \hat{x}} (x^T - \hat{x}^{T}U^{T})(x-U\hat{x}) \\ = \frac{\partial}{\partial \hat{x}} (x^{T}x - \hat{x}^{T}U^{T}x - x^{T}U\hat{x} + \hat{x}^{T}\hat{x}) $$</p>
<p>Here, $\hat{x}^{T}U^{T}x = x^{T}U\hat{x}$ since they are scalar value. </p>
<p>$$= \frac{\partial}{\partial \hat{x}} (x^{T}x - 2x^{T}U\hat{x} + \hat{x}^{T}\hat{x}) \\ = 0 - 2U^{T}x + 2\hat{x} \tag{10}$$</p>
<p>By denominator layout as explained above, </p>
<p>$$\frac{\partial}{\partial \hat{x}} \hat{x}^{T}\hat{x} = 2\hat{x}$$ </p>
<p>$$\frac{\partial}{\partial \hat{x}} x^{T}U\hat{x} = (x^{T}U)^{T} = U^{T}x$$</p>
<p>Set the (10) equation as zero, </p>
<p>$$  - 2U^{T}x + 2\hat{x} = 0 \\ \hat{x} = U^{T}x$$</p>
<p>Therefore, $\hat{x} = Px$ from the first assumption, and $P = U^{T}$.</p>
<p>From (9) equation, </p>
<p>$$PU = PP^T = I \\ P^{-1} = P^T$$</p>
<p>We proved that $P$ is the orthonormal matrix. </p>
<p>As a result, </p>
<ol>
<li>$P$ is the Principal Components that allows us to get the most meaningful basis to re-express our dataset by linear transforming our dataset. </li>
<li>$P$ is the eigenvector matrix of the covariance matrix of our dataset for the column vectors (predictors)</li>
<li>$P$ is the transpose of right-singular vector, which is the $V^T$ in $U \Sigma V^T$, from Singular Value Decomposition. </li>
</ol>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/10/16/AUROC/" class="image is-7by1">
            <img class="thumbnail" src="/images/auroc.png" alt="Classfication">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-10-16T04:04:57.000Z">2019-10-16</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Classification/">Classification</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    13 minutes read (About 1977 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/10/16/AUROC/">Classfication</a>
            
        </h1>
        <div class="content">
            <h2 id="Introduction-to-Classification-Metrics"><a href="#Introduction-to-Classification-Metrics" class="headerlink" title="Introduction to Classification Metrics"></a>Introduction to Classification Metrics</h2><p>This paper will discuss on binary classification metrics, Sensitivity vs Specificity, Precision vs Recall, and AUROC. </p>
<h3 id="1-Confusion-Matrix"><a href="#1-Confusion-Matrix" class="headerlink" title="1. Confusion Matrix"></a>1. Confusion Matrix</h3><img src="/2019/10/16/AUROC/confusion.png" title="[table]">

<p>True Positive (TP) - Predicts a value as positive when the value is actually positive.<br>False Positive (FP) - Predicts a value as positive when the value is actually negative.<br>True Negative (TN) - Predicts a value as negative when the value is actually negative.<br>False Negative (FN) - Predicts a value as negative when the value is actually positive. </p>
<p>In Hypothesis test, we used to set up the null hypothesis is the against value. </p>
<p>What this means is that we want to set up the null hypothesis as the value such as a case that a person doesn’t get a disease or a case that a transaction is not a fraud, and the alternative hypothesis as opposite. </p>
<p>For example, we want to test if a patient in a group gets a disease.<br>Then, null hypothesis, $H_0$, is the patient doesn’t get a disease. With some test, if a p-value, which is the probability that happens an event in the probability distribution of null hypothesis, is less than a value like 0.05, then we reject the null hypothesis and accept the alternative hypothesis, which is to conclude that the patient gets a disease. </p>
<p>Actual Positive ($H_0$ is false) : The patient gets a disease.<br>Actual Negative ($H_0$ is true) : The patient doesn’t get a disease. </p>
<p>Predicted Positive (Accepting $H_1$: reject $H_0$) : The patient gets a disease<br>Predicted Negative (not Accepting $H_1$: fail to reject $H_0$) : The patient doesn’t get a disease. </p>
<p>Type 1 Error (False Positive) : When $H_0$ is false, which is that the patient actually gets a disease, but we reject $H_0$ by a classifier, which is that we predict patient doesn’t get a disease. </p>
<p>Type 2 Error (False Negative) : When $H_0$ is true, which is that the patient actually doesn’t get a disease, but we fail to reject $H_0$ by a classifier, which is that we predict patient gets a disease. </p>
<h3 id="2-Metrics"><a href="#2-Metrics" class="headerlink" title="2. Metrics"></a>2. Metrics</h3><p>Sensitivity, Recall, or True Positive Rate (TPR) : $\frac{TP}{TP + FN}$</p>
<p>Specificity, True Negative Rate (TNR), 1 - FPR: $\frac{TN}{TN + FP}$</p>
<p>Notice that the denominator for both is the total number of actual value. The denominator of sensitivity is the total number of actual positive value, and the denominator of specificity is the total number of actual negative value. </p>
<p>That means, sensitivity is the probability of correctly predicting positive values among actual positive values. Specificity is the probability of correctly predicting negative values among actual negative values. Therefore, with this metrics, we can see how a model correctly predicts overall actual values for both positive and negative values. </p>
<p>Precision, Positive Predicted Value : $\frac{TP}{TP + FP}$</p>
<p>Recall, Sensitivity, or True Positive Rate (TPR) : $\frac{TP}{TP + FN}$</p>
<p>Here, we can see that the denominator of precision is the total number of predicted value as positive. However, the denominator of recall is the total number of actual positive value. </p>
<p>That means, precision is the probability of correctly predicting positive values among predicted values as positve. This will show how useful the model is, or the quality of the model. Recall is the probability of correctly predicting positive values among actual positive values. This will show how complete the results are, or the quantity of the results. </p>
<p>Therefore, with this metrics, we can see how a model correctly predicts positive values among predicted values and actual values. </p>
<p>In above example of predicting diseased patients, we might want to predict the diseased patients more; therefore, we want to focus on increasing the TRUE POSITIVE values and not focus on predicting TRUE NEGATIVE, but not losing too much predicting accuracy, which is to focus on increasing Precision and Recall. </p>
<p>As a result, sensitivity and specificity is generally used for overall balanced binary target variable or a case that we don’t have to focus on positive or negative values like if it is a dog or a cat in an image classification, but precision and recall should be used to predict an imbalanced binary target variable. </p>
<h3 id="3-Trade-off"><a href="#3-Trade-off" class="headerlink" title="3. Trade-off"></a>3. Trade-off</h3><p>It will be the best scenario if we have high performance of sensitivity and specicity or precision and recall. However, in many cases, it is hard to see such cases since there is a trade-off. </p>
<p>For Precision and Recall, the only difference between them is the denominator. The denominator of precision has Type 1 Error (FP), and the denominator of recall has Type 2 Error (FN). </p>
<p>Precision, Positive Predicted Value : $\frac{TP}{TP + FP}$</p>
<p>Recall, Sensitivity, or True Positive Rate (TPR) : $\frac{TP}{TP + FN}$</p>
<p>Precision and Recall shares same parameter, which is TP; therefore, by shifting the threshold of probability that classifies the values, FP and FN values can be varied. </p>
<p>Example codes below with Pima Indians Diabetes</p>
<p>This dataset has imbalanced binary target variable, “diabetes” as below. </p>
<img src="/2019/10/16/AUROC/target.png" title="[table]">


<p>I splitted the dataset by training and test set, and performed logistic regression to predict the “diabetes” variable in the test set. </p>
<p>glm.pred1 is the predicted probability values that the patients is diabetes, which is “pos”. </p>
<p>When the threshold is 0.1, which is to classify the predicted value as “pos” if the probability is greater than 0.1. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred1 &lt;- as.factor(ifelse(glm.pred1 &gt;= 0.1, &quot;pos&quot;,&quot;neg&quot;))</span><br><span class="line">confusionMatrix(pred1, as.factor(test$diabetes), positive=&quot;pos&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/10/16/AUROC/pred1CM.png" title="[table]">


<p>When the threshold is 0.3.</p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred3 &lt;- as.factor(ifelse(glm.pred1 &gt;= 0.3, &quot;pos&quot;,&quot;neg&quot;))</span><br><span class="line">confusionMatrix(pred3, as.factor(test$diabetes), positive=&quot;pos&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/10/16/AUROC/pred3CM.png" title="[table]">

<p>When the threshold is 0.5.</p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred5 &lt;- as.factor(ifelse(glm.pred1 &gt;= 0.5, &quot;pos&quot;, &quot;neg&quot;))</span><br><span class="line">confusionMatrix(pred5, as.factor(test$diabetes), positive=&quot;pos&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/10/16/AUROC/pred5CM.png" title="[table]">

<p>When the threshold is 0.7.</p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred7 &lt;- as.factor(ifelse(glm.pred1 &gt;= 0.7, &quot;pos&quot;, &quot;neg&quot;))</span><br><span class="line">confusionMatrix(pred7, as.factor(test$diabetes), positive=&quot;pos&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/10/16/AUROC/pred7CM.png" title="[table]">

<p>As you can see above, by increasing the threshold value, Recall is decreasing, and Precision is increasing by trade-off because the FN is increasing and FP is decreasing. Therefore, we have to find the optimal threshold. </p>
<p>When the threshold is 0.5, we have the best Accuracy with 77.83%.<br>However, when the threshold is 0.3, it seems to have optimal values for Precision and Recall with.. </p>
<p>Precision : 0.6100<br>Recall : 0.7625</p>
<p>As I told above, when we predict imbalanced binary dataset and we want to focus on predicting the positive values like finding diabetes patients, then we want to increase the Precision and Recall values, even if the Accuracy is not the best value. </p>
<p>In this case, we would select the threshold as 0.3. Or, if the threshold is already set up, then we might have to change our model to improve the Precision and Recall. </p>
<p>The trade-off between Sensitivity and Specificity will be discussed below section, AUROC (Area Under ROC curve)</p>
<h3 id="4-AUROC"><a href="#4-AUROC" class="headerlink" title="4. AUROC"></a>4. AUROC</h3><p>ROC (Receiver Operating Characteristic) is a plot used to see the quality of the model in many cases of classification problem. The X-axis of ROC curve is False Positive Rate, and the Y-axis of the curve is True Positive Rate. It is created by various thresholds. </p>
<p>False Positive Rate: $\frac{FP}{FP + TN} = 1 - TNR = 1 - Specificity$</p>
<p>True Positive Rate, also Recall, and Sensitivity: $\frac{TP}{TP + FN}$</p>
<p>Therefore, sensitivty and specificity or ROC curve deal with the each two columns of confusion matrix. We can see the overall accuracy by various thresholds with the metrics for both positive and negative values. </p>
<p>The trade-off between Sensitivity and Specificty or ROC curve is quite similar with the trade-off between Precision and Recall as above example shows. </p>
<p>When threshold is 0.1, the Sensitivity is 0.95, and the Specificity is 0.3467, then the FPR will be $1-0.3467 = 0.6533$. </p>
<p>Threshold is 0.1 :</p>
<ul>
<li>Sensitivity = 0.95</li>
<li>Specificity = 0.3467</li>
<li>FPR = $1-0.3467 = 0.6533$</li>
</ul>
<p>Threshold is 0.3 : </p>
<ul>
<li>Sensitivity = 0.7625</li>
<li>Specificity = 0.7400</li>
<li>FPR = $1-0.7400 = 0.26$</li>
</ul>
<p>Threshold is 0.5 :</p>
<ul>
<li>Sensitivity = 0.6125</li>
<li>Specificity = 0.8667</li>
<li>FPR = $1-0.8667 = 0.1333$</li>
</ul>
<p>Threshold is 0.7 :</p>
<ul>
<li>Sensitivity = 0.4</li>
<li>Specificity = 0.9533</li>
<li>FPR = $1-0.9533 = 0.0467$</li>
</ul>
<p>In this scenario, we also would think that when the threshold is 0.3 is the optimal cutoff because they seem the optimal values for both. However, we can see the overall quality of the classifier with the AUC values. </p>
<p>AUC (Area Under Curve) is the area under the ROC curve. The greater the AUC value is, the better classifier we get. </p>
<p>Below is the implementation of my own functions for developing all of the above metrics, such as Precision and Recall, Sensitivity and Specificity, ROC curves, and AUC values. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">#This function creates a dataframe that has TP, TN, FP, and FN values</span><br><span class="line">#This function should have same levels for the both target and pred variable</span><br><span class="line">tptnfpfn &lt;- function(x,y)&#123;</span><br><span class="line">  tap &lt;- tapply(x,x,length)</span><br><span class="line">  f.names &lt;- tap[1] %&gt;% names</span><br><span class="line">  </span><br><span class="line">  if(tap[1] &gt; tap[2])&#123;</span><br><span class="line">    target &lt;- ifelse(x == f.names, 0, 1)</span><br><span class="line">    pred &lt;- ifelse(y == f.names, 0, 1)</span><br><span class="line">  &#125;</span><br><span class="line">  if(tap[2] &gt; tap[1])&#123;</span><br><span class="line">    target &lt;- ifelse(x == f.names, 1, 0)</span><br><span class="line">    pred &lt;- ifelse(y == f.names, 1, 0)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  #target &lt;- x</span><br><span class="line">  #pred &lt;- y</span><br><span class="line">  </span><br><span class="line">  dat &lt;- data.frame(target, pred)</span><br><span class="line">  </span><br><span class="line">  TP &lt;- length(which(dat$target == 1 &amp; dat$pred == 1))</span><br><span class="line">  FP &lt;- length(which(dat$target == 0 &amp; dat$pred == 1))</span><br><span class="line">  TN &lt;- length(which(dat$target == 0 &amp; dat$pred == 0))</span><br><span class="line">  FN &lt;- length(which(dat$target == 1 &amp; dat$pred == 0))</span><br><span class="line">  </span><br><span class="line">  new.dat &lt;- data.frame(TP,FP,TN,FN)</span><br><span class="line">  return(new.dat)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Precision = TP / (TP + FP) &lt;- the denominator is total predicted positive values</span><br><span class="line">precision &lt;- function(tp.dat)&#123;</span><br><span class="line">  precision &lt;- tp.dat$TP / (tp.dat$TP + tp.dat$FP)</span><br><span class="line">  return(precision)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Recall = sensitivity = TP / (TP + FN) &lt;- the denominator is total actual positive values </span><br><span class="line"></span><br><span class="line">recall &lt;- function(tp.dat)&#123;</span><br><span class="line">  recall &lt;- tp.dat$TP / (tp.dat$TP + tp.dat$FN)</span><br><span class="line">  return(recall)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#Sensitivity = Recall</span><br><span class="line"></span><br><span class="line">#Specificity = TN / (TN + FP) &lt;- the denominator is total actual negative values</span><br><span class="line">spec &lt;- function(tp.dat)&#123;</span><br><span class="line">  specificity &lt;- tp.dat$TN / (tp.dat$TN + tp.dat$FP)</span><br><span class="line">  return(specificity)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#ROC = TPR vs FPR = Recall vs 1-TNR = TP/(TP+FN) vs FP/(FP+TN)</span><br><span class="line">roc.func &lt;- function(target,pred)&#123;</span><br><span class="line">  dummy &lt;- data.frame(TPR = rep(0, length(target)), </span><br><span class="line">                      FPR = rep(0, length(target)), </span><br><span class="line">                      Spec = rep(0,length(target)),</span><br><span class="line">                      Precision = rep(0, length(target)),</span><br><span class="line">                      f1score = rep(0, length(target)))</span><br><span class="line">  </span><br><span class="line">  tap &lt;- tapply(target,target,length)</span><br><span class="line">  if(tap[1] &gt; tap[2])&#123;</span><br><span class="line">    f.name &lt;- levels(as.factor(target))[2]</span><br><span class="line">    s.name &lt;- levels(as.factor(target))[1]</span><br><span class="line">  &#125;</span><br><span class="line">  if(tap[2] &gt; tap[1])&#123;</span><br><span class="line">    f.name &lt;- levels(as.factor(target))[1]</span><br><span class="line">    s.name &lt;- levels(as.factor(target))[2]</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  for(i in 1:length(target))&#123;</span><br><span class="line">    #splitting the probabilities by cutoff with same levels</span><br><span class="line">    pred.cutoff &lt;- ifelse(pred &gt;= sort(pred)[i], f.name, s.name)</span><br><span class="line">    </span><br><span class="line">    tptn &lt;- tptnfpfn(target,pred.cutoff)</span><br><span class="line">    </span><br><span class="line">    dummy$cutoff[i] &lt;- sort(pred)[i]</span><br><span class="line">    dummy$TPR[i] &lt;- recall(tptn)</span><br><span class="line">    dummy$FPR[i] &lt;- tptn$FP / (tptn$FP + tptn$TN)</span><br><span class="line">    dummy$Spec[i] &lt;- spec(tptn)</span><br><span class="line">    dummy$Precision[i] &lt;- precision(tptn)</span><br><span class="line">    dummy$f1score[i] &lt;- f1.score(tptn)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  #dummy$TPR &lt;- ifelse(dummy$TPR == &quot;NaN&quot;, 0, dummy$TPR)</span><br><span class="line">  #dummy$FPR &lt;- ifelse(dummy$FPR == &quot;NaN&quot;, 0, dummy$FPR)</span><br><span class="line">  return(dummy)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#This auc function is from below link. </span><br><span class="line">#Refer to </span><br><span class="line">#https://mbq.me/blog/augh-roc/</span><br><span class="line">#a little changes is applied into the codes from above link</span><br><span class="line">#This is using the test statistic from &quot;Mann-Whitney-Wilcoxon test&quot;</span><br><span class="line">#Further link:</span><br><span class="line">#https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test#Area-under-curve_(AUC)_statistic_for_ROC_curves</span><br><span class="line">auc.func &lt;- function(target, pred)&#123;</span><br><span class="line">  tap &lt;- tapply(target, target, length)</span><br><span class="line">  f.name &lt;- tap[1] %&gt;% names</span><br><span class="line">  </span><br><span class="line">  if(tap[1] &gt; tap[2])&#123;</span><br><span class="line">    target1 &lt;- ifelse(target == f.name, TRUE, FALSE)</span><br><span class="line">  &#125;</span><br><span class="line">  if(tap[2] &gt; tap[1])&#123;</span><br><span class="line">    target1 &lt;- ifelse(target == f.name, TRUE, FALSE)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  n1 &lt;- sum(!target1)</span><br><span class="line">  n2 &lt;- sum(target1)</span><br><span class="line">  U &lt;- sum(rank(pred)[!target1]) - n1 * (n1 + 1) / 2</span><br><span class="line">  </span><br><span class="line">  return(1 - U / (n1*n2))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Below is the results by built-in syntax in R with “ROCR” package</p>
<img src="/2019/10/16/AUROC/1.png" title="[table]">
<img src="/2019/10/16/AUROC/2.png" title="[table]">
<img src="/2019/10/16/AUROC/8.png" title="[table]">

<p>Below is the results by the created functions as above. </p>
<img src="/2019/10/16/AUROC/3.png" title="[table]">
<img src="/2019/10/16/AUROC/4.png" title="[table]">
<img src="/2019/10/16/AUROC/5.png" title="[table]">
<img src="/2019/10/16/AUROC/6.png" title="[table]">
<img src="/2019/10/16/AUROC/7.png" title="[table]">


<p>Full implementation will be <a href="https://github.com/DavidKwon91/MathematicsBehindAlgorithms/blob/master/AUROC/AUROC.md" target="_blank" rel="noopener">here</a></p>
<p>Reference:<br><a href="https://en.wikipedia.org/wiki/Precision_and_recall" target="_blank" rel="noopener">Precision and Recall</a><br><a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank" rel="noopener">Receiver Operating Characteristic</a><br><a href="https://en.wikipedia.org/wiki/Type_I_and_type_II_errors" target="_blank" rel="noopener">Type 1 and Type 2 Error</a><br><a href="https://blog.revolutionanalytics.com/2017/03/auc-meets-u-stat.html" target="_blank" rel="noopener">AUC Meets the Wilcoxon-Mann-Whitney U-Statistic</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/10/07/Reticulate/" class="image is-7by1">
            <img class="thumbnail" src="/images/RorPython.jpeg" alt="Reticulate">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-10-07T03:00:23.000Z">2019-10-07</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/Language/">Language</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/Language/Python-in-R/">Python in R</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    4 minutes read (About 561 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/10/07/Reticulate/">Reticulate</a>
            
        </h1>
        <div class="content">
            <h2 id="R-Python"><a href="#R-Python" class="headerlink" title="R? Python?"></a>R? Python?</h2><p>My main language for data analysis is R, but sometimes Python is used for its convenience. I wanted to find a way of using both language interactively in R, and this article is the introduction of a R package, “Reticulate,” which allows us to use both language in R. </p>
<h3 id="Reticulate"><a href="#Reticulate" class="headerlink" title="Reticulate"></a>Reticulate</h3><p>The R package, “Reticulate,” is quite recent package published in July 2019. It is still being updated for some issues, but it’s useful if you want use Python modules in R. You can see and download a cheat sheet published in R website, <a href="https://resources.rstudio.com/rstudio-developed/reticulate" target="_blank" rel="noopener">Here</a>. </p>
<p>I performed simple decision tree and XGBoost with Bayesian Optimization with both languages, R and Python, through “Reticulate” package in R, and it works successfully. </p>
<p>For the preparateion to use Reticulate, you have to install or update your R, Rstudio, pip, conda, and Python as the recent version. </p>
<p>You start with installing “reticulate” package in R by </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;reticulate&quot;)</span><br><span class="line">library(reticulate)</span><br></pre></td></tr></table></figure>

<p>And, you can create your virtual environment locally, and make sure you have installed all modules of Python you will use in the environment. </p>
<p>Creation your Python virtual environment with</p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virtualenv_create(envname = &quot;yourname&quot;)</span><br></pre></td></tr></table></figure>

<p>Or, you can just use the environment that is already set up in local. Once you have create the environment, then you can see the list of environments with</p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virtualenv_list()</span><br></pre></td></tr></table></figure>

<p>And, you install the python modules into the virtual environment from your local conda or python environment. When you install those Python modules in virtual environment, you have to restart R and make sure the modules that you are going to use has been installed in your virtual environment by</p>
<figure class="highlight plain hljs"><figcaption><span>createPythonEnv&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#virtualenv_install(&quot;r-reticulate&quot;, &quot;bayesian-optimization&quot;)</span><br><span class="line">#virtualenv_install(&quot;r-reticulate&quot;, &quot;pandas&quot;)</span><br><span class="line">#virtualenv_install(&quot;r-reticulate&quot;, &quot;seaborn&quot;)</span><br><span class="line">#virtualenv_install(&quot;r-reticulate&quot;, &quot;sklearn&quot;)</span><br><span class="line">#virtualenv_install(&quot;r-reticulate&quot;, &quot;xgboost&quot;)</span><br><span class="line">use_virtualenv(&quot;r-reticulate&quot;)</span><br><span class="line"></span><br><span class="line">py_module_available(&quot;seaborn&quot;)</span><br><span class="line">py_module_available(&quot;sklearn&quot;)</span><br><span class="line">py_module_available(&quot;pandas&quot;)</span><br><span class="line">py_module_available(&quot;bayes_opt&quot;)</span><br><span class="line">py_module_available(&quot;xgboost&quot;)</span><br></pre></td></tr></table></figure>

<p>Below is the implementation for simple decision tree in R. </p>
<img src="/2019/10/07/Reticulate/dt1.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/dt2.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/dt3.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/dt4.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/dt5.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/dt6.png" title="[Reticulate]">


<p>Here is the implementation for simple decision tree with Python Virtual Envrionment in R.</p>
<p>You have to declare that you will be using Python environment by</p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">repl_python()</span><br></pre></td></tr></table></figure>

<p>Or, if you are using rMarkdown, then you make chunk with</p>
<img src="/2019/10/07/Reticulate/pythonDec.png" title="[Reticulate]">

<p>Performing simple decision tree with Python virtual environment. </p>
<img src="/2019/10/07/Reticulate/pydt1.png" title="[Reticulate]">

<p>You can bring the R objects into Python environment by </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r.yourobjects</span><br></pre></td></tr></table></figure>

<img src="/2019/10/07/Reticulate/pydt2.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/pydt3.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/pydt4.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/pydt5.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/pydt6.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/pydt7.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/pydt8.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/pydt9.png" title="[Reticulate]">


<p>After you are done with Python environment, you call the following codes in your console</p>
<img src="/2019/10/07/Reticulate/pythonDec2.png" title="[Reticulate]">


<p>You can also bring the Python objects into R environment by</p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">py$yourobjects</span><br></pre></td></tr></table></figure>

<img src="/2019/10/07/Reticulate/pytoR.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/fromPythontoR-1.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/pytoR2.png" title="[Reticulate]">
<img src="/2019/10/07/Reticulate/fromPythontoR-2.png" title="[Reticulate]">




<p><a href="https://github.com/DavidKwon91/MathematicsBehindAlgorithms/blob/master/Reticulate/PythonInR.md" target="_blank" rel="noopener">Here</a> is the full implemented codes for the “reticulate” package. </p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/10/05/Monty-Hall/" class="image is-7by1">
            <img class="thumbnail" src="/images/montyhall.png" alt="Monty Hall">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-10-05T09:53:20.000Z">2019-10-05</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Monty-Hall/">Monty Hall</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    3 minutes read (About 516 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/10/05/Monty-Hall/">Monty Hall</a>
            
        </h1>
        <div class="content">
            <h2 id="Monty-Hall-Problem-with-Bayes’-Theorem"><a href="#Monty-Hall-Problem-with-Bayes’-Theorem" class="headerlink" title="Monty Hall Problem with Bayes’ Theorem"></a>Monty Hall Problem with Bayes’ Theorem</h2><p>Monty Hall Problem is a famous probability problem based on the TV show, “Let’s make a Deal.” </p>
<p>Suppose that you are a contestant and are asked to choose a door among 3 doors, which contains 1 car and 2 goats. If you choose a door that is car behind the door, then you win the prize. </p>
<p>Let C = car behind the door $i$, and D = open door $j$. </p>
<p>Here, we can formulate the probability for the car behind the door as the following:</p>
<p>$$P(C=1) = 1/3 \qquad P(C = 2) = 1/3 \qquad P(C = 3) = 1/3$$</p>
<p>Let’s say you pick the door 1, and host pick one of door 2 or 3, which goat behind the door. You now have a choice whether you stick with the originally choosed door and you switch. </p>
<p>Suppose the host choose the door 3, given that a goat is behind the door 3. </p>
<p>The trick of the probablity here is that the host knows which door has a car behind it and will never open the door. Then, there’s 50-50 chances that one of the two doors, which is not the door that host opens, has a car behind it. </p>
<p>The probability of the door that is chosen by the host, given that the car behind the door, is zero, since the host never pick the door that has a car,<br>the probability of the door that is chosen by the contestant, given that the car behind the door, is a half, since it is the half probability,<br>and the probability of the door that is not chosen by the host, given that the car behind the door, is one, because the host never pick the door that has a car and the door that the contestant chose, there’s only one door left. </p>
<p>Then, the likelihood will be </p>
<p>$$P(D = 3 | C = 1) = 1/2 \qquad P(D = 3 | C = 2) = 1 \qquad P(D = 3 | C = 1) = 0 $$</p>
<p>Now, the formula of Bayes Theorem is the following:</p>
<p>$$P(C = i | D = 3) = \frac{P(D = 3 | C = i)P(C = i)}{P(D = 3)}$$</p>
<p>It is easy to answer that the probability for $P(D = 3)$ is $\frac{1}{2}$, because you already choose door 1, and the host will choose one of the rest 2 doors. However, we can calculate the marginal probability for $P(D = 3)$ as the following. </p>
<p>$$P(D = 3) = \sum_{i=1}^{3} P(C = i, D = 3) = \sum_{i=1}^{3} P(D = 3 | C = i)P(C = i) = \\ P(D = 3 | C = 1)P(C = 1) + P(D = 3 | C = 2)P(C = 2) + P(D = 3 | C = 3)P(C = 3) = \\ \frac{1}{3} \times \frac{1}{2} + \frac{1}{3} \times 1 + \frac{1}{3} \times 0 = \frac{1}{2}$$</p>
<p>Now, let’s calculate the posterior probability in the Bayes Theorem, which is the probability of the door has a car behind it, given the door 3 has opened. </p>
<p>$$P(C = 1 | D = 3) = \frac{P(D = 3 |C = 1)P(C = 1)}{P(D = 3)} =  \frac{\frac{1}{2} \times \frac{1}{3}}{\frac{1}{2}} = \frac{1}{3}$$</p>
<p>$$P(C = 2 | D = 3) = \frac{P(D = 3 |C = 2)P(C = 2)}{P(D = 3)} =  \frac{1 \times \frac{1}{3}}{\frac{1}{2}} = \frac{2}{3}$$</p>
<p>$$P(C = 3 | D = 3) = \frac{P(D = 3 |C = 3)P(C = 3)}{P(D = 3)} =  \frac{0 \times \frac{1}{3}}{\frac{1}{2}} = 0$$</p>
<p>Therefore, as a contestant, you have to switch your choice to door 2 that has more probability than the door you chose at the first time. </p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/09/19/DecisionTree/" class="image is-7by1">
            <img class="thumbnail" src="/images/treeimage.jpg" alt="DecisionTree">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-19T08:13:52.000Z">2019-09-19</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Decision-Tree/">Decision Tree</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    15 minutes read (About 2225 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/19/DecisionTree/">DecisionTree</a>
            
        </h1>
        <div class="content">
            <h1 id="Introduction-to-Decision-Tree"><a href="#Introduction-to-Decision-Tree" class="headerlink" title="Introduction to Decision Tree"></a>Introduction to Decision Tree</h1><p>In this post, I will discuss about Decision Tree, which is the fundamental algorithm of the well-known Tree-based algorithms. This post will also address two metrics that are employed in Decision tree and many other machine learning algorithms, even in deep learning. </p>
<h2 id="1-Metric"><a href="#1-Metric" class="headerlink" title="1. Metric"></a>1. Metric</h2><p>Why do we need the “Metric” in tree-based algorithm? </p>
<p>The answer will be to find the best splits in the tree. We use the metric, calculate a variable, measure the split score with the value, and choose the value for the best split at each step of finding the split or the predictors in the tree. Many other metrics or score functions are used to find the “best” split for different tree-based algorithms. Two metrics will be discussed, which are Gini Impurity (or Gini Index) and Information Gain with Entropy. </p>
<h4 id="1-1-Gini-Index"><a href="#1-1-Gini-Index" class="headerlink" title="1.1 Gini Index"></a>1.1 Gini Index</h4><p>Suppose we have a dependent categorical variable, which have $\mathcal{J}$ classes. Let $i \in \{1,2,…,\mathcal{J}\}$ and $p_i$ is the probability given each factor levels of dependent variable.d</p>
<p>Then, the Gini Index, or Gini Impurity is defined as below, </p>
<p>$$I_{\mathcal{G}}(p) = \sum_{i=1}^{J}p_i \sum_{k \neq i}p_k = \sum_{i=1}^{J}p_i(1-p_i) = \sum_{i=1}^{J}(p_i-p_i^2) = \sum_{i=1}^{J}p_i - \sum_{i=1}^{J}p_i^2 \\ = 1-\sum_{i=1}^{J}p_i^2$$</p>
<p>For example, </p>
<p>suppose we have a binary variable that follows below. </p>
<p>$X = [FALSE,FALSE,TRUE,TRUE,….,TRUE,FALSE]$</p>
<img src="/2019/09/19/DecisionTree/image1.png" title="[table]">

<p>Then, the Gini Impurity will be the followng by the definition,</p>
<p>$$1 - (\frac{8}{20})^2 - (\frac{12}{20})^2 = 0.48$$</p>
<p>What about the Gini Impurity for this binary variable?</p>
<img src="/2019/09/19/DecisionTree/image2.png" title="[table]">

<p>Then, the Gini Impurity will be</p>
<p>$$1 - (\frac{1}{20})^2 - (\frac{19}{20})^2 = 0.095$$</p>
<p>Now, we see that the smaller Gini Impurity is, we find the better split point.</p>
<p>Therefore, we want to find the smaller Gini Impurity score as possible as we can at each step to find best split in the tree. </p>
<p>Now, let we have two binary variables and make table for each factors like the below, </p>
<p>$$Y = [FALSE, FALSE, … ,  TRUE] \\ X = [YES, YES, … , NO]$$</p>
<p>And, we want to predict $Y$ with $X$. </p>
<img src="/2019/09/19/DecisionTree/image3.png" title="[table]">

<p>We will have only one stump in a Decision Tree for this table. The root node, which is the very top of the tree, will be the predictor, $X = [YES, YES, … , NO]$</p>
<p>The tree is something like..</p>
<img src="/2019/09/19/DecisionTree/Tree1.png" title="[table]">

<p>The typical tree in real world is much more complicated with more nodes and splits, since they have many predictors.<br>This tree that has only one split is sometimes called a stump, which is often called a weak learner in Adaboost.<br>But, I will just call this as a tree for this example.  </p>
<p>Then, the process of finding Gini Impurity is the below;</p>
<p>1) Find Conditional probability</p>
<p>For example, the Conditional probability given $X$ is</p>
<p>$$P(Y = FALSE |X = YES) = \frac{5}{7} \\ P(Y = TRUE | X = YES) = \frac{2}{7} \\ P(Y = FALSE | X = NO) = \frac{3}{13} \\ P(Y = TRUE | X = NO) = \frac{10}{13}$$</p>
<p>2) Find each Gini Index for the conditional probability</p>
<p>$$I_{Gini}(Y|X=YES) = 1 - (\frac{5}{7})^2 - (\frac{2}{7})^2  =  0.4081633 \\ I_{Gini}(Y|X=NO) = 1 - (\frac{3}{13})^2 - (\frac{10}{13})^2 = 0.3550296 $$</p>
<p>Then, we find a Gini Impurity score for a leaf node in this tree, which is the very bottom of the tree or the final classification. </p>
<img src="/2019/09/19/DecisionTree/Tree2.png" title="[table]">



<p>3) Find Marginal probability</p>
<p>The margianl probabilty given $X$ is</p>
<p>$$P_X(X=YES) = \frac{7}{20} \\ P_X(X=NO) = \frac{13}{20}$$</p>
<p>Since the tree is splitted by $X$, we find the Gini Impurity of the total tree. </p>
<p>4) Multiply and add the Gini Index and marginal probability associated with the given probability respectively. Finally, we have Gini Impurity Score for a split. </p>
<p>Then, the <em>Total Gini Impurity Score</em> for this tree is, </p>
<p>$$I_{Gini}(Y|X=YES) \times P_X(X=YES) + I_{Gini}(Y|X=NO) \times P_X(X=NO) = \\ 0.4081633 \times \frac{7}{20} + 0.3550296 \times \frac{13}{20} = 0.3736264$$</p>
<p>Then, we eventually find the total Gini Impurity score for a tree. </p>
<img src="/2019/09/19/DecisionTree/Tree3.png" title="[table]">

<p>In real world problem, we have many predictors and so many nodes, possibly many trees such in Random Forest. </p>
<p>With this process of finding Gini Impurity, we compare the Gini score and find the best split point of the best predictor with best splits. </p>
<h4 id="1-2-Entropy"><a href="#1-2-Entropy" class="headerlink" title="1.2 Entropy"></a>1.2 Entropy</h4><p>Entropy is defined as below; </p>
<p>$$H(T) = -\sum_{i=1}^{J}p_ilog_2(p_i)$$</p>
<p>And the process of finding best split in a tree is to find the Information Gain. </p>
<p>The Information Gain is often called <em>Kullback-Leiber divergence</em> and defined as below;</p>
<p>$$IG(T,a) = H(T) - H(T|a)$$</p>
<p>where $IG(Y,X)$ is the Information Gain of Y given X, $H(Y)$ is the parent Entropy of Y, and $H(Y|X)$ is the children Entropy of Y given X, and the $H(Y|X)$ is often called <em>cross-entropy</em>.</p>
<p>The process of finding Information Gain is very similar to the Gini Score, but here we find the bigger value of IG. </p>
<p>I will use the same example problem as above. </p>
<p>For the above example, </p>
<p>$$Y = [FALSE, FALSE, … ,  TRUE] \\ X = [YES, YES, … , NO]$$</p>
<img src="/2019/09/19/DecisionTree/image3.png" title="[table]">

<p>Then, as I have shown, find the conditional probability. </p>
<p>1) Find the conditional probability given $X$</p>
<p>$$P(Y = FALSE |X = YES) = \frac{5}{7} \\ P(Y = TRUE | X = YES) = \frac{2}{7} \\ P(Y = FALSE | X = NO) = \frac{3}{13} \\ P(Y = TRUE | X = NO) = \frac{10}{13}$$</p>
<p>2) Find the entropy given $X$</p>
<p>$$I_{Entropy}(Y|X=YES) = -\frac{5}{7}log_2(\frac{5}{7}) - \frac{2}{7}log_2(\frac{2}{7}) = 0.8631206\\ I_{Entropy}(Y|X=NO) = -\frac{3}{13}log_2(\frac{3}{13}) - \frac{10}{13}log_2(\frac{10}{13})  =  0.7793498 $$</p>
<p>3) Find the marginal probability</p>
<p>The margianl probabilty given $X$ is</p>
<p>$$P_X(X=YES) = \frac{7}{20} \\ P_X(X=NO) = \frac{13}{20}$$</p>
<p>4) Calculate the Total Entropy and Cross-Entropy</p>
<p>Total Entropy for a tree divided by $X$ is</p>
<p>$$-P_X(X=YES) \times log_2(P_X(X=YES))-P_X(X=NO) \times log_2(P_X(X=NO)) \\ = -\frac{7}{20}log_2(\frac{7}{20}) - \frac{13}{20}log_2(\frac{13}{20}) \\ = 0.9340681$$</p>
<p>Cross Entropy for a leaf node is</p>
<p>$$I_{Entropy}(Y|X=YES) \times  P_X(X=YES) + I_{Entropy}(Y|X=NO) \times P_X(X=NO) \\ = 0.8631206 \times \frac{7}{20} + 0.7793498 \times \frac{13}{20} \\ = 0.8086696$$</p>
<p>5) Find Information Gain with Total Entropy and Cross Entropy</p>
<p>Information Gain = Total Entropy - Cross Entropy</p>
<p>$$= 0.9340681 - 0.8086696 = 0.1253985$$ </p>
<p>For another example that shows why bigger Information Gain is better, </p>
<p>suppose we have the dataset following; </p>
<img src="/2019/09/19/DecisionTree/table.png" title="[table]">

<p>It’s obviously better to split for Y given X than the above example, since this has only one mistake. </p>
<p>If we find the Information Gain of this data,</p>
<p>The Total Entropy for a tree divided by $X$ is 1, since this splits exactly the same number of target observations with 10 and 10. </p>
<p>$$-P_X(X=YES) \times log_2(P_X(X=YES))-P_X(X=NO) \times log_2(P_X(X=NO)) = \\ -\frac{10}{20}log_2(\frac{10}{20}) - \frac{10}{20}log_2(\frac{10}{20}) \\ = 0.5 + 0.5 = 1$$</p>
<p>The cross entropy for a leaf node is</p>
<p>$$I_{Entropy}(Y|X=YES) \times  P_X(X=YES) + I_{Entropy}(Y|X=NO) \times P_X(X=NO) \\ = 0.234478$$</p>
<p>calculated by the same process of the above.</p>
<p>Then, the Information Gain is</p>
<p>$$1 - 0.234478 = 0.7655022$$</p>
<p>Therefore, we find the bigger Information Gain as possible as we can. </p>
<p>As a summary, Decision Tree Algorithm is..</p>
<p>1) Find the best split with the metric score for each splits and each predictors. </p>
<p>2) Compare the metric score with every splits and other predictors and find the best split and best predictors. </p>
<p>3) The best predictors that has the best metric score will be the roof node, and keep building a tree with the metric scores. </p>
<p>4) The very bottom of the tree, which is the leaf node, will be our final classification. </p>
<p>If we have continuous predictors, there is no split point like “YES” or “NO”. Therefore, </p>
<p>1) we sort the data associated with the numerical predictors, </p>
<p>2) calculate the average for all adjacent indexes of the numerical predictor, </p>
<p>find the Gini Impurity for all the adjacent average value, </p>
<p>and compare the Gini Impurity to find the best split point of adjacent average of the numerical predictors. </p>
<p>Below is the Implementation in R. I used iris dataset, and a dataset that I created.<br>Some of codes are skipped, and you can refer to my Gibhub website to see full algorithm. </p>
<p><a href="https://github.com/DavidKwon91/MathematicsBehindAlgorithms" target="_blank" rel="noopener">DavidGithub</a></p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#removing one of the classes of the target variable, virginica in Iris Dataset. </span><br><span class="line">#to make target as a binary variable</span><br><span class="line">iris1 &lt;- iris[which(iris$Species != &quot;virginica&quot;),]</span><br><span class="line"></span><br><span class="line">#removing the factor level that we don&apos;t have any more</span><br><span class="line">iris1$Species &lt;- as.factor(as.character(iris1$Species))</span><br><span class="line"></span><br><span class="line">#quick decision tree built in R, rpart</span><br><span class="line">tr &lt;- rpart(Species~., training)</span><br><span class="line">rpart.plot(tr)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/19/DecisionTree/IntroductionDecisionTree-1.png" title="[tree1]">


<p>This is the prediction by Decision Tree built in R. </p>
<img src="/2019/09/19/DecisionTree/IntroductionDecisionTree-2.png" title="[tree1]">


<p>Here is the Decision Tree algorithm with the introduced two metrics that I built. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line">#average function to create adjacent average between predictor indexes</span><br><span class="line">avg &lt;- function(x1,x2)&#123;sum(x1,x2)/2&#125;</span><br><span class="line"></span><br><span class="line">#gini function for a leaf</span><br><span class="line">gini &lt;- function(x)&#123;</span><br><span class="line">  </span><br><span class="line">  if(dim(x)[1]==1)&#123;</span><br><span class="line">    return(1)</span><br><span class="line">  &#125;</span><br><span class="line">  else&#123;</span><br><span class="line">    #conditional probability given predictor (row : target, column : predictor)</span><br><span class="line">    p11&lt;-x[1,1]/sum(x[,1])</span><br><span class="line">    p21&lt;-x[2,1]/sum(x[,1])</span><br><span class="line">    p12&lt;-x[1,2]/sum(x[,2])</span><br><span class="line">    p22&lt;-x[2,2]/sum(x[,2])</span><br><span class="line">    </span><br><span class="line">    a.false.gini &lt;- 1-p11^2-p21^2</span><br><span class="line">    a.true.gini &lt;- 1-p12^2-p22^2</span><br><span class="line">    </span><br><span class="line">    #marginal probability</span><br><span class="line">    a.false.prob &lt;- (x[1,1]+x[1,2]) / sum(x)</span><br><span class="line">    a.true.prob &lt;- (x[2,1]+x[2,2]) / sum(x)</span><br><span class="line">    </span><br><span class="line">    gini.imp &lt;- a.false.prob * a.false.gini + a.true.prob * a.true.gini</span><br><span class="line">    return(gini.imp)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#log function with base 2 for entropy</span><br><span class="line">log2 &lt;- function(x)&#123;</span><br><span class="line">  if(x!=0)&#123;</span><br><span class="line">    return(log(x,base=2))</span><br><span class="line">  &#125;</span><br><span class="line">  if(x==0)&#123;</span><br><span class="line">    return(0)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#entropy function for a leaf</span><br><span class="line">entropy &lt;- function(x)&#123;</span><br><span class="line">  </span><br><span class="line">  if(dim(x)[1]==1)&#123;</span><br><span class="line">    return(0)</span><br><span class="line">  &#125;</span><br><span class="line">  else&#123;</span><br><span class="line">  	#conditional probability given predictor (row : target, column : predictor)</span><br><span class="line">    p11&lt;-x[1,1]/sum(x[,1])</span><br><span class="line">    p21&lt;-x[2,1]/sum(x[,1])</span><br><span class="line">    p12&lt;-x[1,2]/sum(x[,2])</span><br><span class="line">    p22&lt;-x[2,2]/sum(x[,2])</span><br><span class="line">    </span><br><span class="line">    #Calculating weights, which is the bottom of the tree</span><br><span class="line">    a.false.entropy &lt;- -(p11*log2(p11)+p21*log2(p21))</span><br><span class="line">    a.true.entropy &lt;- -(p12*log2(p12)+p22*log2(p22))</span><br><span class="line">    </span><br><span class="line">    #marginal probability</span><br><span class="line">    a.false.prob &lt;- (x[1,1]+x[2,1]) / sum(x)</span><br><span class="line">    a.true.prob &lt;- (x[1,2]+x[2,2]) / sum(x)</span><br><span class="line">    </span><br><span class="line">    #cross entropy</span><br><span class="line">    weighted.entropy &lt;- a.true.prob*a.true.entropy + a.false.prob*a.false.entropy</span><br><span class="line">    </span><br><span class="line">    #total entropy</span><br><span class="line">    total.entropy &lt;- -(a.false.prob*log2(a.false.prob) + a.true.prob*log2(a.true.prob))</span><br><span class="line">    </span><br><span class="line">    #Information Gain, which is the tree score to find best split</span><br><span class="line">    #If the bigger this value is, we find the better split</span><br><span class="line">    #maximum value is 1</span><br><span class="line">    IG &lt;- total.entropy - weighted.entropy</span><br><span class="line">    </span><br><span class="line">    return(IG) </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Calculating impurity to find which predictor is the best to split, which will be the top of the tree or first split in the tree</span><br><span class="line">var.impurity &lt;- function(x, dat, fun)&#123;</span><br><span class="line">  imp.dat &lt;- data.frame(matrix(0, nrow=nrow(dat)-1, ncol=3))</span><br><span class="line">  colnames(imp.dat) &lt;- c(&quot;index&quot;, &quot;impurity&quot;, &quot;adj.avg&quot;)</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">  for(i in 1:(nrow(dat)-1))&#123;</span><br><span class="line">    imp.dat[i,1] &lt;- paste0(&quot;between &quot;, i, &quot; and &quot;, i+1)</span><br><span class="line">    #average value of the adjacent values</span><br><span class="line">    a &lt;- avg(x[i], x[i+1])</span><br><span class="line">    </span><br><span class="line">    predictor.name &lt;- colnames(dat)[which(sapply(dat, function(x,want) isTRUE(all.equal(x,want)),x)==TRUE)]</span><br><span class="line">    </span><br><span class="line">    #Sorting the data by the predictor</span><br><span class="line">    dat1 &lt;- dat[order(x,decreasing=FALSE),]</span><br><span class="line">    mat &lt;- as.matrix(table(dat1[,predictor.name] &lt; a, dat1[,target] ))</span><br><span class="line">    </span><br><span class="line">    #apply the metric, Gini or Entropy</span><br><span class="line">    imp.dat[i,2] &lt;- fun(mat)</span><br><span class="line">    imp.dat[i,3] &lt;- a</span><br><span class="line">    &#125;</span><br><span class="line">  return(imp.dat)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#this function will give you the best split score for each predictors</span><br><span class="line">impurity.fun &lt;- function(dat, fun)&#123;</span><br><span class="line">  predictors &lt;- colnames(dat)[!colnames(dat) %in% target]</span><br><span class="line">  var.impur.dat &lt;- data.frame(matrix(0, nrow=length(predictors),ncol=2))</span><br><span class="line">  colnames(var.impur.dat) &lt;- c(&quot;var&quot;, &quot;impurity&quot;)</span><br><span class="line">  </span><br><span class="line">  for(i in 1:(ncol(dat)-1))&#123;</span><br><span class="line">    var.impur.dat[i,1] &lt;- predictors[i]</span><br><span class="line">    if(fun == &quot;gini&quot;)&#123;</span><br><span class="line">      #the least score of gini is the best split </span><br><span class="line">      var.impur.dat[i,2] &lt;- min(var.impurity(dat[,i], dat, gini)$impurity)</span><br><span class="line">    &#125;</span><br><span class="line">    if(fun == &quot;entropy&quot;)&#123;</span><br><span class="line">      #the greates score of entropy is the best split</span><br><span class="line">      var.impur.dat[i,2] &lt;- max(var.impurity(dat[,i], dat, entropy)$impurity)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  return(var.impur.dat)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#give you the best predictor to split or the top of the tree</span><br><span class="line">topTree.predictor &lt;- function(x,fun)&#123;</span><br><span class="line">  if(fun == &quot;entropy&quot;)&#123;</span><br><span class="line">    return(which.max(impurity.fun(x, &quot;entropy&quot;)[,2]))</span><br><span class="line">  &#125;</span><br><span class="line">  if(fun == &quot;gini&quot;)&#123;</span><br><span class="line">    return(which.min(impurity.fun(x, &quot;gini&quot;)[,2]))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#The best split point associated with the best predictor</span><br><span class="line">impurityOfbest &lt;- function(dat, best.pred, fun)&#123;</span><br><span class="line">  if(fun == &quot;entropy&quot;)&#123;</span><br><span class="line">    impurity.pred &lt;- var.impurity(dat[,best.pred], dat, entropy)$adj.avg[which.max(var.impurity(dat[,best.pred], dat, entropy)$impurity)]</span><br><span class="line">  &#125;</span><br><span class="line">  if(fun == &quot;gini&quot;)&#123;</span><br><span class="line">    impurity.pred &lt;- var.impurity(dat[,best.pred], dat, gini)$adj.avg[which.min(var.impurity(dat[,best.pred], dat, gini)$impurity)]</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  print(paste0(&quot;Best predictor, which is top tree node is &quot;, colnames(dat)[best.pred], &quot; with best split is  &quot;, impurity.pred, &quot; by the metric, &quot;, fun))</span><br><span class="line">  return(impurity.pred)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#by Entropy metric, we want to find the maximum entropy score</span><br><span class="line">fun &lt;- &quot;entropy&quot;</span><br><span class="line">target &lt;- &quot;Species&quot;</span><br><span class="line"></span><br><span class="line">imp.pred &lt;- topTree.predictor(iris1, fun) #This is the best predictor that splits our target the best. </span><br><span class="line">best.impur &lt;- impurityOfbest(iris1, imp.pred, fun) #This is the best split point for the best predictor. </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Let&apos;s see how well this value calculated by the function predicts</span><br><span class="line">table(iris1[,imp.pred] &lt; best.impur, iris1$Species)</span><br><span class="line">#perfectly predicted in training set</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pred.by&lt;- as.factor(ifelse(iris1[,imp.pred] &lt; best.impur, &quot;setosa&quot;,&quot;versiclor&quot;))</span><br><span class="line"></span><br><span class="line">t1 &lt;- iris1 %&gt;% </span><br><span class="line">  ggplot(aes(x=Petal.Width, y=Petal.Length ,col=Species)) + </span><br><span class="line">  geom_jitter() +</span><br><span class="line">  ggtitle(&quot;Actual&quot;)</span><br><span class="line">t2 &lt;- iris1 %&gt;% </span><br><span class="line">  ggplot(aes(x=Petal.Width, y=Petal.Length ,col=pred.by)) + </span><br><span class="line">  geom_jitter() +</span><br><span class="line">  geom_vline(xintercept =  best.impur, colour=&quot;blue&quot;, linetype=&quot;dashed&quot;) + </span><br><span class="line">  annotate(geom=&quot;text&quot;, label=best.impur, x=best.impur, y=0, vjust=-1) +</span><br><span class="line">  ggtitle(&quot;Predicted&quot;)</span><br><span class="line"></span><br><span class="line">grid.arrange(t1,t2)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/19/DecisionTree/Prediction-1.png" title="[tree1]">

















        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/09/15/GaussianProcess/" class="image is-7by1">
            <img class="thumbnail" src="/images/GP image.gif" alt="GaussianProcess">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-15T09:07:27.000Z">2019-09-15</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/ML/">ML</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/ML/Gaussian-Process/">Gaussian Process</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    12 minutes read (About 1840 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/15/GaussianProcess/">GaussianProcess</a>
            
        </h1>
        <div class="content">
            <h2 id="Introduction-to-Gaussian-Process"><a href="#Introduction-to-Gaussian-Process" class="headerlink" title="Introduction to Gaussian Process"></a>Introduction to Gaussian Process</h2><p>In this post, I will discuss about Gaussian Process, which employs Gaussian distribution (also often called normal distribution that plays an important role in statistics. The previous posts, such as CLT, Cholesky Decomposition, and Schur Complement, are posted because they are needed to understand the Gaussian Process. </p>
<p>Before I start with Gaussian Process, some important properties of Gaussian Distribution will be addressed. </p>
<h3 id="large-Gaussian-large-Distribution"><a href="#large-Gaussian-large-Distribution" class="headerlink" title="$\large{Gaussian}$ $\large{ Distribution}$"></a>$\large{Gaussian}$ $\large{ Distribution}$</h3><p>If we have random vector $X$ following Gaussian Distribution, then we have </p>
<p>$$X = \begin{bmatrix} X_1 \\ X_2 \\ … \\ X_n \end{bmatrix} \sim \mathcal{N}(\mu, \Sigma)$$</p>
<p>where $\Sigma = Cov(X_i, X_j) = E[(X_i - \mu_i)(X_j - \mu_j)^{T}]$</p>
<p>Now, Suppose we have two vectors, $X$ and $Y$ that are subsets of a random variable, Z, with the following; </p>
<p>$$P_Z = P_{X,Y} = \begin{bmatrix} X \\ Y \end{bmatrix} \sim \mathcal{N}(\mu, \Sigma) = \mathcal{N}(\begin{bmatrix} \mu_X \\ \mu_Y \end{bmatrix}, \begin{bmatrix} \Sigma_{XX} &amp; \Sigma_{XY} \\ \Sigma_{YX} &amp; \Sigma_{YY} \end{bmatrix})$$</p>
<p>Then, the following properties holds; </p>
<h3 id="1-Normalization"><a href="#1-Normalization" class="headerlink" title="1. Normalization"></a>1. Normalization</h3><p>The density function will be </p>
<p>$$\int_{Z} p(Z;\mu,\Sigma) dz = 1$$</p>
<p>where $Z = \begin{bmatrix} X \\ Y \end{bmatrix}$</p>
<h3 id="2-Marginalization"><a href="#2-Marginalization" class="headerlink" title="2. Marginalization"></a>2. Marginalization</h3><p>The maginal densities will be</p>
<p>$$p(X) = \int_{Y} p(X,Y; \mu, \Sigma) dy \\ p(Y) = \int_{X} p(X,Y; \mu, \Sigma) dx$$</p>
<p>which are Gaussian, therefore</p>
<p>$$X \sim \mathcal{N}(\mu_X, \Sigma_{XX}) \\ Y \sim \mathcal{N}(\mu_Y, \Sigma_{YY})$$</p>
<h3 id="3-Conditioning"><a href="#3-Conditioning" class="headerlink" title="3. Conditioning"></a>3. Conditioning</h3><p>The conditional densities will also be Gaussian, and the conditional expected value and covariance are calculated with the properties of Schur Complement as the following. </p>
<p>Refer to the wiki, <a href="https://en.wikipedia.org/wiki/Schur_complement" target="_blank" rel="noopener">Schur Complement</a>. </p>
<p>The conditional expectation and covariance of X given Y is the Schur Complement of C in $\Sigma$ where if $\Sigma$ is defined as </p>
<p>$$\Sigma = \begin{bmatrix} A &amp; B \\ B^{T} &amp; C \end{bmatrix}$$ </p>
<p>$$Cov(X|Y) = A-BC^{-1}B^{T} \\ E(X|Y) = E(X) + BC^{-1}(Y-E(Y))$$</p>
<p>Hence, the conditional covariance of X given Y will be</p>
<p>$$P_{X,Y} = \mathcal{N}(\begin{bmatrix} \mu_X \\ \mu_Y \end{bmatrix}, \begin{bmatrix} \Sigma_{XX} &amp; \Sigma_{XY} \\ \Sigma_{YX} &amp; \Sigma_{YY} \end{bmatrix})$$</p>
<p>$$X | Y \sim \mathcal{N}(\mu_X + \Sigma_{XY}\Sigma_{YY}^{-1}(Y-\mu_Y), \Sigma_{XX} - \Sigma_{XY}\Sigma_{YY}^{-1}\Sigma_{YX}) \\<br>Y | X \sim \mathcal{N}(\mu_Y + \Sigma_{YX}\Sigma_{XX}^{-1}(X-\mu_X), \Sigma_{YY} - \Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY})$$</p>
<h3 id="4-Summation"><a href="#4-Summation" class="headerlink" title="4. Summation"></a>4. Summation</h3><p>The sum of independent Gaussian random variables is also Gaussian;</p>
<p>Let $y \sim \mathcal{N}(\mu, \Sigma)$ and $z \sim \mathcal{N}(\mu’,\Sigma’)$</p>
<p>Then, </p>
<p>$$y + z \sim \mathcal{N}(\mu + \mu’, \Sigma+\Sigma’)$$</p>
<p>We see some of important properties of Gaussian distribution. </p>
<p>Now, let’s change our focus to linear regression problem. </p>
<p>Typical linear regression equation is defined as, </p>
<p>$$y = ax + b$$</p>
<p>Or, </p>
<p>$$y = \beta_0 + \beta_1X_1 + … + \beta_pX_p + \epsilon$$</p>
<p>And, it can be expressed by</p>
<p>$$y = f(x) + \epsilon$$</p>
<p>where $f(x) = \beta_0 + \beta_1X_1 + … + \beta_pX_p$. </p>
<p>Here, an important assumption of linear regression is that the error term, $\epsilon$ is normally distributed with mean zero.  </p>
<p>If we consider repeated sampling from our population, for large sample sizes, the distribution of the ordinary least squared estimates of the regression coefficients follow a normal distribution by the Central Limit Theorem. </p>
<p>Refer to the previous post, <a href="https://davidkwon91.github.io/2019/09/09/CLT/" target="_blank" rel="noopener">Central Limit Theorem</a></p>
<p>Therefore, the $\epsilon$ follows Gaussian Distribution. </p>
<p>We might then think about what if our $f(x)$ follows Gaussian distribution. </p>
<p>Then, in the above equation, $y = f(x) + \epsilon$ will follow Gaussian distribution by the property that the sum of two independent Gaussian distribution is Gaussian distribution. </p>
<p>Here is the start of the Gaussian Process. </p>
<h2 id="large-Gaussian-large-Process"><a href="#large-Gaussian-large-Process" class="headerlink" title="$\large{Gaussian}$ $\large{ Process}$"></a>$\large{Gaussian}$ $\large{ Process}$</h2><p>A Gaussian Process is one of stochastic process that is a collection of random variables, ${f(x) : x \in \mathcal{X}}$, indexed by elements from some set $\mathcal{X}$, known as the index set, and GP is such stochastic process that any finite subcollection of random variables has a multivariate Gaussian distribution. </p>
<p>Here, we assume the $f(x)$ follows Gaussian distribution with some $\mu$ and $\Sigma$. </p>
<p>The $\mu$ and $\Sigma$ are called as “mean function” and “covariance function” with the notation, </p>
<p>$$f(.) \sim \mathcal{GP}(m(.), k(.,.))$$</p>
<p>where $k(.,.)$ is covariance function and a kernel function. </p>
<p>Hence, we have the equation, </p>
<p>$$y^{i} = f(x^{i}) + \epsilon^{i}$$</p>
<p>where the error term (“noise”), $\epsilon^{i}$ with independent $\mathcal{N}(0, \sigma^2)$ distribution. </p>
<p>We assume the prior distribution over the function $f(.)$ with mean zero Gaussian; </p>
<p>$$f(.) \sim \mathcal{GP}(0, k(.,.))$$</p>
<p>for some valid kernel function $k(.,.)$. </p>
<p>Now, we can convert the $\mathcal{GP}$ prior $f(.)$ into a $\mathcal{GP}$ posterior function after having some data to make predictions $f_p$. </p>
<p>Then, the two functions for the training ($f(.)$) and test ($f_p(.)$), will follows</p>
<p>$$\begin{bmatrix} f \\ f_p \end{bmatrix}|X,X_p \sim \mathcal{N}\big(0, \begin{bmatrix} K(X,X) &amp; K(X, X_p)  \\  K(X_p, X) &amp; K(X_p, X_p) \end{bmatrix}\big)$$</p>
<p>Now, with $\epsilon$ and $\epsilon_p$, </p>
<p>$$\begin{bmatrix} \epsilon \\ \epsilon_p\end{bmatrix} \sim \mathcal{N}(0,\begin{bmatrix} \sigma^2I &amp; 0 \\ 0^{T} &amp; \sigma^2I\end{bmatrix})$$</p>
<p>As mentioned above, the sum of two independent Gaussian distribution is also Gaussian, then eventually we have,</p>
<p>$$\begin{bmatrix} y \\ y_p \end{bmatrix} | X, X_p = \begin{bmatrix} f \\ f_p \end{bmatrix} + \begin{bmatrix} \epsilon \\ \epsilon_p\end{bmatrix} \sim \mathcal{N}(0, \begin{bmatrix} K(X,X) + \sigma^2I &amp; K(X,X_p) \\ K(X_p, X) &amp; K(X_p,X_p)+\sigma^2I \end{bmatrix})$$</p>
<p>Here, we generate the prior distributions, which are multivariate normal distributions, with standard multivariate normal distribution and the Cholesky factor as the following. </p>
<p>Refer to my previous post for Cholesky Decomposition, <a href="https://davidkwon91.github.io/tags/CholeskyDecomposition/" target="_blank" rel="noopener">Cholesky Decomposition post</a>. </p>
<p>1) Compute covariance function, $\Sigma$, and Cholesky Decomposition for $\Sigma = LL^{T}$</p>
<p>2) Generate standard multivariate normal distribution, $u \sim \mathcal{N}(0,1)$</p>
<p>3) Compute $x = \mu + Lu$, then $x$ will the prior distribution for $\mathcal{GP}$.  </p>
<p>Back to our prediction with GP, we have </p>
<p>$$y_p|y,X,X_p \sim \mathcal{N}(\mu_p, \Sigma_p)$$ </p>
<p>by the rules for conditioning Gaussians as we have done above. </p>
<p>Then, we have </p>
<p>$$\mu_p = K(X_p, X)K((X,X)+\sigma^2I)^{-1}y \\ \Sigma_p = K(X_p, X_p) + \sigma^2I - K(X_p,X)(K(X,X)+\sigma^2I)^{-1}K(X,X_p)$$</p>
<p>Here, the conditional mean and covariance function are calculated by the Schur Complement as shown above, and since the mean is zero vector, the mean function is defined as the above. </p>
<p>Conditional expectation and covariance with Schur Complement again, </p>
<p>$$\Sigma = \begin{bmatrix} A &amp; B \\ B^{T} &amp; C \end{bmatrix}$$ </p>
<p>$$Cov(X|Y) = A-BC^{-1}B^{T} \\ E(X|Y) = E(X) + BC^{-1}(Y-E(Y))$$</p>
<p>Here, the $E(X)$ and $E(Y) = 0$ for $\mathcal{GP}$, thus we have just $BC^{-1}Y$ term, which is $\mu_p = K(X_p, X)K((X,X)+\sigma^2I)^{-1}y$</p>
<h3 id="Kernel-Function-K"><a href="#Kernel-Function-K" class="headerlink" title="Kernel Function, $K(.,.)$?"></a>Kernel Function, $K(.,.)$?</h3><p>Commonly used kernel function for $\mathcal{GP}$ is <em>squared exponential kernel function</em>, which is often called Gaussian kernel. </p>
<p>It’s defined as, </p>
<p>$$K(X,X_p) = exp(-\frac{1}{2\nu} ||X-X_p||^2) = exp(-\frac{1}{2\nu} (X-X_p)^{T}(X-X_p))$$</p>
<p>where the $\nu$ is the free parameter for bandwidth of the function smoothness. </p>
<p>Here is the implementation of Gaussian Process in R. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">xcor &lt;- seq(-5,4.9,by=0.1) #index or coordinate for our data</span><br><span class="line"></span><br><span class="line">#Kernel Function (Squared exponential kernel function or Gaussian kernel function)</span><br><span class="line">GP.kernel &lt;- function(x1,x2,l)&#123;</span><br><span class="line">  mat &lt;- matrix(rep(0, length(x1)*length(x2)), nrow=length(x1))</span><br><span class="line">  for (i in 1:length(x1)) &#123;</span><br><span class="line">    for (j in 1:length(x2)) &#123;</span><br><span class="line">      mat[i,j] &lt;- (x1[i]-x2[j])^2</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  kern &lt;- exp(-(0.5*mat)/l) #l is free parameter that can be seen as bandwidth</span><br><span class="line">  return(kern)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Zero-mean and covariance function by the squared exponential kernel function with the index to create prior distribution</span><br><span class="line">mu &lt;- rep(0,length(xcor))</span><br><span class="line">cov &lt;- GP.kernel(xcor,xcor,1) #nu is 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#3 prior distributions</span><br><span class="line">dat &lt;- data.frame(x=xcor, first=mvrnorm(1,mu,cov), sec=mvrnorm(1,mu,cov), thi=mvrnorm(1,mu,cov))</span><br><span class="line">#mvrnorm function is same with the using of Cholesky factor of the covariance function multiplied by the standard multivariate normal distribution added mu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#graph for the 3 priors</span><br><span class="line">dat %&gt;% ggplot() + </span><br><span class="line">  geom_line(aes(x=xcor,y=first, colour=&quot;red&quot;)) + </span><br><span class="line">  geom_line(aes(x=xcor,y=sec, colour=&quot;blue&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=thi, colour=&quot;green&quot;))</span><br></pre></td></tr></table></figure>

<img src="/2019/09/15/GaussianProcess/GP-1.png" title="[GP]">


<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#Assume we have a data</span><br><span class="line"></span><br><span class="line">#5 elements</span><br><span class="line">f &lt;- data.frame(x=c(-4,-3,-1,0,2),</span><br><span class="line">                y=c(-2,0,1,2,-1))</span><br><span class="line">x &lt;- f$x</span><br><span class="line">k.xx &lt;- GP.kernel(x,x,1)</span><br><span class="line">k.xxs &lt;- GP.kernel(x,xcor,1)</span><br><span class="line">k.xsxs &lt;- GP.kernel(xcor,xcor,1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f.star.bar &lt;- t(k.xxs)%*%solve(k.xx)%*%f$y #mean function</span><br><span class="line">cov.f.star &lt;- k.xsxs - t(k.xxs)%*%solve(k.xx)%*%k.xxs #covariance function</span><br><span class="line"></span><br><span class="line">#posterior distribution</span><br><span class="line">dat1 &lt;- data.frame(x=xcor, </span><br><span class="line">                  first=mvrnorm(1,f.star.bar,cov.f.star), </span><br><span class="line">                  sec=mvrnorm(1,f.star.bar,cov.f.star), </span><br><span class="line">                  thi=mvrnorm(1,f.star.bar,cov.f.star))</span><br><span class="line"></span><br><span class="line">#graph for 3 posterior distribution when nu is 1</span><br><span class="line">dat1 %&gt;% ggplot() + </span><br><span class="line">  geom_line(aes(x=xcor,y=first, colour=&quot;red&quot;)) + </span><br><span class="line">  geom_line(aes(x=xcor,y=sec, colour=&quot;blue&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=thi, colour=&quot;green&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=f.star.bar),colour=&quot;black&quot;)+</span><br><span class="line">  geom_errorbar(data=f, aes(x=x, y=NULL, ymin=y-2*0.1, ymax=y+2*0.1),width=0.2)+</span><br><span class="line">  geom_point(data=f,aes(x=x,y=y)) +</span><br><span class="line">  ggtitle(&quot;Posterior from Prior x likelihood/evidence (given x,x*,y)&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/15/GaussianProcess/GP-2.png" title="[GP]">



<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mu &lt;- rep(0,length(xcor))</span><br><span class="line">cov &lt;- GP.kernel(xcor,xcor,0.1) #nu is 0.1</span><br><span class="line"></span><br><span class="line">dat &lt;- data.frame(x=xcor, first=mvrnorm(1,mu,cov), sec=mvrnorm(1,mu,cov), thi=mvrnorm(1,mu,cov))</span><br><span class="line"></span><br><span class="line">dat %&gt;% ggplot() + </span><br><span class="line">  geom_line(aes(x=xcor,y=first, colour=&quot;red&quot;)) + </span><br><span class="line">  geom_line(aes(x=xcor,y=sec, colour=&quot;blue&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=thi, colour=&quot;green&quot;)) +</span><br><span class="line">  ggtitle(&quot;When sigma is 0.1&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mu &lt;- rep(0,length(xcor))</span><br><span class="line">cov &lt;- GP.kernel(xcor,xcor,10) #nu is 10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dat &lt;- data.frame(x=xcor, first=mvrnorm(1,mu,cov), sec=mvrnorm(1,mu,cov), thi=mvrnorm(1,mu,cov))</span><br><span class="line"></span><br><span class="line">dat %&gt;% ggplot() + </span><br><span class="line">  geom_line(aes(x=xcor,y=first, colour=&quot;red&quot;)) + </span><br><span class="line">  geom_line(aes(x=xcor,y=sec, colour=&quot;blue&quot;)) +</span><br><span class="line">  geom_line(aes(x=xcor,y=thi, colour=&quot;green&quot;)) +</span><br><span class="line">  ggtitle(&quot;When sigma is 10&quot;)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/15/GaussianProcess/GP-3.png" title="[GP]">
<img src="/2019/09/15/GaussianProcess/GP-5.png" title="[GP]">









<p>Reference: </p>
<p><a href="http://krasserm.github.io/2018/03/19/gaussian-processes/" target="_blank" rel="noopener">Gaussian Process</a><br><a href="http://i-systems.github.io/HSE545/machine%20learning%20all/14%20Gaussian%20Process%20Regression/reference_files/cs229-gaussian_processes.pdf" target="_blank" rel="noopener">Gaussian Process</a><br><a href="https://distill.pub/2019/visual-exploration-gaussian-processes/#DimensionSwap" target="_blank" rel="noopener">A Visual Exploration of Gaussian Process</a><br><a href="http://www0.cs.ucl.ac.uk/staff/J.Shawe-Taylor/courses/ATML-1.pdf" target="_blank" rel="noopener">Gaussian Process: A Basic Properties and GP regression</a><br><a href="https://www.r-bloggers.com/gaussian-process-regression-with-r/" target="_blank" rel="noopener">Gaussian Process regression with R</a></p>
<p>and my previous posts,</p>
<p><a href="https://davidkwon91.github.io/tags/CholeskyDecomposition/" target="_blank" rel="noopener">Cholesky Decomposition</a><br><a href="https://davidkwon91.github.io/tags/CentralLimitTheorem/" target="_blank" rel="noopener">Central Limit Theorem</a><br><a href="https://davidkwon91.github.io/tags/CentralLimitTheorem/" target="_blank" rel="noopener">Schur Complement</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/09/09/hist/" class="image is-7by1">
            <img class="thumbnail" src="/images/Histogram.png" alt="Histogram">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-09T09:49:56.000Z">2019-09-09</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/Math/">Math</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/Math/Histogram/">Histogram</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    2 minutes read (About 331 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/09/hist/">Histogram</a>
            
        </h1>
        <div class="content">
            <h2 id="Creating-Histogram-function-using-Min-max-transformation"><a href="#Creating-Histogram-function-using-Min-max-transformation" class="headerlink" title="Creating Histogram function using Min-max transformation."></a>Creating Histogram function using Min-max transformation.</h2><p>The histogram plot of a vector or a data feature is to create bins, which is to create a series of interval, for the range of data values, and to count how many data values fall into each bins. </p>
<p>I create bins as the following;</p>
<p>Suppose we have $M$ bins, then</p>
<p>$$B_1 = [0,\frac{1}{M}), B_2 = [\frac{1}{M}, \frac{2}{M}), …, B_{M-1} = [\frac{M-2}{M}, \frac{M-1}{M}), B_{M} = [\frac{M-1}{M}, 1)$$</p>
<p>To create histogram function with the bins, I wanted to transform the data elements in interval $(0,1)$, so I can put them into each bins. </p>
<p>That’s why I used Min-max transformation, which makes the data reducing to a scale between 0 and 1. </p>
<p>Min-max transformation is the following formula;</p>
<p>$$z = \frac{x-min(x)}{max(x)-min(x)}$$</p>
<p>The below codes are the implementation of creating histogram plot in R. I used the values from CLT posts. </p>
<p><a href="https://davidkwon91.github.io/2019/09/09/CLT/" target="_blank" rel="noopener">CLT link</a></p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Hist &lt;- function(vec,bin)&#123;</span><br><span class="line">  vec.minmax &lt;- (vec - min(vec))/(max(vec)-min(vec)) #min-max transformation of the vector or the values</span><br><span class="line">  </span><br><span class="line">  vec.bins &lt;- rep(0,bin)</span><br><span class="line">  for(i in 1:bin)&#123;</span><br><span class="line">  	#find the values that is the closest to the value of each boundary of the bins</span><br><span class="line">    vec.bins[i] &lt;- vec[which(abs(vec.minmax - i/bin) == min(abs(vec.minmax - i/bin)))]</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  dat &lt;- data.frame(x=vec.bins, freq=0)</span><br><span class="line">  for(i in 1:bin)&#123;</span><br><span class="line">  	#put the values into each bins associateed</span><br><span class="line">    dat[i,2] &lt;- length(which(vec.minmax &gt; (i-1)/bin &amp; vec.minmax &lt;= i/bin))</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  #plotting</span><br><span class="line">  p &lt;- dat %&gt;% ggplot(aes(x=x, y=freq)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge(width=0.5)) + theme_bw()</span><br><span class="line"></span><br><span class="line">  return(p)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hist(sampled.1000,15)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/hist/histrogram-1.png" title="[hist]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hist(sampled.10000,15)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/hist/histrogram-2.png" title="[hist]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hist(bin.sampled.10000,15)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/hist/histrogram-3.png" title="[hist]">






<p>Reference:<br><a href="http://faculty.washington.edu/yenchic/18W_425/Lec6_hist_KDE.pdf" target="_blank" rel="noopener">Histogram and Kernel Density EStimation</a><br><a href="https://medium.com/@shirleyliu/histograms-from-scratch-482dba2a4e31" target="_blank" rel="noopener">Histogram from Scratch</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/09/09/CLT/" class="image is-7by1">
            <img class="thumbnail" src="/images/CLT.png" alt="CLT">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-09T07:08:11.000Z">2019-09-09</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/Math/">Math</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/Math/CLT/">CLT</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    4 minutes read (About 591 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/09/CLT/">CLT</a>
            
        </h1>
        <div class="content">
            <p>This post will discuss Central Limit Theorem.<br>Central Limit Theorem is one of the most important topic in statistics, especially in probability theory. </p>
<h3 id="large-Definition"><a href="#large-Definition" class="headerlink" title="$\large{Definition}$:"></a>$\large{Definition}$:</h3><p>The sample average of independent and identically distributed random variables drawn from an unknown distribution with mean $\mu$ and variance $\sigma$ is approximately normal distributed when $n$ gets larger. That is, by the law of large numbers, the sample mean converges in probability and almost surely to the expected value $\mu$ as $n \to \infty$.</p>
<p>Formally, let ${X_1,X_2,…X_n}$ be random samples of size $n$. Then, the sample average is defined as</p>
<p>$$S_n = \frac{X_1 + X_2 + … + X_n}{n}$$ with mean $\mu$ and variance $\sigma^2$</p>
<p>Then,<br>$$\sqrt{n}(S_n-\mu) \sim \mathcal{N}(0,\sigma^2)$$</p>
<p>The normalized random mean variable will be</p>
<p>$$Z_n = \frac{S_n-\mu}{\frac{\sigma}{\sqrt{n}}} \sim \mathcal{N}(0,1)$$</p>
<p>The below part will be implementation of CLT in R. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set.seed(1004)</span><br><span class="line">x1 &lt;- runif(10000, min=0,max=1000) #Generating random variables drawn from unifrom distribution with (0,1000)</span><br><span class="line"></span><br><span class="line">hist(x1) #Histogram</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/CLT/CLT-1.png" title="[hist]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sampled.5 &lt;- rep(0, length(x1))</span><br><span class="line"></span><br><span class="line">for(i in 1:length(x1))&#123;</span><br><span class="line">  sampled.5[i] &lt;- mean(sample(x1, 5, replace=TRUE)) #sample average with size 5</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">hist(sampled.5)</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/CLT/CLT-3.png" title="[hist]">


<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">plot(density(sampled.5)) #density plot of sample average variables</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/CLT/CLT-4.png" title="[hist]">


<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sampled.30 &lt;- rep(0, length(x1))</span><br><span class="line">sampled.1000 &lt;- rep(0,length(x1))</span><br><span class="line">sampled.10000 &lt;- rep(0,length(x1))</span><br><span class="line"></span><br><span class="line">for(i in 1:length(x1))&#123;</span><br><span class="line">  sampled.30[i] &lt;- mean(sample(x1, 30, replace=TRUE)) #sample average of size 30</span><br><span class="line">  sampled.1000[i] &lt;- mean(sample(x1, 1000, replace=TRUE)) #sample average of size 1000</span><br><span class="line">  sampled.10000[i] &lt;- mean(sample(x1,10000,replace=TRUE)) #sample average of size 10000</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>$$\sqrt{n}(S_n-\mu) \sim \mathcal{N}(0,\sigma^2)$$</p>
<p>The following code is the above formula. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqrt(length(sampled.30))*(mean(sampled.30)-mean(x1))</span><br><span class="line"></span><br><span class="line">## [1] -39.36105</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqrt(length(sampled.1000))*(mean(sampled.1000)-mean(x1))</span><br><span class="line"></span><br><span class="line">## [1] 0.1330039</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqrt(length(sampled.10000))*(mean(sampled.10000)-mean(x1))</span><br><span class="line"></span><br><span class="line">## [1] 0.003917203</span><br></pre></td></tr></table></figure>

<p>It shows the $\sqrt{n}(S_n-\mu)$ tends to go to zero as the size of the sample increases, which means that the expected value of the sample average variables gets closer to the expected value of the random variables when $n$ gets larger. </p>
<p>Let’s see other example for random sample average drawn from Binomial distribution. </p>
<p>Suppose we have 10000 random Binomial variables with $p = 0.5$. </p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#Binomial</span><br><span class="line">n &lt;- 10000</span><br><span class="line">p &lt;- 1/2</span><br><span class="line">B &lt;- rbinom(n,1,p)</span><br></pre></td></tr></table></figure>

<p>The mean of Binomial distribution is $p$, that is $E(X) = p$.<br>The variance of Binomial distribution is $p(1-p)$, that is $Var(X) = p(1-p)$</p>
<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">p</span><br><span class="line"></span><br><span class="line">## [1] 0.5</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">p*(1-p)</span><br><span class="line"></span><br><span class="line">## [1] 0.25</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mean(B)</span><br><span class="line"></span><br><span class="line">## [1] 0.4991</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var(B)</span><br><span class="line"></span><br><span class="line">## [1] 0.2500242</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#creating random sample average from Binomial random variables</span><br><span class="line">bin.sampled.30 &lt;- rep(0, length(B))</span><br><span class="line">bin.sampled.1000 &lt;- rep(0,length(B))</span><br><span class="line">bin.sampled.10000 &lt;- rep(0,length(B))</span><br><span class="line"></span><br><span class="line">for(i in 1:length(x1))&#123;</span><br><span class="line">  bin.sampled.30[i] &lt;- mean(sample(B, 30, replace=TRUE))</span><br><span class="line">  bin.sampled.1000[i] &lt;- mean(sample(B, 1000, replace=TRUE))</span><br><span class="line">  bin.sampled.10000[i] &lt;- mean(sample(B,10000,replace=TRUE))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(density(bin.sampled.30))</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/CLT/CLT-14.png" title="[hist]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(density(bin.sampled.1000))</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/CLT/CLT-15.png" title="[hist]">

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(density(bin.sampled.10000))</span><br></pre></td></tr></table></figure>

<img src="/2019/09/09/CLT/CLT-16.png" title="[hist]">



<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqrt(length(bin.sampled.30))*(mean(bin.sampled.30)-mean(B))</span><br><span class="line"></span><br><span class="line">## [1] 0.009333333</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqrt(length(bin.sampled.1000))*(mean(bin.sampled.1000)-mean(B))</span><br><span class="line"></span><br><span class="line">## [1] 0.01169</span><br></pre></td></tr></table></figure>

<figure class="highlight plain hljs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqrt(length(bin.sampled.10000))*(mean(bin.sampled.10000)-mean(B))</span><br><span class="line"></span><br><span class="line">## [1] -0.000371</span><br></pre></td></tr></table></figure>

<p>These results shows that no matter what the distribution of the population is, the sample average drawn from the distribution will be approximately normal distribution as $n$ gets large with mean $\mu$.</p>
<p>Reference:<br><a href="https://www.probabilitycourse.com/chapter7/7_1_2_central_limit_theorem.php" target="_blank" rel="noopener">Central Limit Theorem</a><br><a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank" rel="noopener">Central Limit Theorem (wiki)</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/09/06/Cholesky/" class="image is-7by1">
            <img class="thumbnail" src="/images/CholeskyDecomposition.jpg" alt="Cholesky Decomposition">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-06T10:18:13.000Z">2019-09-06</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/Math/">Math</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/Math/Cholesky-Decomposition/">Cholesky Decomposition</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    3 minutes read (About 416 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/06/Cholesky/">Cholesky Decomposition</a>
            
        </h1>
        <div class="content">
            <h1 id="Introduction-to-the-Cholesky-Decomposition"><a href="#Introduction-to-the-Cholesky-Decomposition" class="headerlink" title="Introduction to the Cholesky Decomposition."></a>Introduction to the Cholesky Decomposition.</h1><p>Cholesky Decomposition is used for its superior efficiency in linear calculation, such as affine function, $Ax = b$. Among many applications of Cholesky Decomposition, I will discuss about how the multivariate normal random numbers are generated with Cholesky Decomposition. </p>
<h2 id="large-Definition"><a href="#large-Definition" class="headerlink" title="$\large{Definition}$:"></a>$\large{Definition}$:</h2><p>Every positive definite matrix $A \in \mathcal{R}^{n \times n}$, </p>
<p>The Cholesky Decomposition is a form of </p>
<p>$$A = LL^{T}$$</p>
<p>where $L$ is the lower triangular matrix with real and positive diagonal elements. $L$ is called Cholesky factor of $A$ and it can be interpreted as the square root of a positive definite matrix. </p>
<h2 id="large-Algorithm"><a href="#large-Algorithm" class="headerlink" title="$\large{Algorithm}$:"></a>$\large{Algorithm}$:</h2><p>This algorithm is used in this <a href="https://aaronschlegel.me/cholesky-decomposition-r-example.html" target="_blank" rel="noopener">link</a>, and there are many apporaches to decompose a matrix with Cholesky Decomposition. </p>
<ol>
<li>Compute $L_{1} = \sqrt{a_{11}}$</li>
<li>For $k = 2, … ,n$, find $L_{k-1}l_{k} = a_{k} for l_{k}$</li>
<li>$l_{kk} = \sqrt{a_{kk}-l_{k}^{T}l_{k}}$</li>
<li>$L_{k} = \begin{bmatrix} L_{k-1} &amp; 0 \\ l_{k}^{T} &amp; l_{kk} \end{bmatrix}$</li>
</ol>
<p>then $L_{k}$ is the lower triangular matrix of Cholesky Decomposition. </p>
<p>Other approaches; </p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Proof_for_positive_semi-definite_matrices" target="_blank" rel="noopener">Cholesky Decomposition</a></li>
<li><a href="http://www.seas.ucla.edu/~vandenbe/133A/lectures/chol.pdf" target="_blank" rel="noopener">link</a></li>
</ul>
<p>$\large{Generating}$ $\large{Multivariate}$ $\large{Normal}$ $\large{Random}$ $\large{Number}$ $\large{with}$ $\large{Cholesky}$ $\large{Decomposition}$</p>
<p>If we have $X$ that follows Normal Distribution, </p>
<p>then</p>
<p>$$X \sim \mathcal{N}(\mu, \Sigma)$$</p>
<p>Let $Z$ follows standard normal distribution with mean 0 and variance 1.</p>
<p>Then,</p>
<p>$$Z \sim \mathcal{N}(0,I)$$</p>
<p>Then, we can express the $X$ with $Z$ as the following; </p>
<p>$$X = A+BZ$$</p>
<p>then $X$ will follow normal distribution as the following; </p>
<p>$$X \sim \mathcal{N}(A,BB’)$$</p>
<p>Here, $\mu$ is the $A$, and $\Sigma$ is the $BB’$. The $B$ is the lower triangular matrix of the decomposed $X$, which is the Cholesky factor. </p>
<p>Therefore, If we have a Cholesky factor, $B$, of covariance matrix of $X$, then the product of $B$ and standard normal random number matrix will generate the multivariate normal random number associated with the mean $\mu$ and covariance $\Sigma$ of $X$. </p>
<p>Reference:</p>
<p><a href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Proof_for_positive_semi-definite_matrices" target="_blank" rel="noopener">Cholesky Decomposition</a><br><a href="http://www.seas.ucla.edu/~vandenbe/133A/lectures/chol.pdf" target="_blank" rel="noopener">Cholesky Factorization</a><br><a href="https://aaronschlegel.me/cholesky-decomposition-r-example.html" target="_blank" rel="noopener">Cholesky Decomposition with R example</a><br><a href="http://rinterested.github.io/statistics/multivariate_normal_draws.html" target="_blank" rel="noopener">link</a><br><a href="https://www2.stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec22.pdf" target="_blank" rel="noopener">Bivariate Normal Distribution</a></p>

        </div>
        
        
        
    </div>
</div>








    <div class="card">
    
    <div class="card-image">
        <a href="/2019/09/03/Matrix Basic/" class="image is-7by1">
            <img class="thumbnail" src="/images/SchurComplement.png" alt="Schur Complement">
        </a>
    </div>
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2019-09-03T03:03:56.000Z">2019-09-03</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/Math/">Math</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/Math/Schur-Complement/">Schur Complement</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    4 minutes read (About 651 words)
                </span>
                
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                <a class="has-link-black-ter" href="/2019/09/03/Matrix Basic/">Schur Complement</a>
            
        </h1>
        <div class="content">
            <p>Brief introduction to Schur Complement. </p>
<p>This is just a brief introduction of the properties of Schur Complement. Schur Complement has many applications in numerical analysis, such as optimization, machine learning algorithms, or probability and matrix theories. </p>
<p>Let’s begin with the definition of the Schur Complement. </p>
<p>$\large{Definition}$: </p>
<p>The Schur Complement of a black matrix is defined as;</p>
<p>Let $M$ be $n \times n$ matrix</p>
<p>$$M =  \begin{bmatrix} A &amp; B \\ C &amp; D \end{bmatrix}$$</p>
<p>Then, the Schur Complement of the block $D$ of the matrix $M$ is </p>
<p>$$M/D := A - BD^{-1}C$$</p>
<p>if $D$ is invertible. </p>
<p>The Schur Complement of the block $A$ of the matrix $M$ is then,</p>
<p>$$M/A := D - CA^{-1}B$$</p>
<p>if $A$ is invertible. </p>
<p>$$\large{Properties}$$</p>
<ul>
<li>Linear system</li>
</ul>
<p>Let<br>$A$ is a $p \times p$ matrix,<br>$D$ is a $q \times q$ matrix, with $n = p + q$<br>So, $B$ is a $p \times q$ matrix. </p>
<p>Let $M$ be defined as,</p>
<p>$$M = \begin{bmatrix} A &amp; B \\ B^{T} &amp; D \end{bmatrix}$$</p>
<p>Suppose we have a linear system as,</p>
<p>$$Ax + By = c \\ B^{T}x + Dy = d$$</p>
<p>Then, </p>
<p>$$\begin{bmatrix} A &amp; B \\ B^{T} &amp; D \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} c \\ d \end{bmatrix}$$</p>
<p>Assuming $D$ is invertible, </p>
<p>then we have $y$ as</p>
<p>$$y = D^{-1}(d - B^{T}x)$$ </p>
<p>Then, from the equation, </p>
<p>$$Ax + By = c \\ Ax + B(D^{-1}(d - B^{T}x)) = c$$</p>
<p>Then, we see that is,</p>
<p>$$(A - BD^{-1}B^{T})x = c - BD^{-1}d$$</p>
<p>Here, the $A - BD^{-1}B^{T}$ is the Schur Complement of a block $D$ of matrix $M$. </p>
<p>It follows, </p>
<p>$$x = (A-BD^{-1}B^{T})^{-1}(c-BD^{-1}d) \\ y = D^{-1}(d-B^{T}(A-BD^{-1}B^{T})^{-1}(c-BD^{-1}d))$$</p>
<p>And, </p>
<p>$$x = (A-BD^{-1}B^{T})^{-1}c - (A-BD^{-1}B^{T})^{-1}BD^{-1}d \\ y = -D^{-1}B^{T}(A-BD^{-1}B^{T})^{-1}c + (D^{-1}+D^{-1}B^{T}(A-BD^{-1}B^{T})^{-1}BD^{-1})d$$</p>
<p>The $x$ and $y$ are formed as linear functions associated with $c$ and $d$, </p>
<p>and it can be a formula for an inverse of $M$ in terms of the Schur Complement of $D$ in $M$.</p>
<p>$$\begin{bmatrix} A &amp; B \\ B^{T} &amp; D \end{bmatrix} ^{-1} = \begin{bmatrix} (A-BD^{-1}B^{T})^{-1} &amp; -(A-BD^{-1}B^{T})^{-1}BD^{-1} \\ -D^{-1}B^{T}(A-BD^{-1}B^{T})^{-1} &amp; D^{-1}+D^{-1}B^{T}(A-BD^{-1}B^{T})^{-1}BD^{-1}  \end{bmatrix}$$</p>
<p>This can be written as,</p>
<p>$$\begin{bmatrix} A &amp; B \\ B^{T} &amp; D \end{bmatrix} ^{-1} = \begin{bmatrix} (A-BD^{-1}B^{T})^{-1} &amp; 0 \\ -D^{-1}B^{T}(A-BD^{-1}B^{T})^{-1} &amp; D^{-1} \end{bmatrix} \begin{bmatrix} I &amp; -BD^{-1} \\ 0 &amp; I \end{bmatrix}$$</p>
<p>Eventually, </p>
<p>$$\begin{bmatrix} A &amp; B \\ B^{T} &amp; D \end{bmatrix} ^{-1} = \begin{bmatrix} I &amp; 0 \\ -D^{-1}B^{T} &amp; I \end{bmatrix} \begin{bmatrix} (A-BD^{-1}B^{T})^{-1} &amp; 0 \\ 0 &amp; D^{-1} \end{bmatrix} \begin{bmatrix} I &amp; -BD^{-1} \\ 0 &amp; I \end{bmatrix}$$</p>
<p>If we take inverse on both sides,</p>
<p>$$\begin{bmatrix} A &amp; B \\ B^{T} &amp; D \end{bmatrix} = \begin{bmatrix} I &amp; BD^{-1} \\ 0 &amp; I \end{bmatrix} \begin{bmatrix} A-BD^{-1}B^{T} &amp; 0 \\ 0 &amp; D \end{bmatrix} \begin{bmatrix} I &amp; 0 \\ D^{-1}B^{T} &amp; I \end{bmatrix}$$</p>
<p>$$= \begin{bmatrix} I &amp; BD^{-1} \\ 0 &amp; I \end{bmatrix} \begin{bmatrix} A-BD^{-1}B^{T} &amp; 0 \\ 0 &amp; D \end{bmatrix} \begin{bmatrix} I &amp; BD^{-1} \\ 0 &amp; I \end{bmatrix} ^{T}$$</p>
<p>This is done by an assumption with $D$ is invertible, and this decomposition is called Block LDU Decomposition. </p>
<p>Then, we can get another factorization of $M$ if A is invertible. </p>
<ul>
<li>Positive Semi-definite</li>
</ul>
<p><em>For any symmetric matrix,</em> $M$,</p>
<p>if D is invertible, then</p>
<p>1) $M \succ 0$   $iff$   $D \succ 0$ <em>and</em>  $A-BC^{-1}B^{T} \succ 0$ </p>
<p>2) <em>If</em> $C \succ 0$, <em>then</em> $M \succeq 0$  $iff$  $A-BC^{-1}B^{T} \succeq 0$</p>
<p>We can easily get the matrix $M$ in the linear combination and the inverse of the $M$ with Schur Complement.<br>We also can check if the $M$ is positive semi-definite.</p>
<p>Reference:</p>
<p><a href="https://en.wikipedia.org/wiki/Schur_complement" target="_blank" rel="noopener">Schur Complement</a><br><a href="http://www.cis.upenn.edu/~jean/schur-comp.pdf" target="_blank" rel="noopener">The Schur Complement and Symmetric Positive Semidefinite (and Definite) Matrices</a></p>

        </div>
        
        
        
    </div>
</div>









    
<div class="card card-transparent">
    <nav class="pagination is-centered" role="navigation" aria-label="pagination">
        <div class="pagination-previous is-invisible is-hidden-mobile">
            <a class="is-flex-grow has-text-black-ter" href="/tags/ML/page/0/">Previous</a>
        </div>
        <div class="pagination-next">
            <a class="is-flex-grow has-text-black-ter" href="/tags/ML/page/2/">Next</a>
        </div>
        <ul class="pagination-list is-hidden-mobile">
            
            <li><a class="pagination-link is-current" href="/tags/ML/">1</a></li>
            
            <li><a class="pagination-link has-text-black-ter" href="/tags/ML/page/2/">2</a></li>
            
        </ul>
    </nav>
</div>
</div>
                




<div class="column is-4-tablet is-4-desktop is-4-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                        <img class="image is-128x128 has-mb-6" src="/images/David.png" alt="Yongbock (David) Kwon">
                    
                    
                    <p class="is-size-4 is-block">
                        Yongbock (David) Kwon
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        The trick in life isn&#39;t getting what you want, it&#39;s wanting it after you get it
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>Seoul, Korea</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Posts
                    </p>
                    <p class="title has-text-weight-normal">
                        16
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Categories
                    </p>
                    <p class="title has-text-weight-normal">
                        18
                    </p>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Tags
                    </p>
                    <p class="title has-text-weight-normal">
                        29
                    </p>
                </div>
            </div>
        </nav>
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/DavidKwon91" target="_blank">
                Follow</a>
        </div>
        
        
        <div class="level is-mobile">
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Github" href="https://github.com/DavidKwon91">
                
                <i class="fab fa-github"></i>
                
            </a>
            
            <a class="level-item button is-white is-marginless" target="_blank"
                title="Linkedin" href="https://www.linkedin.com/in/yongbock-david-kwon-7a195994/">
                
                <i class="fab fa-linkedin"></i>
                
            </a>
            
        </div>
        
    </div>
</div>
    
        
    
        


    
        
<div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Categories
            </h3>
            <ul class="menu-list">
            <li>
        <a class="level is-marginless" href="/categories/Language/">
            <span class="level-start">
                <span class="level-item">Language</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/Language/Python-in-R/">
            <span class="level-start">
                <span class="level-item">Python in R</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/ML/">
            <span class="level-start">
                <span class="level-item">ML</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">7</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/ML/Classification/">
            <span class="level-start">
                <span class="level-item">Classification</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Decision-Tree/">
            <span class="level-start">
                <span class="level-item">Decision Tree</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Gaussian-Process/">
            <span class="level-start">
                <span class="level-item">Gaussian Process</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Likelihood/">
            <span class="level-start">
                <span class="level-item">Likelihood</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Logistic-Regression/">
            <span class="level-start">
                <span class="level-item">Logistic Regression</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Monty-Hall/">
            <span class="level-start">
                <span class="level-item">Monty Hall</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/ML/Principal-Components-Analysis/">
            <span class="level-start">
                <span class="level-item">Principal Components Analysis</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/Math/">
            <span class="level-start">
                <span class="level-item">Math</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">4</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/Math/CLT/">
            <span class="level-start">
                <span class="level-item">CLT</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Math/Cholesky-Decomposition/">
            <span class="level-start">
                <span class="level-item">Cholesky Decomposition</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Math/Histogram/">
            <span class="level-start">
                <span class="level-item">Histogram</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Math/Schur-Complement/">
            <span class="level-start">
                <span class="level-item">Schur Complement</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/Today/">
            <span class="level-start">
                <span class="level-item">Today</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">3</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/Today/August/">
            <span class="level-start">
                <span class="level-item">August</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Today/September/">
            <span class="level-start">
                <span class="level-item">September</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">2</span>
            </span>
        </a></li></ul></li>
            </ul>
        </div>
    </div>
</div>
    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Tag Cloud
        </h3>
        <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Bayes-Theorem/" style="font-size: 10px;">Bayes Theorem</a> <a href="/tags/CentralLimitTheorem/" style="font-size: 10px;">CentralLimitTheorem</a> <a href="/tags/CharlesBukowski/" style="font-size: 10px;">CharlesBukowski</a> <a href="/tags/CholeskyDecomposition/" style="font-size: 10px;">CholeskyDecomposition</a> <a href="/tags/Classification/" style="font-size: 10px;">Classification</a> <a href="/tags/Decision-Tree/" style="font-size: 10px;">Decision Tree</a> <a href="/tags/Entropy/" style="font-size: 10px;">Entropy</a> <a href="/tags/GaussianProcess/" style="font-size: 10px;">GaussianProcess</a> <a href="/tags/Gini-Index/" style="font-size: 10px;">Gini Index</a> <a href="/tags/Histogram/" style="font-size: 10px;">Histogram</a> <a href="/tags/Likelihood/" style="font-size: 13.33px;">Likelihood</a> <a href="/tags/LogLikelihood/" style="font-size: 10px;">LogLikelihood</a> <a href="/tags/Logit/" style="font-size: 10px;">Logit</a> <a href="/tags/ML/" style="font-size: 20px;">ML</a> <a href="/tags/Matrix/" style="font-size: 16.67px;">Matrix</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Month-Hall-Problem/" style="font-size: 10px;">Month Hall Problem</a> <a href="/tags/Odds/" style="font-size: 10px;">Odds</a> <a href="/tags/PCA/" style="font-size: 10px;">PCA</a> <a href="/tags/Poem/" style="font-size: 10px;">Poem</a> <a href="/tags/Probability/" style="font-size: 10px;">Probability</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/R/" style="font-size: 10px;">R</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD</a> <a href="/tags/SchurComplement/" style="font-size: 10px;">SchurComplement</a> <a href="/tags/Today/" style="font-size: 13.33px;">Today</a> <a href="/tags/Unsupervised-Learning/" style="font-size: 10px;">Unsupervised Learning</a>
    </div>
</div>

    
        
<div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Recent
        </h3>
        
        <article class="media">
            
            <a href="/2019/10/24/PCA/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/pca.png" alt="PCA">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-10-24T06:08:47.000Z">2019-10-24</time></div>
                    <a href="/2019/10/24/PCA/" class="title has-link-black-ter is-size-6 has-text-weight-normal">PCA</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/ML/">ML</a> / <a class="has-link-grey -link" href="/categories/ML/Principal-Components-Analysis/">Principal Components Analysis</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/10/16/AUROC/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/auroc.png" alt="Classfication">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-10-16T04:04:57.000Z">2019-10-16</time></div>
                    <a href="/2019/10/16/AUROC/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Classfication</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/ML/">ML</a> / <a class="has-link-grey -link" href="/categories/ML/Classification/">Classification</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/10/07/Reticulate/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/RorPython.jpeg" alt="Reticulate">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-10-07T03:00:23.000Z">2019-10-07</time></div>
                    <a href="/2019/10/07/Reticulate/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Reticulate</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/Language/">Language</a> / <a class="has-link-grey -link" href="/categories/Language/Python-in-R/">Python in R</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/10/05/Monty-Hall/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/montyhall.png" alt="Monty Hall">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-10-05T09:53:20.000Z">2019-10-05</time></div>
                    <a href="/2019/10/05/Monty-Hall/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Monty Hall</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/ML/">ML</a> / <a class="has-link-grey -link" href="/categories/ML/Monty-Hall/">Monty Hall</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <a href="/2019/09/19/DecisionTree/" class="media-left">
                <p class="image is-64x64">
                    <img class="thumbnail" src="/images/treeimage.jpg" alt="DecisionTree">
                </p>
            </a>
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2019-09-19T08:13:52.000Z">2019-09-19</time></div>
                    <a href="/2019/09/19/DecisionTree/" class="title has-link-black-ter is-size-6 has-text-weight-normal">DecisionTree</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/ML/">ML</a> / <a class="has-link-grey -link" href="/categories/ML/Decision-Tree/">Decision Tree</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>

    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            Archives
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2019/10/">
                <span class="level-start">
                    <span class="level-item">October 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2019/09/">
                <span class="level-start">
                    <span class="level-item">September 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">8</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2019/08/">
                <span class="level-start">
                    <span class="level-item">August 2019</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">4</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Tags
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/AUC/">
                        <span class="tag">AUC</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Bayes-Theorem/">
                        <span class="tag">Bayes Theorem</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/CentralLimitTheorem/">
                        <span class="tag">CentralLimitTheorem</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/CharlesBukowski/">
                        <span class="tag">CharlesBukowski</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/CholeskyDecomposition/">
                        <span class="tag">CholeskyDecomposition</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Classification/">
                        <span class="tag">Classification</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Decision-Tree/">
                        <span class="tag">Decision Tree</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Entropy/">
                        <span class="tag">Entropy</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/GaussianProcess/">
                        <span class="tag">GaussianProcess</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Gini-Index/">
                        <span class="tag">Gini Index</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Histogram/">
                        <span class="tag">Histogram</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Likelihood/">
                        <span class="tag">Likelihood</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/LogLikelihood/">
                        <span class="tag">LogLikelihood</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Logit/">
                        <span class="tag">Logit</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/ML/">
                        <span class="tag">ML</span>
                        <span class="tag is-grey">12</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Matrix/">
                        <span class="tag">Matrix</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Metric/">
                        <span class="tag">Metric</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Month-Hall-Problem/">
                        <span class="tag">Month Hall Problem</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Odds/">
                        <span class="tag">Odds</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/PCA/">
                        <span class="tag">PCA</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Poem/">
                        <span class="tag">Poem</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Probability/">
                        <span class="tag">Probability</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Python/">
                        <span class="tag">Python</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/R/">
                        <span class="tag">R</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/ROC/">
                        <span class="tag">ROC</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/SVD/">
                        <span class="tag">SVD</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/SchurComplement/">
                        <span class="tag">SchurComplement</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Today/">
                        <span class="tag">Today</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Unsupervised-Learning/">
                        <span class="tag">Unsupervised Learning</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/logo.svg" alt="David Kwon" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2019 David Kwon&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a>
                
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Creative Commons" href="https://creativecommons.org/">
                        
                        <i class="fab fa-creative-commons"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/">
                        
                        <i class="fab fa-creative-commons-by"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("en");</script>

<script>
var IcarusThemeSettings = {
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


    <script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>



    
    
    
    <script src="/js/animation.js"></script>
    

    
    
    
    <script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
    <script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
    <script src="/js/gallery.js" defer></script>
    

    
    

<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


    
    
<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>

    
    

<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>


    
    

    
    
    
    

    
    
    
    
    


<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
</body>
</html>